hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$   for ts in ds h; do
>     for SCALE in 10 50 100; do
>     #for SCALE in 2 5; do
>       echo "ts:${ts}"
>       echo "SCALE:${SCALE}"
>       #ts=ds
>       #SCALE=5
>       start=$(date +"%s.%9N")
>       ./tpc${ts}-setup.sh ${SCALE} /tmp/tpc${ts}-gen
>       end=$(date +"%s.%9N")
>       echo timediff:`echo "scale=9;$end - $start" | bc`
>       hadoop fs -du -h /tmp/tpc${ts}-gen/${SCALE}
>       hadoop fs -count -q /tmp/tpc${ts}-gen/${SCALE}
>       echo "----------------------------------------------------------------------------------------------------------------------------------------"
>       echo "use tpc${ts}_bin_partitioned_orc_${SCALE};" > dbuse.sql
>       MAX_REDUCERS=2500 # maximum number of useful reducers for any scale
>       REDUCERS=$((test ${SCALE} -gt ${MAX_REDUCERS} && echo ${MAX_REDUCERS}) || echo ${SCALE})
>       echo "REDUCERS:${REDUCERS}"
>       if [[ "${ts}" =~ "ds" ]]; then
>         nummax=100
>       else
>         nummax=23
>       fi
>       echo "nummax:${nummax}"
>       num=1
>       #for num in {1..${nummax}}
>       while [ $num -lt $nummax ]
>       do
>         if [[ "${ts}" =~ "ds" ]]; then
>           queryfile="sample-queries-tpc${ts}/query${num}.sql"
>         else
>           queryfile="sample-queries-tpc${ts}/tpch_query${num}.sql"
>         fi
>         echo "queryfile:${queryfile}"
>         start=$(date +"%s.%9N")
>         hive --hivevar REDUCERS=${REDUCERS} -i dbuse.sql -i settings/load-partitioned.sql -f ${queryfile}
>         #hive --hivevar REDUCERS=${REDUCERS} -i dbuse.sql -f ${queryfile}
>         end=$(date +"%s.%9N")
>         echo timediff:`echo "scale=9;$end - $start" | bc`
>         echo "----------------------------------------------------------------------------------------------------------------------------------------"
>         a=$[$a+1]
>       done
>
>       hive -e "DROP DATABASE IF EXISTS tpc${ts}_bin_partitioned_orc_${SCALE} CASCADE"
>       hive -e "DROP DATABASE IF EXISTS tpc${ts}_text_${SCALE} CASCADE"
>       #hive -e "use tpcds_bin_partitioned_orc_10;show tables" | xargs -I '{}' hive -e 'drop table {}'
>       hadoop fs -rm -r -f /tmp/tpc${ts}-gen/${SCALE}
>     done
>   done
ts:ds
SCALE:10
2023-01-04 13:32:06,025 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:06,025 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:32:07,041 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:07,041 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
ls: `/tmp/tpcds-gen/10': No such file or directory
Generating data at scale factor 10.
2023-01-04 13:32:08,179 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:08,179 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:32:08,956 INFO Configuration.deprecation: mapred.task.timeout is deprecated. Instead, use mapreduce.task.timeout
2023-01-04 13:32:09,024 INFO client.RMProxy: Connecting to ResourceManager at my-hadoop-yarn-rm/100.110.242.98:8032
2023-01-04 13:32:09,181 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:09,181 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:32:09,723 INFO input.FileInputFormat: Total input files to process : 1
2023-01-04 13:32:09,833 INFO mapreduce.JobSubmitter: number of splits:10
2023-01-04 13:32:09,853 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb
2023-01-04 13:32:10,003 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1672829192530_0005
2023-01-04 13:32:10,004 INFO mapreduce.JobSubmitter: Executing with tokens: []
2023-01-04 13:32:10,089 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:10,089 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:32:10,098 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:32:10,098 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:32:10,142 INFO conf.Configuration: resource-types.xml not found
2023-01-04 13:32:10,143 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2023-01-04 13:32:10,184 INFO impl.YarnClientImpl: Submitted application application_1672829192530_0005
2023-01-04 13:32:10,236 INFO mapreduce.Job: The url to track the job: http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672829192530_0005/
2023-01-04 13:32:10,236 INFO mapreduce.Job: Running job: job_1672829192530_0005
2023-01-04 13:32:16,300 INFO mapreduce.Job: Job job_1672829192530_0005 running in uber mode : false
2023-01-04 13:32:16,301 INFO mapreduce.Job:  map 0% reduce 0%
2023-01-04 13:35:53,124 INFO mapreduce.Job:  map 10% reduce 0%
2023-01-04 13:36:11,158 INFO mapreduce.Job:  map 30% reduce 0%
2023-01-04 13:36:56,239 INFO mapreduce.Job:  map 40% reduce 0%
2023-01-04 13:36:58,242 INFO mapreduce.Job:  map 70% reduce 0%
2023-01-04 13:36:59,244 INFO mapreduce.Job:  map 80% reduce 0%
2023-01-04 13:37:00,246 INFO mapreduce.Job:  map 90% reduce 0%
2023-01-04 13:38:08,353 INFO mapreduce.Job:  map 100% reduce 0%
2023-01-04 13:38:09,356 INFO mapreduce.Job: Job job_1672829192530_0005 completed successfully
2023-01-04 13:38:09,406 INFO mapreduce.Job: Counters: 33
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=2292160
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		JFS: Number of bytes read=4319
		JFS: Number of bytes written=12194709106
		JFS: Number of read operations=455
		JFS: Number of large read operations=0
		JFS: Number of write operations=284
	Job Counters
		Killed map tasks=5
		Launched map tasks=15
		Other local map tasks=15
		Total time spent by all maps in occupied slots (ms)=2886235
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=2886235
		Total vcore-milliseconds taken by all map tasks=2886235
		Total megabyte-milliseconds taken by all map tasks=2955504640
	Map-Reduce Framework
		Map input records=10
		Map output records=0
		Input split bytes=1000
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=6202
		CPU time spent (ms)=281610
		Physical memory (bytes) snapshot=11619082240
		Virtual memory (bytes) snapshot=42739376128
		Total committed heap usage (bytes)=3524788224
		Peak Map Physical memory (bytes)=1180921856
		Peak Map Virtual memory (bytes)=4311171072
	File Input Format Counters
		Bytes Read=3319
	File Output Format Counters
		Bytes Written=0
2023-01-04 13:38:10,484 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:38:10,484 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2023-01-04 13:38:11,560 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 13:38:11,561 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
TPC-DS text data generation complete.
Loading text data into external tables.
Optimizing table date_dim (1/24).
Optimizing table time_dim (2/24).
Optimizing table item (3/24).
Optimizing table customer (4/24).
Optimizing table customer_demographics (5/24).
Optimizing table household_demographics (6/24).
Optimizing table customer_address (7/24).
Optimizing table store (8/24).
Optimizing table promotion (9/24).
Optimizing table warehouse (10/24).
Optimizing table ship_mode (11/24).
Optimizing table reason (12/24).
Optimizing table income_band (13/24).
Optimizing table call_center (14/24).
Optimizing table web_page (15/24).
Optimizing table catalog_page (16/24).
Optimizing table web_site (17/24).
Optimizing table store_sales (18/24).
Optimizing table store_returns (19/24).
Optimizing table web_sales (20/24).
Optimizing table web_returns (21/24).
Optimizing table catalog_sales (22/24).
Optimizing table catalog_returns (23/24).
Optimizing table inventory (24/24).
Loading constraints
Data loaded into database tpcds_bin_partitioned_orc_10.
timediff:3750.860841327
2023-01-04 14:34:36,824 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 14:34:36,825 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
0        0        /tmp/tpcds-gen/10/_SUCCESS
7.4 K    7.4 K    /tmp/tpcds-gen/10/call_center
1.6 M    1.6 M    /tmp/tpcds-gen/10/catalog_page
211.3 M  211.3 M  /tmp/tpcds-gen/10/catalog_returns
2.8 G    2.8 G    /tmp/tpcds-gen/10/catalog_sales
63.8 M   63.8 M   /tmp/tpcds-gen/10/customer
26.4 M   26.4 M   /tmp/tpcds-gen/10/customer_address
76.9 M   76.9 M   /tmp/tpcds-gen/10/customer_demographics
9.8 M    9.8 M    /tmp/tpcds-gen/10/date_dim
223      223      /tmp/tpcds-gen/10/dbgen_version
148.1 K  148.1 K  /tmp/tpcds-gen/10/household_demographics
328      328      /tmp/tpcds-gen/10/income_band
2.6 G    2.6 G    /tmp/tpcds-gen/10/inventory
27.5 M   27.5 M   /tmp/tpcds-gen/10/item
60.7 K   60.7 K   /tmp/tpcds-gen/10/promotion
1.6 K    1.6 K    /tmp/tpcds-gen/10/reason
1.1 K    1.1 K    /tmp/tpcds-gen/10/ship_mode
26.5 K   26.5 K   /tmp/tpcds-gen/10/store
322.7 M  322.7 M  /tmp/tpcds-gen/10/store_returns
3.7 G    3.7 G    /tmp/tpcds-gen/10/store_sales
4.9 M    4.9 M    /tmp/tpcds-gen/10/time_dim
1.2 K    1.2 K    /tmp/tpcds-gen/10/warehouse
18.9 K   18.9 K   /tmp/tpcds-gen/10/web_page
97.3 M   97.3 M   /tmp/tpcds-gen/10/web_returns
1.4 G    1.4 G    /tmp/tpcds-gen/10/web_sales
11.9 K   11.9 K   /tmp/tpcds-gen/10/web_site
2023-01-04 14:34:37,892 INFO fs.TrashPolicyDefault: The configured checkpoint interval is 0 minutes. Using an interval of 0 minutes that is used for deletion instead
2023-01-04 14:34:37,892 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
        none             inf            none             inf           26           80        12194709106 /tmp/tpcds-gen/10
----------------------------------------------------------------------------------------------------------------------------------------
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       echo "use tpc${ts}_bin_partitioned_orc_${SCALE};" > dbuse.sql
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       MAX_REDUCERS=2500 # maximum number of useful reducers for any scale
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       REDUCERS=$((test ${SCALE} -gt ${MAX_REDUCERS} && echo ${MAX_REDUCERS}) || echo ${SCALE})
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       echo "REDUCERS:${REDUCERS}"
REDUCERS:10
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       if [[ "${ts}" =~ "ds" ]]; then
>         nummax=100
>       else
>         nummax=23
>       fi
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       echo "nummax:${nummax}"
nummax:100
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       num=1
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       #for num in {1..${nummax}}
hdfs@hive-client-794d58494f-9fs5h:~/apache-hive-3.1.2-bin/hive-testbench$       while [ $num -lt $nummax ]
>       do
>         if [[ "${ts}" =~ "ds" ]]; then
>           queryfile="sample-queries-tpc${ts}/query${num}.sql"
>         else
>           queryfile="sample-queries-tpc${ts}/tpch_query${num}.sql"
>         fi
>         echo "queryfile:${queryfile}"
>         start=$(date +"%s.%9N")
>         hive --hivevar REDUCERS=${REDUCERS} -i dbuse.sql -i settings/load-partitioned.sql -f ${queryfile}
>         #hive --hivevar REDUCERS=${REDUCERS} -i dbuse.sql -f ${queryfile}
>         end=$(date +"%s.%9N")
>         echo timediff:`echo "scale=9;$end - $start" | bc`
>         echo "----------------------------------------------------------------------------------------------------------------------------------------"
>         num=$[$num+1]
>       done
queryfile:sample-queries-tpcds/query1.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = bfe26c2f-d7ad-4c77-9179-8270369fe0fc

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 8536f813-cffb-4503-be9d-5e5cc3f2afa6
No Stats for tpcds_bin_partitioned_orc_10@store_returns, Columns: sr_fee, sr_store_sk, sr_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_state, s_store_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_customer_sk, c_customer_id
Query ID = hdfs_20230105063651_ea1df379-e5e1-42cd-b854-0e420f9c05a5
Total jobs = 10
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2023-01-05 06:37:13	Dump the side-table for tag: 1 with group count: 366 into file: file:/tmp/hdfs/bfe26c2f-d7ad-4c77-9179-8270369fe0fc/hive_2023-01-05_06-36-51_984_7272661083147498469-1/-local-10021/HashTable-Stage-2/MapJoin-mapfile51--.hashtable2023-01-05 06:37:13	Uploaded 1 File to: file:/tmp/hdfs/bfe26c2f-d7ad-4c77-9179-8270369fe0fc/hive_2023-01-05_06-36-51_984_7272661083147498469-1/-local-10021/HashTable-Stage-2/MapJoin-mapfile51--.hashtable (7984 bytes)
2023-01-05 06:37:13	End of local task; Time Taken: 2.538 sec.
2023-01-05 06:37:14	Uploaded 1 File to: file:/tmp/hdfs/bfe26c2f-d7ad-4c77-9179-8270369fe0fc/hive_2023-01-05_06-36-51_984_7272661083147498469-1/-local-10023/HashTable-Stage-11/MapJoin-mapfile61--.hashtable (7984 bytes)
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 10
Launching Job 2 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0010, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0010/
Starting Job = job_1672890466700_0009, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0009/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0009
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0010
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 1
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-01-05 06:37:31,474 Stage-2 map = 0%,  reduce = 0%
2023-01-05 06:37:31,474 Stage-11 map = 0%,  reduce = 0%
2023-01-05 06:37:48,733 Stage-11 map = 1%,  reduce = 0%, Cumulative CPU 18.87 sec
2023-01-05 06:37:49,015 Stage-2 map = 2%,  reduce = 0%, Cumulative CPU 16.86 sec
2023-01-05 06:37:54,815 Stage-11 map = 3%,  reduce = 0%, Cumulative CPU 25.94 sec
2023-01-05 06:37:55,120 Stage-2 map = 4%,  reduce = 0%, Cumulative CPU 23.22 sec
2023-01-05 06:38:00,896 Stage-11 map = 6%,  reduce = 0%, Cumulative CPU 33.32 sec
2023-01-05 06:38:01,220 Stage-2 map = 8%,  reduce = 0%, Cumulative CPU 29.61 sec
2023-01-05 06:38:06,980 Stage-11 map = 10%,  reduce = 0%, Cumulative CPU 40.24 sec
2023-01-05 06:38:07,314 Stage-2 map = 13%,  reduce = 0%, Cumulative CPU 35.79 sec
2023-01-05 06:38:13,053 Stage-11 map = 14%,  reduce = 0%, Cumulative CPU 46.98 sec
2023-01-05 06:38:13,402 Stage-2 map = 16%,  reduce = 0%, Cumulative CPU 41.89 sec
2023-01-05 06:38:19,130 Stage-11 map = 16%,  reduce = 0%, Cumulative CPU 53.71 sec
2023-01-05 06:38:19,488 Stage-2 map = 18%,  reduce = 0%, Cumulative CPU 48.11 sec
2023-01-05 06:38:24,191 Stage-11 map = 19%,  reduce = 0%, Cumulative CPU 60.61 sec
2023-01-05 06:38:25,575 Stage-2 map = 23%,  reduce = 0%, Cumulative CPU 54.69 sec
2023-01-05 06:38:30,262 Stage-11 map = 22%,  reduce = 0%, Cumulative CPU 68.11 sec
2023-01-05 06:38:30,648 Stage-2 map = 27%,  reduce = 0%, Cumulative CPU 61.58 sec
2023-01-05 06:38:36,338 Stage-11 map = 26%,  reduce = 0%, Cumulative CPU 75.8 sec
2023-01-05 06:38:36,736 Stage-2 map = 30%,  reduce = 0%, Cumulative CPU 67.91 sec
2023-01-05 06:38:42,407 Stage-11 map = 29%,  reduce = 0%, Cumulative CPU 82.79 sec
2023-01-05 06:38:42,822 Stage-2 map = 32%,  reduce = 0%, Cumulative CPU 74.39 sec
2023-01-05 06:38:48,478 Stage-11 map = 31%,  reduce = 0%, Cumulative CPU 89.85 sec
2023-01-05 06:38:48,905 Stage-2 map = 37%,  reduce = 0%, Cumulative CPU 80.68 sec
2023-01-05 06:38:54,545 Stage-11 map = 34%,  reduce = 0%, Cumulative CPU 96.94 sec
2023-01-05 06:38:54,989 Stage-2 map = 41%,  reduce = 0%, Cumulative CPU 86.82 sec
2023-01-05 06:39:00,618 Stage-11 map = 39%,  reduce = 0%, Cumulative CPU 103.7 sec
2023-01-05 06:39:01,073 Stage-2 map = 43%,  reduce = 0%, Cumulative CPU 92.93 sec
2023-01-05 06:39:06,684 Stage-11 map = 42%,  reduce = 0%, Cumulative CPU 110.48 sec
2023-01-05 06:39:07,154 Stage-2 map = 47%,  reduce = 0%, Cumulative CPU 99.03 sec
2023-01-05 06:39:12,755 Stage-11 map = 44%,  reduce = 0%, Cumulative CPU 117.22 sec
2023-01-05 06:39:13,240 Stage-2 map = 51%,  reduce = 0%, Cumulative CPU 105.09 sec
2023-01-05 06:39:18,824 Stage-11 map = 47%,  reduce = 0%, Cumulative CPU 123.55 sec
2023-01-05 06:39:19,324 Stage-2 map = 55%,  reduce = 0%, Cumulative CPU 111.03 sec
2023-01-05 06:39:24,889 Stage-11 map = 50%,  reduce = 0%, Cumulative CPU 128.29 sec
2023-01-05 06:39:25,400 Stage-2 map = 56%,  reduce = 0%, Cumulative CPU 115.6 sec
2023-01-05 06:39:30,954 Stage-11 map = 53%,  reduce = 0%, Cumulative CPU 133.14 sec
2023-01-05 06:39:31,480 Stage-2 map = 58%,  reduce = 0%, Cumulative CPU 120.1 sec
2023-01-05 06:39:37,021 Stage-11 map = 55%,  reduce = 0%, Cumulative CPU 138.32 sec
2023-01-05 06:39:37,559 Stage-2 map = 61%,  reduce = 0%, Cumulative CPU 125.14 sec
2023-01-05 06:39:43,089 Stage-11 map = 57%,  reduce = 0%, Cumulative CPU 144.72 sec
2023-01-05 06:39:43,643 Stage-2 map = 65%,  reduce = 0%, Cumulative CPU 130.83 sec
2023-01-05 06:39:48,717 Stage-2 map = 67%,  reduce = 0%, Cumulative CPU 137.15 sec
2023-01-05 06:39:49,158 Stage-11 map = 60%,  reduce = 0%, Cumulative CPU 151.67 sec
2023-01-05 06:39:49,732 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 137.3 sec
2023-01-05 06:39:54,216 Stage-11 map = 64%,  reduce = 0%, Cumulative CPU 151.67 sec
2023-01-05 06:39:56,840 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 140.88 sec
MapReduce Total cumulative CPU time: 2 minutes 20 seconds 880 msec
Ended Job = job_1672890466700_0010
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 06:40:00,302 Stage-11 map = 66%,  reduce = 0%, Cumulative CPU 164.49 sec
2023-01-05 06:40:04,347 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 168.71 sec
2023-01-05 06:40:04	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 06:40:06	End of local task; Time Taken: 1.044 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 3 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0011, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0011/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0011
2023-01-05 06:40:12,450 Stage-11 map = 100%,  reduce = 100%, Cumulative CPU 174.19 sec
MapReduce Total cumulative CPU time: 2 minutes 54 seconds 190 msec
Ended Job = job_1672890466700_0009
Hadoop job information for Stage-19: number of mappers: 1; number of reducers: 0
2023-01-05 06:40:15,917 Stage-19 map = 0%,  reduce = 0%
2023-01-05 06:40:19,996 Stage-19 map = 100%,  reduce = 0%, Cumulative CPU 0.65 sec
MapReduce Total cumulative CPU time: 650 msec
Ended Job = job_1672890466700_0011
Stage-24 is filtered out by condition resolver.
Stage-25 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 06:40:28	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 06:40:29	End of local task; Time Taken: 0.409 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 5 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0012, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0012/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0012
Hadoop job information for Stage-17: number of mappers: 1; number of reducers: 0
2023-01-05 06:40:39,829 Stage-17 map = 0%,  reduce = 0%
2023-01-05 06:40:44,905 Stage-17 map = 100%,  reduce = 0%, Cumulative CPU 1.47 sec
MapReduce Total cumulative CPU time: 1 seconds 470 msec
Ended Job = job_1672890466700_0012
Stage-22 is filtered out by condition resolver.
Stage-23 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 06:40:54	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 06:40:55	Dump the side-table for tag: 0 with group count: 0 into file: file:/tmp/hdfs/bfe26c2f-d7ad-4c77-9179-8270369fe0fc/hive_2023-01-05_06-36-51_984_7272661083147498469-1/-local-10013/HashTable-Stage-14/MapJoin-mapfile10--.hashtable2023-01-05 06:40:55	Uploaded 1 File to: file:/tmp/hdfs/bfe26c2f-d7ad-4c77-9179-8270369fe0fc/hive_2023-01-05_06-36-51_984_7272661083147498469-1/-local-10013/HashTable-Stage-14/MapJoin-mapfile10--.hashtable (260 bytes)
2023-01-05 06:40:55	End of local task; Time Taken: 0.398 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0013, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0013/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0013
Hadoop job information for Stage-14: number of mappers: 1; number of reducers: 0
2023-01-05 06:41:03,680 Stage-14 map = 0%,  reduce = 0%
2023-01-05 06:41:06,740 Stage-14 map = 100%,  reduce = 0%, Cumulative CPU 1.25 sec
MapReduce Total cumulative CPU time: 1 seconds 250 msec
Ended Job = job_1672890466700_0013
Launching Job 8 out of 10
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0014, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0014/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0014
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2023-01-05 06:41:14,696 Stage-6 map = 0%,  reduce = 0%
2023-01-05 06:41:18,769 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 0.69 sec
2023-01-05 06:41:24,901 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.81 sec
MapReduce Total cumulative CPU time: 1 seconds 810 msec
Ended Job = job_1672890466700_0014
MapReduce Jobs Launched:
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 140.88 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-11: Map: 1  Reduce: 1   Cumulative CPU: 174.19 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 1   Cumulative CPU: 0.65 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-17: Map: 1   Cumulative CPU: 1.47 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-14: Map: 1   Cumulative CPU: 1.25 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-6: Map: 1  Reduce: 1   Cumulative CPU: 1.81 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 5 minutes 20 seconds 250 msec
OK
Time taken: 275.477 seconds
timediff:281.302684479
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query2.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = adb1ade4-dd59-4109-842a-bed9e6c29c33

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = dfbd32f1-0768-443b-b0a6-5ffc4fe697cd
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_ext_sales_price
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_ext_sales_price
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_day_name, d_date_sk, d_week_seq
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_year
Query ID = hdfs_20230105064132_0cfc60a2-6b9a-4916-adcd-8f4d7d742703
Total jobs = 10
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]


SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2023-01-05 06:41:55	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 06:41:59	End of local task; Time Taken: 3.978 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0016, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0016/
Starting Job = job_1672890466700_0015, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0015/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0016
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0015
Hadoop job information for Stage-21: number of mappers: 5; number of reducers: 0
2023-01-05 06:42:24,515 Stage-21 map = 0%,  reduce = 0%
Hadoop job information for Stage-19: number of mappers: 5; number of reducers: 0
2023-01-05 06:42:25,460 Stage-19 map = 0%,  reduce = 0%
2023-01-05 06:42:50,605 Stage-21 map = 3%,  reduce = 0%, Cumulative CPU 55.3 sec
2023-01-05 06:42:56,108 Stage-21 map = 4%,  reduce = 0%, Cumulative CPU 86.65 sec
2023-01-05 06:42:56,710 Stage-19 map = 1%,  reduce = 0%, Cumulative CPU 41.76 sec
2023-01-05 06:43:01,879 Stage-19 map = 2%,  reduce = 0%, Cumulative CPU 53.44 sec
2023-01-05 06:43:02,701 Stage-21 map = 6%,  reduce = 0%, Cumulative CPU 119.0 sec
2023-01-05 06:43:08,085 Stage-19 map = 3%,  reduce = 0%, Cumulative CPU 64.9 sec
2023-01-05 06:43:08,309 Stage-21 map = 7%,  reduce = 0%, Cumulative CPU 164.21 sec
2023-01-05 06:43:14,255 Stage-19 map = 4%,  reduce = 0%, Cumulative CPU 75.5 sec
2023-01-05 06:43:15,201 Stage-21 map = 9%,  reduce = 0%, Cumulative CPU 184.94 sec
2023-01-05 06:43:20,442 Stage-19 map = 5%,  reduce = 0%, Cumulative CPU 86.95 sec
2023-01-05 06:43:20,617 Stage-21 map = 10%,  reduce = 0%, Cumulative CPU 204.19 sec
2023-01-05 06:43:26,295 Stage-21 map = 11%,  reduce = 0%, Cumulative CPU 213.88 sec
2023-01-05 06:43:26,624 Stage-19 map = 6%,  reduce = 0%, Cumulative CPU 98.26 sec
2023-01-05 06:43:27,392 Stage-21 map = 12%,  reduce = 0%, Cumulative CPU 224.24 sec
2023-01-05 06:43:31,745 Stage-19 map = 7%,  reduce = 0%, Cumulative CPU 109.41 sec
2023-01-05 06:43:32,808 Stage-21 map = 13%,  reduce = 0%, Cumulative CPU 243.91 sec
2023-01-05 06:43:38,310 Stage-21 map = 14%,  reduce = 0%, Cumulative CPU 253.31 sec
2023-01-05 06:43:38,344 Stage-19 map = 8%,  reduce = 0%, Cumulative CPU 173.79 sec
2023-01-05 06:43:39,399 Stage-21 map = 15%,  reduce = 0%, Cumulative CPU 264.25 sec
2023-01-05 06:43:44,520 Stage-19 map = 9%,  reduce = 0%, Cumulative CPU 222.09 sec
2023-01-05 06:43:44,804 Stage-21 map = 17%,  reduce = 0%, Cumulative CPU 287.54 sec
2023-01-05 06:43:50,707 Stage-19 map = 10%,  reduce = 0%, Cumulative CPU 244.45 sec
2023-01-05 06:43:51,196 Stage-21 map = 18%,  reduce = 0%, Cumulative CPU 312.36 sec
2023-01-05 06:43:55,884 Stage-19 map = 11%,  reduce = 0%, Cumulative CPU 267.05 sec
2023-01-05 06:43:56,517 Stage-21 map = 19%,  reduce = 0%, Cumulative CPU 326.11 sec
2023-01-05 06:43:57,599 Stage-21 map = 20%,  reduce = 0%, Cumulative CPU 337.84 sec
2023-01-05 06:44:02,048 Stage-19 map = 12%,  reduce = 0%, Cumulative CPU 288.92 sec
2023-01-05 06:44:02,992 Stage-21 map = 22%,  reduce = 0%, Cumulative CPU 362.52 sec
2023-01-05 06:44:08,204 Stage-19 map = 13%,  reduce = 0%, Cumulative CPU 310.75 sec
2023-01-05 06:44:09,336 Stage-21 map = 23%,  reduce = 0%, Cumulative CPU 386.31 sec
2023-01-05 06:44:11,413 Stage-21 map = 24%,  reduce = 0%, Cumulative CPU 394.14 sec
2023-01-05 06:44:14,369 Stage-19 map = 14%,  reduce = 0%, Cumulative CPU 332.27 sec
2023-01-05 06:44:14,617 Stage-21 map = 25%,  reduce = 0%, Cumulative CPU 404.96 sec
2023-01-05 06:44:20,553 Stage-19 map = 15%,  reduce = 0%, Cumulative CPU 354.39 sec
2023-01-05 06:44:21,001 Stage-21 map = 27%,  reduce = 0%, Cumulative CPU 433.39 sec
2023-01-05 06:44:23,295 Stage-21 map = 28%,  reduce = 0%, Cumulative CPU 436.85 sec
2023-01-05 06:44:26,747 Stage-19 map = 16%,  reduce = 0%, Cumulative CPU 375.99 sec
2023-01-05 06:44:27,497 Stage-21 map = 29%,  reduce = 0%, Cumulative CPU 455.75 sec
2023-01-05 06:44:28,528 Stage-21 map = 30%,  reduce = 0%, Cumulative CPU 458.94 sec
2023-01-05 06:44:31,874 Stage-19 map = 17%,  reduce = 0%, Cumulative CPU 392.05 sec
2023-01-05 06:44:32,797 Stage-21 map = 31%,  reduce = 0%, Cumulative CPU 478.65 sec
2023-01-05 06:44:35,911 Stage-21 map = 32%,  reduce = 0%, Cumulative CPU 485.63 sec
2023-01-05 06:44:38,059 Stage-19 map = 18%,  reduce = 0%, Cumulative CPU 420.27 sec
2023-01-05 06:44:39,036 Stage-21 map = 33%,  reduce = 0%, Cumulative CPU 501.2 sec
2023-01-05 06:44:44,206 Stage-19 map = 19%,  reduce = 0%, Cumulative CPU 441.21 sec
2023-01-05 06:44:45,321 Stage-21 map = 35%,  reduce = 0%, Cumulative CPU 524.41 sec
2023-01-05 06:44:50,387 Stage-19 map = 20%,  reduce = 0%, Cumulative CPU 462.25 sec
2023-01-05 06:44:50,624 Stage-21 map = 36%,  reduce = 0%, Cumulative CPU 536.32 sec
2023-01-05 06:44:51,658 Stage-21 map = 37%,  reduce = 0%, Cumulative CPU 548.47 sec
2023-01-05 06:44:55,491 Stage-19 map = 21%,  reduce = 0%, Cumulative CPU 474.07 sec
2023-01-05 06:44:56,919 Stage-21 map = 38%,  reduce = 0%, Cumulative CPU 570.35 sec
2023-01-05 06:45:01,672 Stage-19 map = 22%,  reduce = 0%, Cumulative CPU 495.58 sec
2023-01-05 06:45:03,301 Stage-21 map = 40%,  reduce = 0%, Cumulative CPU 593.2 sec
2023-01-05 06:45:07,828 Stage-19 map = 23%,  reduce = 0%, Cumulative CPU 515.94 sec
2023-01-05 06:45:08,595 Stage-21 map = 41%,  reduce = 0%, Cumulative CPU 605.3 sec
2023-01-05 06:45:09,623 Stage-21 map = 42%,  reduce = 0%, Cumulative CPU 616.89 sec
2023-01-05 06:45:10,932 Stage-19 map = 24%,  reduce = 0%, Cumulative CPU 528.27 sec
2023-01-05 06:45:14,003 Stage-19 map = 25%,  reduce = 0%, Cumulative CPU 543.22 sec
2023-01-05 06:45:14,901 Stage-21 map = 43%,  reduce = 0%, Cumulative CPU 641.15 sec
2023-01-05 06:45:18,020 Stage-21 map = 44%,  reduce = 0%, Cumulative CPU 647.93 sec
2023-01-05 06:45:20,174 Stage-19 map = 26%,  reduce = 0%, Cumulative CPU 568.38 sec
2023-01-05 06:45:21,198 Stage-21 map = 45%,  reduce = 0%, Cumulative CPU 663.4 sec
2023-01-05 06:45:26,324 Stage-19 map = 27%,  reduce = 0%, Cumulative CPU 587.86 sec
2023-01-05 06:45:27,458 Stage-21 map = 46%,  reduce = 0%, Cumulative CPU 684.5 sec
2023-01-05 06:45:32,515 Stage-19 map = 28%,  reduce = 0%, Cumulative CPU 609.08 sec
2023-01-05 06:45:32,983 Stage-21 map = 48%,  reduce = 0%, Cumulative CPU 707.76 sec
2023-01-05 06:45:37,849 Stage-19 map = 29%,  reduce = 0%, Cumulative CPU 622.32 sec
2023-01-05 06:45:39,204 Stage-21 map = 49%,  reduce = 0%, Cumulative CPU 731.25 sec
2023-01-05 06:45:40,932 Stage-19 map = 30%,  reduce = 0%, Cumulative CPU 635.04 sec
2023-01-05 06:45:41,314 Stage-21 map = 50%,  reduce = 0%, Cumulative CPU 734.76 sec
2023-01-05 06:45:45,014 Stage-19 map = 31%,  reduce = 0%, Cumulative CPU 652.86 sec
2023-01-05 06:45:45,527 Stage-21 map = 51%,  reduce = 0%, Cumulative CPU 754.77 sec
2023-01-05 06:45:48,641 Stage-21 map = 52%,  reduce = 0%, Cumulative CPU 762.18 sec
2023-01-05 06:45:50,168 Stage-19 map = 32%,  reduce = 0%, Cumulative CPU 671.32 sec
2023-01-05 06:45:51,820 Stage-21 map = 53%,  reduce = 0%, Cumulative CPU 778.36 sec
2023-01-05 06:45:56,327 Stage-19 map = 33%,  reduce = 0%, Cumulative CPU 692.79 sec
2023-01-05 06:45:57,092 Stage-21 map = 55%,  reduce = 0%, Cumulative CPU 797.23 sec
2023-01-05 06:46:00,292 Stage-21 map = 56%,  reduce = 0%, Cumulative CPU 809.25 sec
2023-01-05 06:46:02,471 Stage-19 map = 35%,  reduce = 0%, Cumulative CPU 718.36 sec
2023-01-05 06:46:03,435 Stage-21 map = 57%,  reduce = 0%, Cumulative CPU 825.5 sec
2023-01-05 06:46:05,561 Stage-21 map = 58%,  reduce = 0%, Cumulative CPU 828.98 sec
2023-01-05 06:46:08,627 Stage-19 map = 36%,  reduce = 0%, Cumulative CPU 739.12 sec
2023-01-05 06:46:09,796 Stage-21 map = 59%,  reduce = 0%, Cumulative CPU 849.07 sec
2023-01-05 06:46:10,825 Stage-21 map = 60%,  reduce = 0%, Cumulative CPU 852.69 sec
2023-01-05 06:46:13,750 Stage-19 map = 37%,  reduce = 0%, Cumulative CPU 751.86 sec
2023-01-05 06:46:14,783 Stage-19 map = 38%,  reduce = 0%, Cumulative CPU 760.63 sec
2023-01-05 06:46:15,008 Stage-21 map = 61%,  reduce = 0%, Cumulative CPU 866.54 sec
2023-01-05 06:46:16,039 Stage-21 map = 62%,  reduce = 0%, Cumulative CPU 871.56 sec
2023-01-05 06:46:21,020 Stage-19 map = 39%,  reduce = 0%, Cumulative CPU 782.53 sec
2023-01-05 06:46:21,413 Stage-21 map = 64%,  reduce = 0%, Cumulative CPU 887.2 sec
2023-01-05 06:46:26,168 Stage-19 map = 40%,  reduce = 0%, Cumulative CPU 797.87 sec
2023-01-05 06:46:27,718 Stage-21 map = 65%,  reduce = 0%, Cumulative CPU 902.34 sec
2023-01-05 06:46:28,794 Stage-21 map = 66%,  reduce = 0%, Cumulative CPU 905.81 sec
2023-01-05 06:46:32,437 Stage-19 map = 41%,  reduce = 0%, Cumulative CPU 814.85 sec
2023-01-05 06:46:34,016 Stage-21 map = 67%,  reduce = 0%, Cumulative CPU 916.98 sec
2023-01-05 06:46:38,674 Stage-19 map = 42%,  reduce = 0%, Cumulative CPU 831.13 sec
2023-01-05 06:46:41,297 Stage-21 map = 68%,  reduce = 0%, Cumulative CPU 929.76 sec
2023-01-05 06:46:43,858 Stage-19 map = 43%,  reduce = 0%, Cumulative CPU 847.22 sec
2023-01-05 06:46:50,167 Stage-19 map = 44%,  reduce = 0%, Cumulative CPU 861.52 sec
2023-01-05 06:46:56,322 Stage-19 map = 45%,  reduce = 0%, Cumulative CPU 873.45 sec
2023-01-05 06:47:02,460 Stage-19 map = 46%,  reduce = 0%, Cumulative CPU 886.55 sec
2023-01-05 06:47:07,394 Stage-21 map = 69%,  reduce = 0%, Cumulative CPU 950.61 sec
2023-01-05 06:47:14,741 Stage-19 map = 47%,  reduce = 0%, Cumulative CPU 910.42 sec
2023-01-05 06:47:25,959 Stage-19 map = 48%,  reduce = 0%, Cumulative CPU 931.84 sec
2023-01-05 06:47:34,137 Stage-19 map = 49%,  reduce = 0%, Cumulative CPU 946.57 sec
2023-01-05 06:47:37,611 Stage-21 map = 70%,  reduce = 0%, Cumulative CPU 968.5 sec
2023-01-05 06:47:44,351 Stage-19 map = 50%,  reduce = 0%, Cumulative CPU 965.91 sec
2023-01-05 06:47:51,489 Stage-19 map = 51%,  reduce = 0%, Cumulative CPU 982.96 sec
2023-01-05 06:47:59,634 Stage-19 map = 52%,  reduce = 0%, Cumulative CPU 998.11 sec
2023-01-05 06:48:06,511 Stage-21 map = 71%,  reduce = 0%, Cumulative CPU 988.87 sec
2023-01-05 06:48:07,793 Stage-19 map = 53%,  reduce = 0%, Cumulative CPU 1016.33 sec
2023-01-05 06:48:10,842 Stage-19 map = 54%,  reduce = 0%, Cumulative CPU 954.28 sec
2023-01-05 06:48:16,956 Stage-19 map = 55%,  reduce = 0%, Cumulative CPU 968.44 sec
2023-01-05 06:48:18,127 Stage-21 map = 72%,  reduce = 0%, Cumulative CPU 922.55 sec
2023-01-05 06:48:20,015 Stage-19 map = 56%,  reduce = 0%, Cumulative CPU 976.15 sec
2023-01-05 06:48:26,136 Stage-19 map = 57%,  reduce = 0%, Cumulative CPU 991.73 sec
2023-01-05 06:48:27,749 Stage-21 map = 73%,  reduce = 0%, Cumulative CPU 936.16 sec
2023-01-05 06:48:32,277 Stage-19 map = 58%,  reduce = 0%, Cumulative CPU 1007.12 sec
2023-01-05 06:48:38,383 Stage-19 map = 59%,  reduce = 0%, Cumulative CPU 1021.64 sec
2023-01-05 06:48:39,221 Stage-21 map = 74%,  reduce = 0%, Cumulative CPU 951.78 sec
2023-01-05 06:48:44,489 Stage-19 map = 60%,  reduce = 0%, Cumulative CPU 1037.11 sec
2023-01-05 06:48:45,792 Stage-21 map = 75%,  reduce = 0%, Cumulative CPU 894.1 sec
2023-01-05 06:48:48,552 Stage-19 map = 61%,  reduce = 0%, Cumulative CPU 1048.45 sec
2023-01-05 06:48:52,094 Stage-21 map = 76%,  reduce = 0%, Cumulative CPU 911.54 sec
2023-01-05 06:48:59,716 Stage-19 map = 62%,  reduce = 0%, Cumulative CPU 1065.24 sec
2023-01-05 06:49:03,526 Stage-21 map = 77%,  reduce = 0%, Cumulative CPU 929.58 sec
2023-01-05 06:49:06,835 Stage-19 map = 63%,  reduce = 0%, Cumulative CPU 1077.01 sec
2023-01-05 06:49:16,305 Stage-21 map = 78%,  reduce = 0%, Cumulative CPU 960.3 sec
2023-01-05 06:49:16,971 Stage-19 map = 64%,  reduce = 0%, Cumulative CPU 1092.82 sec
2023-01-05 06:49:23,072 Stage-19 map = 65%,  reduce = 0%, Cumulative CPU 1102.53 sec
2023-01-05 06:49:27,791 Stage-21 map = 79%,  reduce = 0%, Cumulative CPU 978.36 sec
2023-01-05 06:49:35,289 Stage-19 map = 66%,  reduce = 0%, Cumulative CPU 1121.26 sec
2023-01-05 06:49:40,323 Stage-21 map = 80%,  reduce = 0%, Cumulative CPU 1002.92 sec
2023-01-05 06:49:41,385 Stage-19 map = 67%,  reduce = 0%, Cumulative CPU 1131.36 sec
2023-01-05 06:49:51,925 Stage-21 map = 81%,  reduce = 0%, Cumulative CPU 1026.83 sec
2023-01-05 06:49:59,318 Stage-21 map = 82%,  reduce = 0%, Cumulative CPU 1044.17 sec
2023-01-05 06:49:59,680 Stage-19 map = 68%,  reduce = 0%, Cumulative CPU 1164.28 sec
2023-01-05 06:50:04,594 Stage-21 map = 83%,  reduce = 0%, Cumulative CPU 1055.96 sec
2023-01-05 06:50:11,906 Stage-19 map = 69%,  reduce = 0%, Cumulative CPU 1185.76 sec
2023-01-05 06:50:17,002 Stage-21 map = 84%,  reduce = 0%, Cumulative CPU 1078.93 sec
2023-01-05 06:50:22,119 Stage-21 map = 85%,  reduce = 0%, Cumulative CPU 1084.87 sec
2023-01-05 06:50:26,160 Stage-19 map = 70%,  reduce = 0%, Cumulative CPU 1209.65 sec
2023-01-05 06:50:29,509 Stage-21 map = 86%,  reduce = 0%, Cumulative CPU 1102.17 sec
2023-01-05 06:50:34,711 Stage-21 map = 87%,  reduce = 0%, Cumulative CPU 1112.58 sec
2023-01-05 06:50:37,372 Stage-19 map = 71%,  reduce = 0%, Cumulative CPU 1230.77 sec
2023-01-05 06:50:40,925 Stage-21 map = 88%,  reduce = 0%, Cumulative CPU 1123.62 sec
2023-01-05 06:50:47,207 Stage-21 map = 89%,  reduce = 0%, Cumulative CPU 1134.53 sec
2023-01-05 06:50:52,408 Stage-21 map = 90%,  reduce = 0%, Cumulative CPU 1139.69 sec
2023-01-05 06:50:53,493 Stage-21 map = 91%,  reduce = 0%, Cumulative CPU 1145.46 sec
2023-01-05 06:50:53,633 Stage-19 map = 72%,  reduce = 0%, Cumulative CPU 1261.72 sec
2023-01-05 06:51:04,827 Stage-21 map = 92%,  reduce = 0%, Cumulative CPU 1157.33 sec
2023-01-05 06:51:05,796 Stage-19 map = 73%,  reduce = 0%, Cumulative CPU 1284.88 sec
2023-01-05 06:51:11,026 Stage-21 map = 93%,  reduce = 0%, Cumulative CPU 1163.62 sec
2023-01-05 06:51:11,874 Stage-19 map = 74%,  reduce = 0%, Cumulative CPU 1295.52 sec
2023-01-05 06:51:17,955 Stage-19 map = 75%,  reduce = 0%, Cumulative CPU 1305.67 sec
2023-01-05 06:51:24,042 Stage-19 map = 76%,  reduce = 0%, Cumulative CPU 1316.99 sec
2023-01-05 06:51:32,165 Stage-19 map = 77%,  reduce = 0%, Cumulative CPU 1330.34 sec
2023-01-05 06:51:34,816 Stage-21 map = 94%,  reduce = 0%, Cumulative CPU 1189.58 sec
2023-01-05 06:51:41,291 Stage-19 map = 78%,  reduce = 0%, Cumulative CPU 1345.81 sec
2023-01-05 06:51:47,371 Stage-19 map = 79%,  reduce = 0%, Cumulative CPU 1356.51 sec
2023-01-05 06:51:54,472 Stage-19 map = 80%,  reduce = 0%, Cumulative CPU 1367.2 sec
2023-01-05 06:51:58,693 Stage-21 map = 95%,  reduce = 0%, Cumulative CPU 1215.76 sec
2023-01-05 06:52:04,908 Stage-21 map = 96%,  reduce = 0%, Cumulative CPU 1221.8 sec
2023-01-05 06:52:14,736 Stage-19 map = 81%,  reduce = 0%, Cumulative CPU 1391.72 sec
2023-01-05 06:52:17,340 Stage-21 map = 97%,  reduce = 0%, Cumulative CPU 1233.55 sec
2023-01-05 06:52:22,502 Stage-21 map = 98%,  reduce = 0%, Cumulative CPU 1238.95 sec
2023-01-05 06:52:28,633 Stage-21 map = 99%,  reduce = 0%, Cumulative CPU 1244.75 sec
2023-01-05 06:52:37,022 Stage-19 map = 82%,  reduce = 0%, Cumulative CPU 1416.08 sec
2023-01-05 06:52:52,441 Stage-21 map = 100%,  reduce = 0%, Cumulative CPU 1270.27 sec
2023-01-05 06:52:54,242 Stage-19 map = 83%,  reduce = 0%, Cumulative CPU 1437.28 sec
MapReduce Total cumulative CPU time: 21 minutes 10 seconds 270 msec
Ended Job = job_1672890466700_0015
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0017, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0017/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0017
2023-01-05 06:53:02,359 Stage-19 map = 84%,  reduce = 0%, Cumulative CPU 1449.65 sec
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 1
2023-01-05 06:53:04,294 Stage-10 map = 0%,  reduce = 0%
2023-01-05 06:53:07,420 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 1.16 sec
2023-01-05 06:53:14,521 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 2.4 sec
2023-01-05 06:53:14,528 Stage-19 map = 85%,  reduce = 0%, Cumulative CPU 1467.45 sec
MapReduce Total cumulative CPU time: 2 seconds 400 msec
Ended Job = job_1672890466700_0017
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 06:53:20,629 Stage-19 map = 86%,  reduce = 0%, Cumulative CPU 1476.31 sec
2023-01-05 06:53:21	Starting to launch local task to process map join;	maximum memory = 239075328
Execution completed successfully
MapredLocal task succeeded
Launching Job 4 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0018, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0018/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0018
2023-01-05 06:53:30,782 Stage-19 map = 87%,  reduce = 0%, Cumulative CPU 1489.41 sec
Hadoop job information for Stage-20: number of mappers: 1; number of reducers: 0
2023-01-05 06:53:31,447 Stage-20 map = 0%,  reduce = 0%
2023-01-05 06:53:38,876 Stage-19 map = 88%,  reduce = 0%, Cumulative CPU 1501.56 sec
2023-01-05 06:53:51,760 Stage-20 map = 100%,  reduce = 0%, Cumulative CPU 2.83 sec
MapReduce Total cumulative CPU time: 2 seconds 830 msec
Ended Job = job_1672890466700_0018
2023-01-05 06:53:55,075 Stage-19 map = 89%,  reduce = 0%, Cumulative CPU 1517.39 sec
2023-01-05 06:54:07,232 Stage-19 map = 90%,  reduce = 0%, Cumulative CPU 1535.37 sec
2023-01-05 06:54:18,369 Stage-19 map = 91%,  reduce = 0%, Cumulative CPU 1553.75 sec
2023-01-05 06:54:30,522 Stage-19 map = 92%,  reduce = 0%, Cumulative CPU 1572.45 sec
2023-01-05 06:54:36,599 Stage-19 map = 93%,  reduce = 0%, Cumulative CPU 1581.94 sec
2023-01-05 06:54:48,743 Stage-19 map = 94%,  reduce = 0%, Cumulative CPU 1600.07 sec
2023-01-05 06:54:54,822 Stage-19 map = 95%,  reduce = 0%, Cumulative CPU 1609.51 sec
2023-01-05 06:55:02,926 Stage-19 map = 96%,  reduce = 0%, Cumulative CPU 1623.24 sec
2023-01-05 06:55:15,075 Stage-19 map = 97%,  reduce = 0%, Cumulative CPU 1639.82 sec
2023-01-05 06:55:27,221 Stage-19 map = 98%,  reduce = 0%, Cumulative CPU 1651.87 sec
2023-01-05 06:55:33,296 Stage-19 map = 99%,  reduce = 0%, Cumulative CPU 1658.23 sec
2023-01-05 06:55:41,404 Stage-19 map = 100%,  reduce = 0%, Cumulative CPU 1666.89 sec
MapReduce Total cumulative CPU time: 27 minutes 46 seconds 890 msec
Ended Job = job_1672890466700_0016
Launching Job 5 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0019, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0019/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0019
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-01-05 06:55:51,024 Stage-2 map = 0%,  reduce = 0%
2023-01-05 06:55:56,368 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 0.74 sec
2023-01-05 06:56:00,440 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 1.97 sec
MapReduce Total cumulative CPU time: 1 seconds 970 msec
Ended Job = job_1672890466700_0019
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2023-01-05 06:56:07	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 06:56:08	End of local task; Time Taken: 1.221 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 6 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0020, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0020/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0020
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2023-01-05 06:56:18,328 Stage-18 map = 0%,  reduce = 0%
2023-01-05 06:56:22,404 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 0.98 sec
MapReduce Total cumulative CPU time: 980 msec
Ended Job = job_1672890466700_0020
Stage-22 is filtered out by condition resolver.
Stage-23 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 06:56:33	Dump the side-table for tag: 0 with group count: 53 into file: file:/tmp/hdfs/adb1ade4-dd59-4109-842a-bed9e6c29c33/hive_2023-01-05_06-41-32_976_2667334001190518633-1/-local-10013/HashTable-Stage-16/MapJoin-mapfile10--.hashtable
2023-01-05 06:56:33	End of local task; Time Taken: 0.401 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 8 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0021, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0021/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0021
Hadoop job information for Stage-16: number of mappers: 1; number of reducers: 0
2023-01-05 06:56:43,145 Stage-16 map = 0%,  reduce = 0%
2023-01-05 06:56:49,256 Stage-16 map = 100%,  reduce = 0%, Cumulative CPU 2.34 sec
MapReduce Total cumulative CPU time: 2 seconds 340 msec
Ended Job = job_1672890466700_0021
Launching Job 9 out of 10
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0022, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0022/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0022
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 06:56:58,090 Stage-5 map = 0%,  reduce = 0%
2023-01-05 06:57:03,223 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.29 sec
2023-01-05 06:57:07,295 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.24 sec
MapReduce Total cumulative CPU time: 2 seconds 240 msec
Ended Job = job_1672890466700_0022
MapReduce Jobs Launched:
Stage-Stage-21: Map: 5   Cumulative CPU: 1270.27 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-10: Map: 1  Reduce: 1   Cumulative CPU: 2.4 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-20: Map: 1   Cumulative CPU: 2.83 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 5   Cumulative CPU: 1666.89 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 1.97 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-18: Map: 1   Cumulative CPU: 0.98 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-16: Map: 1   Cumulative CPU: 2.34 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.24 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 49 minutes 9 seconds 920 msec
OK
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5114	1.08	0.99	NULL	NULL	0.69	0.93	0.98
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5115	0.88	1.07	1.05	1.02	1.04	1.13	1.07
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5116	1.04	1.03	0.95	1.12	1.04	0.94	0.98
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5117	0.95	0.95	1.04	0.99	0.97	0.98	1.00
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5118	1.02	0.97	0.96	0.99	1.06	1.00	1.03
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5119	0.94	0.96	1.00	0.95	0.94	0.91	0.98
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5120	1.00	1.00	1.04	1.05	1.04	0.92	1.01
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5121	1.02	3.35	0.92	1.04	0.94	0.96	1.12
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5122	0.32	0.66	1.36	1.03	0.99	1.00	0.95
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5123	1.03	0.97	0.99	1.03	1.09	1.01	1.01
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5124	0.96	0.98	1.02	0.97	1.02	0.99	1.02
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5125	1.03	1.00	0.97	1.00	1.04	1.04	1.00
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5126	1.03	1.05	1.07	1.07	0.96	0.98	0.97
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5127	0.98	1.00	1.04	0.99	0.90	1.02	0.99
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5128	1.00	0.95	0.96	1.00	1.03	1.00	0.97
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5129	1.08	0.99	1.05	1.02	1.05	1.14	1.01
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5130	0.98	0.98	0.93	1.03	0.96	1.04	1.00
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5131	0.97	1.01	0.94	0.99	1.08	0.97	0.93
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5132	0.96	0.96	1.07	1.01	1.00	1.01	1.04
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5133	1.03	0.94	0.96	0.97	1.03	1.08	1.05
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5134	1.02	1.01	0.94	0.96	1.03	0.96	1.00
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5135	1.00	1.01	0.97	1.05	1.03	1.04	0.98
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5136	1.02	0.97	0.94	1.00	0.97	0.96	0.94
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5137	1.08	1.01	0.98	0.97	1.10	0.94	1.01
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5138	1.04	0.96	0.97	0.97	0.96	1.03	0.99
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5139	1.02	0.93	1.02	1.02	1.00	0.98	1.05
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5140	0.99	1.06	1.06	1.03	1.00	0.91	1.06
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5141	1.03	0.96	1.01	0.99	1.00	1.03	0.96
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5142	0.99	0.98	0.99	1.01	1.00	0.96	1.01
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5143	0.99	0.56	1.02	0.94	1.04	1.04	1.02
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5144	0.84	1.04	0.45	0.44	0.44	0.45	0.42
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5145	1.01	1.00	0.97	1.00	0.99	0.98	1.05
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5146	1.03	0.97	0.99	1.03	1.04	1.04	1.03
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5147	1.01	0.95	1.01	0.98	0.96	1.04	0.99
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5148	0.98	1.02	0.97	0.98	0.99	1.00	0.97
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5149	1.02	0.98	1.05	1.00	0.99	0.94	1.04
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5150	1.03	1.06	0.96	1.00	1.05	0.95	0.97
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5151	0.94	1.02	1.00	1.00	0.98	1.01	1.00
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5152	0.94	1.01	0.98	1.04	1.02	1.00	0.98
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5153	1.06	0.97	0.97	1.00	0.98	0.99	1.00
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5154	0.98	0.96	0.97	0.92	1.01	0.97	1.03
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5155	0.98	0.96	0.99	1.03	0.97	1.02	1.00
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5156	1.05	1.05	1.00	1.00	1.03	0.99	1.01
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5157	0.66	0.86	0.76	0.65	0.66	0.69	0.63
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5158	1.03	1.02	0.95	1.02	1.01	1.03	1.01
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5159	0.99	1.04	0.98	0.98	1.00	0.99	0.98
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5160	0.96	1.02	0.97	1.00	1.00	1.03	1.00
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5161	1.02	1.03	1.02	0.96	1.01	0.98	1.01
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5162	0.99	1.05	1.03	0.95	1.00	1.01	1.03
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5163	0.97	1.00	1.02	1.02	1.01	0.99	1.00
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5164	1.01	1.01	0.99	1.03	0.99	1.01	0.99
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
5165	1.59	3.50	0.99	1.01	0.98	1.00	1.98
Time taken: 936.842 seconds, Fetched: 2513 row(s)
timediff:942.542967493
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query3.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = ad751abc-842d-480d-87cc-152e7f9cc434

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 65c99a81-cd42-4647-be99-e78c35f1aa4a
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_moy, d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_item_sk, ss_sales_price
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_brand, i_manufact_id, i_brand_id, i_item_sk
Query ID = hdfs_20230105065715_56feaa3f-1f0f-4645-911c-017eeb4137b8
Total jobs = 2

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 06:57:30	Dump the side-table for tag: 1 with group count: 6000 into file: file:/tmp/hdfs/ad751abc-842d-480d-87cc-152e7f9cc434/hive_2023-01-05_06-57-15_772_8552118666164651656-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
2023-01-05 06:57:30	Dump the side-table for tag: 1 with group count: 25 into file: file:/tmp/hdfs/ad751abc-842d-480d-87cc-152e7f9cc434/hive_2023-01-05_06-57-15_772_8552118666164651656-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2023-01-05 06:57:30	End of local task; Time Taken: 1.635 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0023, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0023/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0023
Hadoop job information for Stage-3: number of mappers: 4; number of reducers: 4
2023-01-05 06:57:45,567 Stage-3 map = 0%,  reduce = 0%
2023-01-05 06:58:01,950 Stage-3 map = 1%,  reduce = 0%, Cumulative CPU 11.11 sec
2023-01-05 06:58:03,991 Stage-3 map = 5%,  reduce = 0%, Cumulative CPU 49.87 sec
2023-01-05 06:58:07,263 Stage-3 map = 6%,  reduce = 0%, Cumulative CPU 51.68 sec
2023-01-05 06:58:10,326 Stage-3 map = 9%,  reduce = 0%, Cumulative CPU 59.49 sec
2023-01-05 06:58:13,393 Stage-3 map = 10%,  reduce = 0%, Cumulative CPU 62.32 sec
2023-01-05 06:58:15,441 Stage-3 map = 11%,  reduce = 0%, Cumulative CPU 65.23 sec
2023-01-05 06:58:16,661 Stage-3 map = 13%,  reduce = 0%, Cumulative CPU 69.34 sec
2023-01-05 06:58:19,720 Stage-3 map = 14%,  reduce = 0%, Cumulative CPU 71.22 sec
2023-01-05 06:58:21,755 Stage-3 map = 15%,  reduce = 0%, Cumulative CPU 76.83 sec
2023-01-05 06:58:22,774 Stage-3 map = 16%,  reduce = 0%, Cumulative CPU 78.19 sec
2023-01-05 06:58:25,824 Stage-3 map = 17%,  reduce = 0%, Cumulative CPU 80.25 sec
2023-01-05 06:58:27,865 Stage-3 map = 19%,  reduce = 0%, Cumulative CPU 85.57 sec
2023-01-05 06:58:28,882 Stage-3 map = 20%,  reduce = 0%, Cumulative CPU 87.06 sec
2023-01-05 06:58:31,932 Stage-3 map = 21%,  reduce = 0%, Cumulative CPU 89.19 sec
2023-01-05 06:58:33,964 Stage-3 map = 24%,  reduce = 0%, Cumulative CPU 97.32 sec
2023-01-05 06:58:38,036 Stage-3 map = 26%,  reduce = 0%, Cumulative CPU 99.16 sec
2023-01-05 06:58:40,070 Stage-3 map = 28%,  reduce = 0%, Cumulative CPU 108.26 sec
2023-01-05 06:58:44,136 Stage-3 map = 30%,  reduce = 0%, Cumulative CPU 109.86 sec
2023-01-05 06:58:46,173 Stage-3 map = 32%,  reduce = 0%, Cumulative CPU 118.22 sec
2023-01-05 06:58:50,240 Stage-3 map = 33%,  reduce = 0%, Cumulative CPU 121.39 sec
2023-01-05 06:58:51,462 Stage-3 map = 34%,  reduce = 0%, Cumulative CPU 123.17 sec
2023-01-05 06:58:52,487 Stage-3 map = 36%,  reduce = 0%, Cumulative CPU 130.21 sec
2023-01-05 06:58:55,755 Stage-3 map = 37%,  reduce = 0%, Cumulative CPU 133.44 sec
2023-01-05 06:58:57,784 Stage-3 map = 39%,  reduce = 0%, Cumulative CPU 138.2 sec
2023-01-05 06:59:01,854 Stage-3 map = 40%,  reduce = 0%, Cumulative CPU 144.25 sec
2023-01-05 06:59:03,887 Stage-3 map = 42%,  reduce = 0%, Cumulative CPU 147.95 sec
2023-01-05 06:59:04,902 Stage-3 map = 43%,  reduce = 0%, Cumulative CPU 151.41 sec
2023-01-05 06:59:07,954 Stage-3 map = 44%,  reduce = 0%, Cumulative CPU 154.24 sec
2023-01-05 06:59:09,981 Stage-3 map = 47%,  reduce = 0%, Cumulative CPU 161.82 sec
2023-01-05 06:59:16,087 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 172.63 sec
2023-01-05 06:59:20,154 Stage-3 map = 51%,  reduce = 0%, Cumulative CPU 175.23 sec
2023-01-05 06:59:22,189 Stage-3 map = 54%,  reduce = 0%, Cumulative CPU 182.87 sec
2023-01-05 06:59:27,466 Stage-3 map = 55%,  reduce = 0%, Cumulative CPU 187.99 sec
2023-01-05 06:59:28,480 Stage-3 map = 57%,  reduce = 0%, Cumulative CPU 192.36 sec
2023-01-05 06:59:31,522 Stage-3 map = 58%,  reduce = 0%, Cumulative CPU 193.84 sec
2023-01-05 06:59:33,557 Stage-3 map = 59%,  reduce = 0%, Cumulative CPU 196.16 sec
2023-01-05 06:59:34,574 Stage-3 map = 61%,  reduce = 0%, Cumulative CPU 200.72 sec
2023-01-05 06:59:35,587 Stage-3 map = 70%,  reduce = 0%, Cumulative CPU 201.47 sec
2023-01-05 06:59:37,619 Stage-3 map = 71%,  reduce = 0%, Cumulative CPU 203.25 sec
2023-01-05 06:59:40,671 Stage-3 map = 81%,  reduce = 0%, Cumulative CPU 206.56 sec
2023-01-05 06:59:43,710 Stage-3 map = 90%,  reduce = 0%, Cumulative CPU 207.36 sec
2023-01-05 06:59:45,743 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 208.7 sec
2023-01-05 06:59:46,758 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 214.11 sec
MapReduce Total cumulative CPU time: 3 minutes 34 seconds 110 msec
Ended Job = job_1672890466700_0023
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0024, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0024/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0024
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 06:59:55,644 Stage-4 map = 0%,  reduce = 0%
2023-01-05 07:00:00,732 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 0.64 sec
2023-01-05 07:00:06,831 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 1.87 sec
MapReduce Total cumulative CPU time: 1 seconds 870 msec
Ended Job = job_1672890466700_0024
MapReduce Jobs Launched:
Stage-Stage-3: Map: 4  Reduce: 4   Cumulative CPU: 214.11 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 1.87 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 35 seconds 980 msec
OK
1998	7013003	exportinameless #3                                	1080.27
1998	4003002	exportiedu pack #2                                	1064.04
1998	1002002	importoamalg #2                                   	812.64
1998	10003014	exportiunivamalg #14                              	764.77
1998	7014008	edu packnameless #8                               	751.97
1998	8008007	namelessnameless #7                               	730.96
1998	8012006	importomaxi #6                                    	730.35
1998	4004001	edu packedu pack #1                               	646.17
1998	3003001	exportiexporti #1                                 	583.74
1998	8011007	amalgmaxi #7                                      	558.74
1998	5003001	exportischolar #1                                 	536.60
1998	8011005	amalgmaxi #5                                      	454.06
1998	2004002	edu packimporto #2                                	414.44
1998	4002001	importoedu pack #1                                	368.44
1998	7009009	maxibrand #9                                      	186.75
1999	7009009	maxibrand #9                                      	1268.75
1999	4004001	edu packedu pack #1                               	1129.57
1999	10003014	exportiunivamalg #14                              	896.31
1999	4003002	exportiedu pack #2                                	886.46
1999	2004002	edu packimporto #2                                	865.87
1999	3003001	exportiexporti #1                                 	840.75
1999	8011005	amalgmaxi #5                                      	751.47
1999	7013003	exportinameless #3                                	675.44
1999	7014008	edu packnameless #8                               	598.66
1999	8011007	amalgmaxi #7                                      	563.33
1999	1002002	importoamalg #2                                   	506.32
1999	5003001	exportischolar #1                                 	484.15
1999	8008007	namelessnameless #7                               	430.20
1999	8012006	importomaxi #6                                    	367.38
1999	4002001	importoedu pack #1                                	196.25
2000	1002002	importoamalg #2                                   	1477.69
2000	2004002	edu packimporto #2                                	1105.65
2000	10003014	exportiunivamalg #14                              	819.77
2000	3003001	exportiexporti #1                                 	797.12
2000	4004001	edu packedu pack #1                               	774.82
2000	7014008	edu packnameless #8                               	694.58
2000	4003002	exportiedu pack #2                                	692.52
2000	5003001	exportischolar #1                                 	663.60
2000	4002001	importoedu pack #1                                	653.58
2000	8012006	importomaxi #6                                    	597.51
2000	9003005	importoimporto #2                                 	587.72
2000	8008007	namelessnameless #7                               	489.38
2000	2003002	exportiimporto #2                                 	364.16
2001	1002002	importoamalg #2                                   	1243.49
2001	2004002	edu packimporto #2                                	808.72
2001	7014008	edu packnameless #8                               	762.02
2001	9003005	importoimporto #2                                 	671.54
2001	2003002	exportiimporto #2                                 	655.92
2001	8012006	importomaxi #6                                    	605.34
2001	6011006	amalgbrand #6                                     	572.10
2001	10003014	exportiunivamalg #14                              	530.54
2001	10001002	amalgunivamalg #2                                 	513.63
2001	4003002	exportiedu pack #2                                	503.55
2001	8008007	scholarunivamalg #10                              	360.33
2002	2004002	edu packimporto #2                                	792.63
2002	10003014	exportiunivamalg #14                              	712.56
2002	8008007	scholarunivamalg #10                              	699.69
2002	8012006	importomaxi #6                                    	674.55
2002	10001002	amalgunivamalg #2                                 	651.97
2002	6011006	amalgbrand #6                                     	621.47
2002	7014008	edu packnameless #8                               	600.47
2002	2003002	univnameless #5                                   	597.51
2002	5003001	exportischolar #1                                 	558.80
2002	4003002	exportiedu pack #2                                	549.48
2002	8012002	amalgedu pack #1                                  	451.15
2002	1004001	edu packamalg #1                                  	420.13
2002	1002002	importoamalg #2                                   	324.03
Time taken: 173.614 seconds, Fetched: 67 row(s)
timediff:179.357469224
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query4.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = ece73c1a-8ff2-4e15-9215-cf2fa69bd9e5

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 11f58331-1ccf-4295-b9c1-f5f262e50e3f
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_preferred_cust_flag, c_customer_sk, c_login, c_last_name, c_customer_id, c_birth_country, c_first_name, c_email_address
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk, ss_ext_discount_amt, ss_ext_sales_price, ss_ext_wholesale_cost, ss_ext_list_price
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_ext_wholesale_cost, cs_ext_sales_price, cs_ext_list_price, cs_ext_discount_amt, cs_bill_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_ext_wholesale_cost, ws_ext_sales_price, ws_ext_discount_amt, ws_bill_customer_sk, ws_ext_list_price
Query ID = hdfs_20230105070014_8baf92d7-9865-4528-bd8e-750109d57ac9
Total jobs = 38

SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.



2023-01-05 07:01:13	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 07:01:22	Dump the side-table for tag: 1 with group count: 365 into file: file:/tmp/hdfs/ece73c1a-8ff2-4e15-9215-cf2fa69bd9e5/hive_2023-01-05_07-00-14_823_1513787721927763218-1/-local-10069/HashTable-Stage-63/MapJoin-mapfile231--.hashtable

2023-01-05 07:01:23	End of local task; Time Taken: 9.007 sec.
2023-01-05 07:01:23	End of local task; Time Taken: 7.516 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 3 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 4 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 5 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 6 out of 38
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0029, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0029/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0029
Starting Job = job_1672890466700_0030, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0030/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0030
Starting Job = job_1672890466700_0025, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0025/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0025
Starting Job = job_1672890466700_0028, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0028/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0028
Starting Job = job_1672890466700_0027, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0027/
Starting Job = job_1672890466700_0026, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0026/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0027
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0026
Hadoop job information for Stage-59: number of mappers: 2; number of reducers: 0
Hadoop job information for Stage-43: number of mappers: 3; number of reducers: 0
Hadoop job information for Stage-51: number of mappers: 4; number of reducers: 0
2023-01-05 07:01:49,228 Stage-51 map = 0%,  reduce = 0%
2023-01-05 07:01:49,228 Stage-43 map = 0%,  reduce = 0%
2023-01-05 07:01:49,228 Stage-59 map = 0%,  reduce = 0%
2023-01-05 07:02:04,980 Stage-59 map = 1%,  reduce = 0%, Cumulative CPU 10.19 sec
2023-01-05 07:02:05,739 Stage-51 map = 1%,  reduce = 0%, Cumulative CPU 12.9 sec
2023-01-05 07:02:08,228 Stage-43 map = 1%,  reduce = 0%, Cumulative CPU 16.68 sec
2023-01-05 07:02:10,185 Stage-59 map = 2%,  reduce = 0%, Cumulative CPU 23.8 sec
2023-01-05 07:02:10,873 Stage-51 map = 4%,  reduce = 0%, Cumulative CPU 21.59 sec
2023-01-05 07:02:11,425 Stage-43 map = 3%,  reduce = 0%, Cumulative CPU 27.82 sec
2023-01-05 07:02:11,905 Stage-51 map = 6%,  reduce = 0%, Cumulative CPU 34.54 sec
2023-01-05 07:02:14,489 Stage-43 map = 4%,  reduce = 0%, Cumulative CPU 32.94 sec
2023-01-05 07:02:16,324 Stage-59 map = 4%,  reduce = 0%, Cumulative CPU 31.03 sec
2023-01-05 07:02:16,551 Stage-43 map = 5%,  reduce = 0%, Cumulative CPU 34.94 sec
2023-01-05 07:02:17,052 Stage-51 map = 7%,  reduce = 0%, Cumulative CPU 35.93 sec
2023-01-05 07:02:18,070 Stage-51 map = 10%,  reduce = 0%, Cumulative CPU 53.34 sec
2023-01-05 07:02:20,677 Stage-43 map = 7%,  reduce = 0%, Cumulative CPU 40.94 sec
2023-01-05 07:02:21,450 Stage-59 map = 5%,  reduce = 0%, Cumulative CPU 35.59 sec
2023-01-05 07:02:22,486 Stage-59 map = 6%,  reduce = 0%, Cumulative CPU 39.22 sec
2023-01-05 07:02:22,752 Stage-43 map = 8%,  reduce = 0%, Cumulative CPU 44.36 sec
2023-01-05 07:02:23,166 Stage-51 map = 12%,  reduce = 0%, Cumulative CPU 57.6 sec
2023-01-05 07:02:24,189 Stage-51 map = 14%,  reduce = 0%, Cumulative CPU 62.32 sec
2023-01-05 07:02:25,831 Stage-43 map = 10%,  reduce = 0%, Cumulative CPU 49.72 sec
2023-01-05 07:02:27,565 Stage-59 map = 7%,  reduce = 0%, Cumulative CPU 42.69 sec
2023-01-05 07:02:28,307 Stage-51 map = 15%,  reduce = 0%, Cumulative CPU 63.83 sec
2023-01-05 07:02:29,527 Stage-51 map = 16%,  reduce = 0%, Cumulative CPU 67.28 sec
2023-01-05 07:02:30,543 Stage-51 map = 17%,  reduce = 0%, Cumulative CPU 69.11 sec
2023-01-05 07:02:32,017 Stage-43 map = 12%,  reduce = 0%, Cumulative CPU 58.09 sec
2023-01-05 07:02:33,700 Stage-59 map = 9%,  reduce = 0%, Cumulative CPU 49.4 sec
2023-01-05 07:02:34,642 Stage-51 map = 18%,  reduce = 0%, Cumulative CPU 70.92 sec
2023-01-05 07:02:35,322 Stage-43 map = 13%,  reduce = 0%, Cumulative CPU 61.87 sec
2023-01-05 07:02:35,659 Stage-51 map = 20%,  reduce = 0%, Cumulative CPU 76.46 sec
2023-01-05 07:02:38,406 Stage-43 map = 15%,  reduce = 0%, Cumulative CPU 68.42 sec
2023-01-05 07:02:39,843 Stage-59 map = 11%,  reduce = 0%, Cumulative CPU 57.25 sec
2023-01-05 07:02:40,820 Stage-51 map = 21%,  reduce = 0%, Cumulative CPU 78.62 sec
2023-01-05 07:02:40,862 Stage-59 map = 12%,  reduce = 0%, Cumulative CPU 61.25 sec
2023-01-05 07:02:41,470 Stage-43 map = 16%,  reduce = 0%, Cumulative CPU 71.79 sec
2023-01-05 07:02:42,040 Stage-51 map = 24%,  reduce = 0%, Cumulative CPU 85.24 sec
2023-01-05 07:02:44,551 Stage-43 map = 18%,  reduce = 0%, Cumulative CPU 77.26 sec
2023-01-05 07:02:45,968 Stage-59 map = 14%,  reduce = 0%, Cumulative CPU 64.0 sec
2023-01-05 07:02:46,659 Stage-43 map = 19%,  reduce = 0%, Cumulative CPU 80.91 sec
2023-01-05 07:02:46,987 Stage-59 map = 15%,  reduce = 0%, Cumulative CPU 66.43 sec
2023-01-05 07:02:47,159 Stage-51 map = 26%,  reduce = 0%, Cumulative CPU 89.27 sec
2023-01-05 07:02:48,181 Stage-51 map = 27%,  reduce = 0%, Cumulative CPU 94.2 sec
2023-01-05 07:02:50,735 Stage-43 map = 21%,  reduce = 0%, Cumulative CPU 86.57 sec
2023-01-05 07:02:52,097 Stage-59 map = 16%,  reduce = 0%, Cumulative CPU 68.78 sec
2023-01-05 07:02:52,308 Stage-51 map = 28%,  reduce = 0%, Cumulative CPU 96.28 sec
2023-01-05 07:02:52,772 Stage-43 map = 22%,  reduce = 0%, Cumulative CPU 89.85 sec
2023-01-05 07:02:53,144 Stage-59 map = 18%,  reduce = 0%, Cumulative CPU 71.47 sec
2023-01-05 07:02:53,334 Stage-51 map = 29%,  reduce = 0%, Cumulative CPU 98.02 sec
2023-01-05 07:02:54,384 Stage-51 map = 30%,  reduce = 0%, Cumulative CPU 102.71 sec
2023-01-05 07:02:55,937 Stage-43 map = 24%,  reduce = 0%, Cumulative CPU 96.28 sec
2023-01-05 07:02:58,254 Stage-59 map = 20%,  reduce = 0%, Cumulative CPU 77.29 sec
2023-01-05 07:02:58,488 Stage-51 map = 31%,  reduce = 0%, Cumulative CPU 104.85 sec
2023-01-05 07:02:59,004 Stage-43 map = 25%,  reduce = 0%, Cumulative CPU 100.63 sec
2023-01-05 07:02:59,505 Stage-51 map = 32%,  reduce = 0%, Cumulative CPU 106.29 sec
2023-01-05 07:03:00,528 Stage-51 map = 33%,  reduce = 0%, Cumulative CPU 111.64 sec
2023-01-05 07:03:02,069 Stage-43 map = 26%,  reduce = 0%, Cumulative CPU 107.42 sec
2023-01-05 07:03:04,395 Stage-59 map = 24%,  reduce = 0%, Cumulative CPU 83.39 sec
2023-01-05 07:03:04,608 Stage-51 map = 34%,  reduce = 0%, Cumulative CPU 113.26 sec
2023-01-05 07:03:05,142 Stage-43 map = 27%,  reduce = 0%, Cumulative CPU 110.97 sec
2023-01-05 07:03:05,623 Stage-51 map = 35%,  reduce = 0%, Cumulative CPU 117.48 sec
2023-01-05 07:03:06,642 Stage-51 map = 36%,  reduce = 0%, Cumulative CPU 119.5 sec
2023-01-05 07:03:08,206 Stage-43 map = 29%,  reduce = 0%, Cumulative CPU 116.71 sec
2023-01-05 07:03:10,483 Stage-59 map = 27%,  reduce = 0%, Cumulative CPU 87.22 sec
2023-01-05 07:03:10,721 Stage-51 map = 37%,  reduce = 0%, Cumulative CPU 121.25 sec
2023-01-05 07:03:11,749 Stage-51 map = 40%,  reduce = 0%, Cumulative CPU 127.45 sec
2023-01-05 07:03:14,333 Stage-43 map = 31%,  reduce = 0%, Cumulative CPU 125.13 sec
2023-01-05 07:03:16,584 Stage-59 map = 30%,  reduce = 0%, Cumulative CPU 91.73 sec
2023-01-05 07:03:17,390 Stage-43 map = 32%,  reduce = 0%, Cumulative CPU 127.21 sec
2023-01-05 07:03:18,067 Stage-51 map = 43%,  reduce = 0%, Cumulative CPU 134.58 sec
2023-01-05 07:03:20,444 Stage-43 map = 34%,  reduce = 0%, Cumulative CPU 133.03 sec
2023-01-05 07:03:22,683 Stage-59 map = 34%,  reduce = 0%, Cumulative CPU 95.86 sec
2023-01-05 07:03:23,168 Stage-51 map = 44%,  reduce = 0%, Cumulative CPU 135.8 sec
2023-01-05 07:03:23,519 Stage-43 map = 35%,  reduce = 0%, Cumulative CPU 135.3 sec
2023-01-05 07:03:24,185 Stage-51 map = 46%,  reduce = 0%, Cumulative CPU 140.07 sec
2023-01-05 07:03:26,582 Stage-43 map = 37%,  reduce = 0%, Cumulative CPU 140.45 sec
2023-01-05 07:03:27,771 Stage-59 map = 36%,  reduce = 0%, Cumulative CPU 97.72 sec
2023-01-05 07:03:28,629 Stage-43 map = 38%,  reduce = 0%, Cumulative CPU 142.54 sec
2023-01-05 07:03:28,798 Stage-59 map = 38%,  reduce = 0%, Cumulative CPU 99.84 sec
2023-01-05 07:03:29,259 Stage-51 map = 48%,  reduce = 0%, Cumulative CPU 142.53 sec
2023-01-05 07:03:30,275 Stage-51 map = 50%,  reduce = 0%, Cumulative CPU 146.39 sec
2023-01-05 07:03:32,711 Stage-43 map = 40%,  reduce = 0%, Cumulative CPU 148.34 sec
2023-01-05 07:03:33,916 Stage-59 map = 39%,  reduce = 0%, Cumulative CPU 103.94 sec
2023-01-05 07:03:34,370 Stage-51 map = 51%,  reduce = 0%, Cumulative CPU 147.67 sec
2023-01-05 07:03:34,783 Stage-43 map = 41%,  reduce = 0%, Cumulative CPU 150.85 sec
2023-01-05 07:03:34,936 Stage-59 map = 40%,  reduce = 0%, Cumulative CPU 107.24 sec
2023-01-05 07:03:35,387 Stage-51 map = 53%,  reduce = 0%, Cumulative CPU 150.46 sec
2023-01-05 07:03:36,402 Stage-51 map = 54%,  reduce = 0%, Cumulative CPU 154.65 sec
2023-01-05 07:03:37,888 Stage-43 map = 42%,  reduce = 0%, Cumulative CPU 156.77 sec
2023-01-05 07:03:40,053 Stage-59 map = 41%,  reduce = 0%, Cumulative CPU 111.6 sec
2023-01-05 07:03:40,481 Stage-51 map = 55%,  reduce = 0%, Cumulative CPU 156.17 sec
2023-01-05 07:03:40,947 Stage-43 map = 44%,  reduce = 0%, Cumulative CPU 159.52 sec
2023-01-05 07:03:41,077 Stage-59 map = 42%,  reduce = 0%, Cumulative CPU 115.56 sec
2023-01-05 07:03:41,507 Stage-51 map = 56%,  reduce = 0%, Cumulative CPU 158.22 sec
2023-01-05 07:03:42,541 Stage-51 map = 58%,  reduce = 0%, Cumulative CPU 162.4 sec
2023-01-05 07:03:44,006 Stage-43 map = 46%,  reduce = 0%, Cumulative CPU 164.67 sec
2023-01-05 07:03:46,188 Stage-59 map = 44%,  reduce = 0%, Cumulative CPU 120.16 sec
2023-01-05 07:03:46,614 Stage-51 map = 59%,  reduce = 0%, Cumulative CPU 163.71 sec
2023-01-05 07:03:47,641 Stage-51 map = 60%,  reduce = 0%, Cumulative CPU 168.29 sec
2023-01-05 07:03:48,668 Stage-51 map = 62%,  reduce = 0%, Cumulative CPU 169.79 sec
2023-01-05 07:03:50,157 Stage-43 map = 48%,  reduce = 0%, Cumulative CPU 172.27 sec
2023-01-05 07:03:52,293 Stage-59 map = 47%,  reduce = 0%, Cumulative CPU 132.0 sec
2023-01-05 07:03:52,762 Stage-51 map = 63%,  reduce = 0%, Cumulative CPU 171.33 sec
2023-01-05 07:03:53,266 Stage-43 map = 49%,  reduce = 0%, Cumulative CPU 174.18 sec
2023-01-05 07:03:53,783 Stage-51 map = 66%,  reduce = 0%, Cumulative CPU 178.15 sec
2023-01-05 07:03:56,312 Stage-43 map = 51%,  reduce = 0%, Cumulative CPU 179.23 sec
2023-01-05 07:03:58,419 Stage-59 map = 49%,  reduce = 0%, Cumulative CPU 139.61 sec
2023-01-05 07:03:58,924 Stage-51 map = 67%,  reduce = 0%, Cumulative CPU 179.45 sec
2023-01-05 07:03:59,395 Stage-43 map = 52%,  reduce = 0%, Cumulative CPU 181.1 sec
2023-01-05 07:03:59,943 Stage-51 map = 69%,  reduce = 0%, Cumulative CPU 185.0 sec
2023-01-05 07:04:02,460 Stage-43 map = 55%,  reduce = 0%, Cumulative CPU 185.8 sec
2023-01-05 07:04:04,541 Stage-59 map = 51%,  reduce = 0%, Cumulative CPU 147.0 sec
2023-01-05 07:04:05,039 Stage-51 map = 71%,  reduce = 0%, Cumulative CPU 186.3 sec
2023-01-05 07:04:05,531 Stage-43 map = 56%,  reduce = 0%, Cumulative CPU 188.07 sec
2023-01-05 07:04:06,062 Stage-51 map = 73%,  reduce = 0%, Cumulative CPU 191.17 sec
2023-01-05 07:04:08,806 Stage-43 map = 59%,  reduce = 0%, Cumulative CPU 192.87 sec
2023-01-05 07:04:10,664 Stage-59 map = 53%,  reduce = 0%, Cumulative CPU 154.65 sec
2023-01-05 07:04:10,848 Stage-43 map = 60%,  reduce = 0%, Cumulative CPU 195.18 sec
2023-01-05 07:04:11,361 Stage-51 map = 76%,  reduce = 0%, Cumulative CPU 194.75 sec
2023-01-05 07:04:12,383 Stage-51 map = 78%,  reduce = 0%, Cumulative CPU 197.99 sec
2023-01-05 07:04:13,919 Stage-43 map = 61%,  reduce = 0%, Cumulative CPU 197.57 sec
2023-01-05 07:04:14,939 Stage-43 map = 62%,  reduce = 0%, Cumulative CPU 200.86 sec
2023-01-05 07:04:16,470 Stage-51 map = 79%,  reduce = 0%, Cumulative CPU 199.09 sec
2023-01-05 07:04:16,751 Stage-59 map = 56%,  reduce = 0%, Cumulative CPU 159.28 sec
2023-01-05 07:04:16,971 Stage-43 map = 63%,  reduce = 0%, Cumulative CPU 202.89 sec
2023-01-05 07:04:17,484 Stage-51 map = 80%,  reduce = 0%, Cumulative CPU 200.96 sec
2023-01-05 07:04:18,498 Stage-51 map = 82%,  reduce = 0%, Cumulative CPU 203.92 sec
2023-01-05 07:04:20,058 Stage-43 map = 65%,  reduce = 0%, Cumulative CPU 208.76 sec
2023-01-05 07:04:21,878 Stage-59 map = 58%,  reduce = 0%, Cumulative CPU 162.12 sec
2023-01-05 07:04:22,571 Stage-51 map = 83%,  reduce = 0%, Cumulative CPU 205.12 sec
2023-01-05 07:04:22,896 Stage-59 map = 59%,  reduce = 0%, Cumulative CPU 164.2 sec
2023-01-05 07:04:23,123 Stage-43 map = 66%,  reduce = 0%, Cumulative CPU 211.34 sec
2023-01-05 07:04:23,586 Stage-51 map = 84%,  reduce = 0%, Cumulative CPU 206.75 sec
2023-01-05 07:04:24,610 Stage-51 map = 87%,  reduce = 0%, Cumulative CPU 210.6 sec
2023-01-05 07:04:26,383 Stage-43 map = 68%,  reduce = 0%, Cumulative CPU 217.13 sec
2023-01-05 07:04:28,036 Stage-59 map = 61%,  reduce = 0%, Cumulative CPU 167.69 sec
2023-01-05 07:04:28,698 Stage-51 map = 88%,  reduce = 0%, Cumulative CPU 211.5 sec
2023-01-05 07:04:29,062 Stage-59 map = 62%,  reduce = 0%, Cumulative CPU 170.56 sec
2023-01-05 07:04:29,441 Stage-43 map = 69%,  reduce = 0%, Cumulative CPU 219.82 sec
2023-01-05 07:04:29,718 Stage-51 map = 90%,  reduce = 0%, Cumulative CPU 217.02 sec
2023-01-05 07:04:32,527 Stage-43 map = 70%,  reduce = 0%, Cumulative CPU 225.87 sec
2023-01-05 07:04:34,175 Stage-59 map = 65%,  reduce = 0%, Cumulative CPU 174.37 sec
2023-01-05 07:04:35,195 Stage-59 map = 66%,  reduce = 0%, Cumulative CPU 177.92 sec
2023-01-05 07:04:35,596 Stage-43 map = 71%,  reduce = 0%, Cumulative CPU 229.57 sec
2023-01-05 07:04:35,853 Stage-51 map = 91%,  reduce = 0%, Cumulative CPU 223.18 sec
2023-01-05 07:04:38,654 Stage-43 map = 74%,  reduce = 0%, Cumulative CPU 236.68 sec
2023-01-05 07:04:40,295 Stage-59 map = 69%,  reduce = 0%, Cumulative CPU 181.01 sec
2023-01-05 07:04:41,331 Stage-59 map = 70%,  reduce = 0%, Cumulative CPU 183.76 sec
2023-01-05 07:04:41,976 Stage-51 map = 93%,  reduce = 0%, Cumulative CPU 228.74 sec
2023-01-05 07:04:44,791 Stage-43 map = 77%,  reduce = 0%, Cumulative CPU 246.44 sec
2023-01-05 07:04:46,429 Stage-59 map = 75%,  reduce = 0%, Cumulative CPU 189.13 sec
2023-01-05 07:04:46,827 Stage-43 map = 78%,  reduce = 0%, Cumulative CPU 249.54 sec
2023-01-05 07:04:48,106 Stage-51 map = 94%,  reduce = 0%, Cumulative CPU 233.85 sec
2023-01-05 07:04:50,089 Stage-43 map = 80%,  reduce = 0%, Cumulative CPU 255.31 sec
2023-01-05 07:04:52,610 Stage-59 map = 79%,  reduce = 0%, Cumulative CPU 195.81 sec
2023-01-05 07:04:53,172 Stage-43 map = 81%,  reduce = 0%, Cumulative CPU 258.15 sec
2023-01-05 07:04:54,220 Stage-51 map = 96%,  reduce = 0%, Cumulative CPU 239.04 sec
2023-01-05 07:04:56,252 Stage-43 map = 84%,  reduce = 0%, Cumulative CPU 265.39 sec
2023-01-05 07:04:58,733 Stage-59 map = 82%,  reduce = 0%, Cumulative CPU 203.08 sec
2023-01-05 07:04:59,360 Stage-51 map = 97%,  reduce = 0%, Cumulative CPU 241.53 sec
2023-01-05 07:05:02,387 Stage-43 map = 88%,  reduce = 0%, Cumulative CPU 275.07 sec
2023-01-05 07:05:04,857 Stage-59 map = 83%,  reduce = 0%, Cumulative CPU 207.71 sec
2023-01-05 07:05:05,460 Stage-43 map = 89%,  reduce = 0%, Cumulative CPU 279.6 sec
2023-01-05 07:05:05,495 Stage-51 map = 98%,  reduce = 0%, Cumulative CPU 246.71 sec
2023-01-05 07:05:06,516 Stage-51 map = 99%,  reduce = 0%, Cumulative CPU 249.3 sec
2023-01-05 07:05:08,526 Stage-43 map = 90%,  reduce = 0%, Cumulative CPU 282.18 sec
2023-01-05 07:05:09,578 Stage-51 map = 100%,  reduce = 0%, Cumulative CPU 250.91 sec
2023-01-05 07:05:10,962 Stage-59 map = 84%,  reduce = 0%, Cumulative CPU 212.35 sec
2023-01-05 07:05:11,575 Stage-43 map = 91%,  reduce = 0%, Cumulative CPU 285.62 sec
MapReduce Total cumulative CPU time: 4 minutes 10 seconds 910 msec
Ended Job = job_1672890466700_0029
Stage-76 is selected by condition resolver.
Stage-77 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
2023-01-05 07:05:14,645 Stage-43 map = 94%,  reduce = 0%, Cumulative CPU 288.71 sec
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 07:05:16,741 Stage-43 map = 95%,  reduce = 0%, Cumulative CPU 291.79 sec
2023-01-05 07:05:17,102 Stage-59 map = 85%,  reduce = 0%, Cumulative CPU 217.12 sec
2023-01-05 07:05:20,835 Stage-43 map = 97%,  reduce = 0%, Cumulative CPU 294.63 sec
2023-01-05 07:05:22,917 Stage-43 map = 98%,  reduce = 0%, Cumulative CPU 296.79 sec
2023-01-05 07:05:23,418 Stage-59 map = 86%,  reduce = 0%, Cumulative CPU 220.45 sec
2023-01-05 07:05:23,931 Stage-43 map = 99%,  reduce = 0%, Cumulative CPU 297.49 sec
Hadoop job information for Stage-47: number of mappers: 4; number of reducers: 0
2023-01-05 07:05:24,953 Stage-47 map = 0%,  reduce = 0%
2023-01-05 07:05:27,069 Stage-43 map = 100%,  reduce = 0%, Cumulative CPU 298.9 sec
MapReduce Total cumulative CPU time: 4 minutes 58 seconds 900 msec
Ended Job = job_1672890466700_0028
Stage-70 is selected by condition resolver.
Stage-71 is filtered out by condition resolver.
Stage-2 is filtered out by condition resolver.

2023-01-05 07:05:34	Processing rows:	200000	Hashtable size:	199999	Memory usage:	220275968	percentage:	0.921
Hive Runtime Error: Map local work exhausted memory
2023-01-05 07:05:35,054 Stage-59 map = 87%,  reduce = 0%, Cumulative CPU 225.38 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-76

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 9 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Hadoop job information for Stage-55: number of mappers: 3; number of reducers: 0
2023-01-05 07:05:39,141 Stage-55 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0031, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0031/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0031
2023-01-05 07:05:41,431 Stage-59 map = 88%,  reduce = 0%, Cumulative CPU 227.47 sec
2023-01-05 07:05:42,370 Stage-47 map = 1%,  reduce = 0%, Cumulative CPU 9.39 sec
2023-01-05 07:05:43,440 Stage-47 map = 4%,  reduce = 0%, Cumulative CPU 26.0 sec
2023-01-05 07:05:44,468 Stage-47 map = 5%,  reduce = 0%, Cumulative CPU 33.14 sec
2023-01-05 07:05:46,652 Stage-59 map = 89%,  reduce = 0%, Cumulative CPU 230.04 sec
2023-01-05 07:05:48,925 Stage-47 map = 8%,  reduce = 0%, Cumulative CPU 39.2 sec
2023-01-05 07:05:53,332 Stage-59 map = 90%,  reduce = 0%, Cumulative CPU 232.71 sec
2023-01-05 07:05:53	Processing rows:	200000	Hashtable size:	199999	Memory usage:	233822320	percentage:	0.978
2023-01-05 07:05:54,421 Stage-47 map = 9%,  reduce = 0%, Cumulative CPU 42.61 sec
2023-01-05 07:05:54,436 Stage-55 map = 2%,  reduce = 0%, Cumulative CPU 10.68 sec
2023-01-05 07:05:55,436 Stage-47 map = 11%,  reduce = 0%, Cumulative CPU 46.04 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-70

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 10 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:05:58,472 Stage-59 map = 91%,  reduce = 0%, Cumulative CPU 234.76 sec
2023-01-05 07:05:59,503 Stage-47 map = 12%,  reduce = 0%, Cumulative CPU 48.76 sec
2023-01-05 07:05:59,554 Stage-55 map = 5%,  reduce = 0%, Cumulative CPU 31.34 sec
Starting Job = job_1672890466700_0032, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0032/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0032
2023-01-05 07:06:01,540 Stage-47 map = 14%,  reduce = 0%, Cumulative CPU 51.6 sec
2023-01-05 07:06:04,556 Stage-59 map = 93%,  reduce = 0%, Cumulative CPU 237.52 sec
2023-01-05 07:06:05,607 Stage-47 map = 16%,  reduce = 0%, Cumulative CPU 55.11 sec
2023-01-05 07:06:05,664 Stage-55 map = 8%,  reduce = 0%, Cumulative CPU 39.34 sec
2023-01-05 07:06:07,641 Stage-47 map = 17%,  reduce = 0%, Cumulative CPU 58.85 sec
2023-01-05 07:06:08,656 Stage-47 map = 18%,  reduce = 0%, Cumulative CPU 61.28 sec
2023-01-05 07:06:10,662 Stage-59 map = 94%,  reduce = 0%, Cumulative CPU 239.67 sec
2023-01-05 07:06:11,714 Stage-47 map = 19%,  reduce = 0%, Cumulative CPU 62.95 sec
2023-01-05 07:06:12,006 Stage-55 map = 11%,  reduce = 0%, Cumulative CPU 47.07 sec
2023-01-05 07:06:13,752 Stage-47 map = 21%,  reduce = 0%, Cumulative CPU 67.24 sec
2023-01-05 07:06:14,771 Stage-47 map = 22%,  reduce = 0%, Cumulative CPU 69.44 sec
2023-01-05 07:06:16,760 Stage-59 map = 96%,  reduce = 0%, Cumulative CPU 242.21 sec
2023-01-05 07:06:17,823 Stage-47 map = 23%,  reduce = 0%, Cumulative CPU 70.9 sec
2023-01-05 07:06:18,356 Stage-55 map = 14%,  reduce = 0%, Cumulative CPU 54.64 sec
2023-01-05 07:06:18,835 Stage-47 map = 24%,  reduce = 0%, Cumulative CPU 72.91 sec
2023-01-05 07:06:19,850 Stage-47 map = 26%,  reduce = 0%, Cumulative CPU 77.25 sec
2023-01-05 07:06:22,845 Stage-59 map = 97%,  reduce = 0%, Cumulative CPU 244.13 sec
2023-01-05 07:06:23,448 Stage-55 map = 15%,  reduce = 0%, Cumulative CPU 57.53 sec
2023-01-05 07:06:23,922 Stage-47 map = 27%,  reduce = 0%, Cumulative CPU 79.15 sec
2023-01-05 07:06:24,463 Stage-55 map = 17%,  reduce = 0%, Cumulative CPU 61.55 sec
2023-01-05 07:06:24,940 Stage-47 map = 29%,  reduce = 0%, Cumulative CPU 83.4 sec
2023-01-05 07:06:25,958 Stage-47 map = 30%,  reduce = 0%, Cumulative CPU 85.94 sec
2023-01-05 07:06:28,965 Stage-59 map = 98%,  reduce = 0%, Cumulative CPU 248.48 sec
2023-01-05 07:06:29,790 Stage-55 map = 20%,  reduce = 0%, Cumulative CPU 69.82 sec
2023-01-05 07:06:31,071 Stage-47 map = 32%,  reduce = 0%, Cumulative CPU 92.82 sec
2023-01-05 07:06:35,068 Stage-59 map = 99%,  reduce = 0%, Cumulative CPU 252.43 sec
2023-01-05 07:06:35,911 Stage-55 map = 23%,  reduce = 0%, Cumulative CPU 79.9 sec
2023-01-05 07:06:36,183 Stage-47 map = 33%,  reduce = 0%, Cumulative CPU 97.69 sec
2023-01-05 07:06:37,204 Stage-47 map = 35%,  reduce = 0%, Cumulative CPU 102.52 sec
2023-01-05 07:06:38,220 Stage-47 map = 36%,  reduce = 0%, Cumulative CPU 105.03 sec
2023-01-05 07:06:42,230 Stage-55 map = 26%,  reduce = 0%, Cumulative CPU 88.99 sec
2023-01-05 07:06:43,311 Stage-47 map = 39%,  reduce = 0%, Cumulative CPU 112.37 sec
2023-01-05 07:06:44,213 Stage-59 map = 100%,  reduce = 0%, Cumulative CPU 258.17 sec
MapReduce Total cumulative CPU time: 4 minutes 18 seconds 170 msec
Ended Job = job_1672890466700_0026
Stage-82 is selected by condition resolver.
Stage-83 is filtered out by condition resolver.
Stage-24 is filtered out by condition resolver.
2023-01-05 07:06:48,345 Stage-55 map = 28%,  reduce = 0%, Cumulative CPU 98.44 sec
2023-01-05 07:06:48,389 Stage-47 map = 40%,  reduce = 0%, Cumulative CPU 116.17 sec
2023-01-05 07:06:49,439 Stage-47 map = 42%,  reduce = 0%, Cumulative CPU 121.05 sec
2023-01-05 07:06:50,454 Stage-47 map = 43%,  reduce = 0%, Cumulative CPU 122.65 sec
2023-01-05 07:06:53,514 Stage-55 map = 29%,  reduce = 0%, Cumulative CPU 100.61 sec
2023-01-05 07:06:54,521 Stage-47 map = 44%,  reduce = 0%, Cumulative CPU 124.26 sec
2023-01-05 07:06:54,530 Stage-55 map = 31%,  reduce = 0%, Cumulative CPU 105.59 sec
2023-01-05 07:06:55,628 Stage-47 map = 46%,  reduce = 0%, Cumulative CPU 127.03 sec


2023-01-05 07:06:56,646 Stage-47 map = 47%,  reduce = 0%, Cumulative CPU 128.55 sec
2023-01-05 07:06:59,731 Stage-55 map = 34%,  reduce = 0%, Cumulative CPU 111.68 sec
2023-01-05 07:07:01,915 Stage-47 map = 50%,  reduce = 0%, Cumulative CPU 135.44 sec
Hadoop job information for Stage-63: number of mappers: 2; number of reducers: 0
2023-01-05 07:07:02,324 Stage-63 map = 0%,  reduce = 0%
2023-01-05 07:07:06,166 Stage-55 map = 36%,  reduce = 0%, Cumulative CPU 116.96 sec
2023-01-05 07:07:06,223 Stage-47 map = 51%,  reduce = 0%, Cumulative CPU 136.99 sec


2023-01-05 07:07:07,333 Stage-47 map = 53%,  reduce = 0%, Cumulative CPU 139.91 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-82

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 12 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:07:12,419 Stage-55 map = 39%,  reduce = 0%, Cumulative CPU 123.49 sec
2023-01-05 07:07:12,443 Stage-47 map = 54%,  reduce = 0%, Cumulative CPU 143.29 sec
2023-01-05 07:07:13,465 Stage-47 map = 55%,  reduce = 0%, Cumulative CPU 146.52 sec
2023-01-05 07:07:14,492 Stage-47 map = 56%,  reduce = 0%, Cumulative CPU 147.95 sec
Starting Job = job_1672890466700_0033, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0033/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0033
2023-01-05 07:07:17,540 Stage-47 map = 57%,  reduce = 0%, Cumulative CPU 149.7 sec
2023-01-05 07:07:18,539 Stage-55 map = 41%,  reduce = 0%, Cumulative CPU 128.54 sec
2023-01-05 07:07:19,294 Stage-63 map = 1%,  reduce = 0%, Cumulative CPU 11.26 sec
2023-01-05 07:07:19,566 Stage-47 map = 58%,  reduce = 0%, Cumulative CPU 153.85 sec
2023-01-05 07:07:20,326 Stage-63 map = 3%,  reduce = 0%, Cumulative CPU 24.42 sec
2023-01-05 07:07:20,585 Stage-47 map = 59%,  reduce = 0%, Cumulative CPU 155.62 sec
2023-01-05 07:07:23,650 Stage-47 map = 60%,  reduce = 0%, Cumulative CPU 157.84 sec
2023-01-05 07:07:23,653 Stage-55 map = 44%,  reduce = 0%, Cumulative CPU 134.88 sec
2023-01-05 07:07:25,658 Stage-63 map = 4%,  reduce = 0%, Cumulative CPU 29.32 sec
2023-01-05 07:07:25,684 Stage-47 map = 62%,  reduce = 0%, Cumulative CPU 161.6 sec
2023-01-05 07:07:26,698 Stage-47 map = 63%,  reduce = 0%, Cumulative CPU 163.33 sec
2023-01-05 07:07:26,876 Stage-63 map = 6%,  reduce = 0%, Cumulative CPU 33.4 sec
2023-01-05 07:07:29,753 Stage-47 map = 64%,  reduce = 0%, Cumulative CPU 165.15 sec
2023-01-05 07:07:29,969 Stage-55 map = 47%,  reduce = 0%, Cumulative CPU 141.29 sec
2023-01-05 07:07:30,945 Stage-63 map = 7%,  reduce = 0%, Cumulative CPU 36.62 sec
2023-01-05 07:07:31,781 Stage-47 map = 65%,  reduce = 0%, Cumulative CPU 168.32 sec
2023-01-05 07:07:32,794 Stage-47 map = 66%,  reduce = 0%, Cumulative CPU 169.73 sec
2023-01-05 07:07:32,974 Stage-63 map = 9%,  reduce = 0%, Cumulative CPU 39.98 sec
2023-01-05 07:07:35,846 Stage-47 map = 67%,  reduce = 0%, Cumulative CPU 171.35 sec
2023-01-05 07:07:36,066 Stage-55 map = 50%,  reduce = 0%, Cumulative CPU 146.81 sec
2023-01-05 07:07:36,867 Stage-47 map = 68%,  reduce = 0%, Cumulative CPU 172.83 sec
2023-01-05 07:07:37,879 Stage-47 map = 70%,  reduce = 0%, Cumulative CPU 175.61 sec
2023-01-05 07:07:38,277 Stage-63 map = 12%,  reduce = 0%, Cumulative CPU 45.29 sec
2023-01-05 07:07:41,931 Stage-47 map = 71%,  reduce = 0%, Cumulative CPU 176.87 sec
2023-01-05 07:07:42,370 Stage-55 map = 53%,  reduce = 0%, Cumulative CPU 151.53 sec
2023-01-05 07:07:42,943 Stage-47 map = 72%,  reduce = 0%, Cumulative CPU 178.44 sec
2023-01-05 07:07:43,959 Stage-47 map = 74%,  reduce = 0%, Cumulative CPU 181.76 sec
2023-01-05 07:07:44,796 Stage-63 map = 14%,  reduce = 0%, Cumulative CPU 51.82 sec
2023-01-05 07:07:48,030 Stage-47 map = 75%,  reduce = 0%, Cumulative CPU 182.87 sec
2023-01-05 07:07:48,486 Stage-55 map = 57%,  reduce = 0%, Cumulative CPU 156.84 sec
2023-01-05 07:07:49,050 Stage-47 map = 77%,  reduce = 0%, Cumulative CPU 185.8 sec
2023-01-05 07:07:49,066 Stage-63 map = 15%,  reduce = 0%, Cumulative CPU 55.28 sec
2023-01-05 07:07:50,061 Stage-47 map = 78%,  reduce = 0%, Cumulative CPU 188.44 sec
2023-01-05 07:07:51,114 Stage-63 map = 17%,  reduce = 0%, Cumulative CPU 57.65 sec
2023-01-05 07:07:53,588 Stage-55 map = 58%,  reduce = 0%, Cumulative CPU 158.33 sec
2023-01-05 07:07:54,135 Stage-47 map = 79%,  reduce = 0%, Cumulative CPU 189.67 sec
2023-01-05 07:07:54,602 Stage-55 map = 61%,  reduce = 0%, Cumulative CPU 163.29 sec
2023-01-05 07:07:55,147 Stage-47 map = 81%,  reduce = 0%, Cumulative CPU 192.47 sec
2023-01-05 07:07:55,204 Stage-63 map = 18%,  reduce = 0%, Cumulative CPU 60.05 sec
2023-01-05 07:07:56,218 Stage-63 map = 20%,  reduce = 0%, Cumulative CPU 62.29 sec
2023-01-05 07:07:59,882 Stage-55 map = 64%,  reduce = 0%, Cumulative CPU 169.19 sec
2023-01-05 07:08:00,238 Stage-47 map = 82%,  reduce = 0%, Cumulative CPU 196.08 sec
2023-01-05 07:08:01,269 Stage-47 map = 84%,  reduce = 0%, Cumulative CPU 199.61 sec
2023-01-05 07:08:01,324 Stage-63 map = 22%,  reduce = 0%, Cumulative CPU 65.09 sec
2023-01-05 07:08:02,281 Stage-47 map = 85%,  reduce = 0%, Cumulative CPU 201.46 sec
2023-01-05 07:08:02,339 Stage-63 map = 25%,  reduce = 0%, Cumulative CPU 67.59 sec
2023-01-05 07:08:04,309 Stage-47 map = 86%,  reduce = 0%, Cumulative CPU 202.1 sec
2023-01-05 07:08:06,001 Stage-55 map = 66%,  reduce = 0%, Cumulative CPU 174.6 sec
2023-01-05 07:08:07,351 Stage-47 map = 87%,  reduce = 0%, Cumulative CPU 205.1 sec
2023-01-05 07:08:07,444 Stage-63 map = 26%,  reduce = 0%, Cumulative CPU 70.12 sec
2023-01-05 07:08:08,368 Stage-47 map = 88%,  reduce = 0%, Cumulative CPU 207.67 sec
2023-01-05 07:08:08,465 Stage-63 map = 29%,  reduce = 0%, Cumulative CPU 72.13 sec
2023-01-05 07:08:12,317 Stage-55 map = 69%,  reduce = 0%, Cumulative CPU 182.09 sec
2023-01-05 07:08:12,434 Stage-47 map = 89%,  reduce = 0%, Cumulative CPU 209.29 sec
2023-01-05 07:08:13,446 Stage-47 map = 90%,  reduce = 0%, Cumulative CPU 211.09 sec
2023-01-05 07:08:13,554 Stage-63 map = 30%,  reduce = 0%, Cumulative CPU 74.29 sec
2023-01-05 07:08:14,779 Stage-63 map = 32%,  reduce = 0%, Cumulative CPU 76.52 sec
2023-01-05 07:08:18,411 Stage-55 map = 72%,  reduce = 0%, Cumulative CPU 189.78 sec
2023-01-05 07:08:18,519 Stage-47 map = 91%,  reduce = 0%, Cumulative CPU 215.15 sec
2023-01-05 07:08:19,060 Stage-63 map = 34%,  reduce = 0%, Cumulative CPU 79.17 sec
2023-01-05 07:08:19,531 Stage-47 map = 92%,  reduce = 0%, Cumulative CPU 217.04 sec
2023-01-05 07:08:20,543 Stage-47 map = 93%,  reduce = 0%, Cumulative CPU 219.26 sec
2023-01-05 07:08:21,097 Stage-63 map = 36%,  reduce = 0%, Cumulative CPU 83.14 sec
2023-01-05 07:08:23,614 Stage-47 map = 94%,  reduce = 0%, Cumulative CPU 220.67 sec
2023-01-05 07:08:24,524 Stage-55 map = 75%,  reduce = 0%, Cumulative CPU 198.17 sec
2023-01-05 07:08:25,180 Stage-63 map = 37%,  reduce = 0%, Cumulative CPU 85.41 sec
2023-01-05 07:08:26,678 Stage-47 map = 95%,  reduce = 0%, Cumulative CPU 224.73 sec
2023-01-05 07:08:27,222 Stage-63 map = 39%,  reduce = 0%, Cumulative CPU 89.52 sec
2023-01-05 07:08:28,701 Stage-47 map = 96%,  reduce = 0%, Cumulative CPU 226.05 sec
2023-01-05 07:08:29,808 Stage-55 map = 79%,  reduce = 0%, Cumulative CPU 205.88 sec
2023-01-05 07:08:31,285 Stage-63 map = 40%,  reduce = 0%, Cumulative CPU 91.32 sec
2023-01-05 07:08:31,738 Stage-47 map = 97%,  reduce = 0%, Cumulative CPU 227.39 sec
2023-01-05 07:08:32,749 Stage-47 map = 98%,  reduce = 0%, Cumulative CPU 230.6 sec
2023-01-05 07:08:33,335 Stage-63 map = 42%,  reduce = 0%, Cumulative CPU 95.16 sec
2023-01-05 07:08:35,904 Stage-55 map = 82%,  reduce = 0%, Cumulative CPU 213.87 sec
2023-01-05 07:08:37,420 Stage-63 map = 43%,  reduce = 0%, Cumulative CPU 98.59 sec
2023-01-05 07:08:37,817 Stage-47 map = 99%,  reduce = 0%, Cumulative CPU 231.92 sec
2023-01-05 07:08:38,437 Stage-63 map = 45%,  reduce = 0%, Cumulative CPU 102.05 sec
2023-01-05 07:08:39,841 Stage-47 map = 100%,  reduce = 0%, Cumulative CPU 235.3 sec
MapReduce Total cumulative CPU time: 3 minutes 55 seconds 300 msec
Ended Job = job_1672890466700_0025
2023-01-05 07:08:41,990 Stage-55 map = 85%,  reduce = 0%, Cumulative CPU 221.64 sec
Stage-73 is selected by condition resolver.
Stage-74 is filtered out by condition resolver.
Stage-9 is filtered out by condition resolver.
2023-01-05 07:08:43,544 Stage-63 map = 46%,  reduce = 0%, Cumulative CPU 106.33 sec
2023-01-05 07:08:44,561 Stage-63 map = 48%,  reduce = 0%, Cumulative CPU 110.53 sec


2023-01-05 07:08:48,135 Stage-55 map = 88%,  reduce = 0%, Cumulative CPU 230.6 sec
2023-01-05 07:08:49,688 Stage-63 map = 49%,  reduce = 0%, Cumulative CPU 114.6 sec
2023-01-05 07:08:50,740 Stage-63 map = 51%,  reduce = 0%, Cumulative CPU 117.51 sec
Hadoop job information for Stage-14: number of mappers: 2; number of reducers: 1
2023-01-05 07:08:52,372 Stage-14 map = 0%,  reduce = 0%
2023-01-05 07:08:54,337 Stage-55 map = 90%,  reduce = 0%, Cumulative CPU 235.45 sec
2023-01-05 07:08:55,916 Stage-63 map = 52%,  reduce = 0%, Cumulative CPU 121.62 sec
2023-01-05 07:08:56,932 Stage-63 map = 54%,  reduce = 0%, Cumulative CPU 124.08 sec
2023-01-05 07:08:59,755 Stage-55 map = 92%,  reduce = 0%, Cumulative CPU 240.79 sec
2023-01-05 07:09:01,227 Stage-63 map = 55%,  reduce = 0%, Cumulative CPU 127.78 sec

2023-01-05 07:09:01,963 Stage-14 map = 50%,  reduce = 0%, Cumulative CPU 7.92 sec
2023-01-05 07:09:03,542 Stage-63 map = 57%,  reduce = 0%, Cumulative CPU 129.73 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-73

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 14 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:09:06,085 Stage-55 map = 93%,  reduce = 0%, Cumulative CPU 244.25 sec
Starting Job = job_1672890466700_0034, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0034/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0034
2023-01-05 07:09:07,604 Stage-63 map = 58%,  reduce = 0%, Cumulative CPU 132.31 sec
2023-01-05 07:09:08,618 Stage-63 map = 59%,  reduce = 0%, Cumulative CPU 133.75 sec
2023-01-05 07:09:11,249 Stage-14 map = 66%,  reduce = 0%, Cumulative CPU 23.24 sec
2023-01-05 07:09:12,197 Stage-55 map = 95%,  reduce = 0%, Cumulative CPU 247.19 sec
2023-01-05 07:09:13,694 Stage-63 map = 60%,  reduce = 0%, Cumulative CPU 136.52 sec
2023-01-05 07:09:14,708 Stage-63 map = 63%,  reduce = 0%, Cumulative CPU 139.05 sec
2023-01-05 07:09:17,360 Stage-14 map = 83%,  reduce = 0%, Cumulative CPU 29.42 sec
2023-01-05 07:09:18,327 Stage-55 map = 98%,  reduce = 0%, Cumulative CPU 252.84 sec
2023-01-05 07:09:18,373 Stage-14 map = 100%,  reduce = 0%, Cumulative CPU 30.82 sec
2023-01-05 07:09:19,392 Stage-14 map = 100%,  reduce = 17%, Cumulative CPU 31.34 sec
2023-01-05 07:09:19,803 Stage-63 map = 64%,  reduce = 0%, Cumulative CPU 142.14 sec
2023-01-05 07:09:20,818 Stage-63 map = 67%,  reduce = 0%, Cumulative CPU 144.28 sec
2023-01-05 07:09:23,412 Stage-55 map = 100%,  reduce = 0%, Cumulative CPU 255.18 sec
MapReduce Total cumulative CPU time: 4 minutes 15 seconds 180 msec
2023-01-05 07:09:25,495 Stage-14 map = 100%,  reduce = 67%, Cumulative CPU 39.16 sec
Ended Job = job_1672890466700_0027
2023-01-05 07:09:25,891 Stage-63 map = 68%,  reduce = 0%, Cumulative CPU 146.44 sec
2023-01-05 07:09:26,904 Stage-63 map = 71%,  reduce = 0%, Cumulative CPU 148.32 sec
Stage-79 is selected by condition resolver.
Stage-80 is filtered out by condition resolver.
Stage-19 is filtered out by condition resolver.
2023-01-05 07:09:28,929 Stage-63 map = 72%,  reduce = 0%, Cumulative CPU 148.98 sec


2023-01-05 07:09:31,584 Stage-14 map = 100%,  reduce = 72%, Cumulative CPU 49.14 sec
2023-01-05 07:09:31,970 Stage-63 map = 74%,  reduce = 0%, Cumulative CPU 152.28 sec
2023-01-05 07:09:34	Starting to launch local task to process map join;	maximum memory = 239075328
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2023-01-05 07:09:35,538 Stage-2 map = 0%,  reduce = 0%
2023-01-05 07:09:37,816 Stage-14 map = 100%,  reduce = 75%, Cumulative CPU 53.52 sec
2023-01-05 07:09:43,213 Stage-14 map = 100%,  reduce = 80%, Cumulative CPU 60.04 sec
2023-01-05 07:09:43,734 Stage-63 map = 75%,  reduce = 0%, Cumulative CPU 156.85 sec

Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-79

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 16 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:09:47,044 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 7.72 sec
Starting Job = job_1672890466700_0035, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0035/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0035
2023-01-05 07:09:49,074 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 20.87 sec
2023-01-05 07:09:49,369 Stage-14 map = 100%,  reduce = 88%, Cumulative CPU 68.05 sec
2023-01-05 07:09:49,894 Stage-63 map = 76%,  reduce = 0%, Cumulative CPU 159.46 sec
2023-01-05 07:09:55,455 Stage-14 map = 100%,  reduce = 91%, Cumulative CPU 73.12 sec
2023-01-05 07:09:55,990 Stage-63 map = 78%,  reduce = 0%, Cumulative CPU 162.32 sec
2023-01-05 07:10:01,541 Stage-14 map = 100%,  reduce = 94%, Cumulative CPU 78.48 sec
2023-01-05 07:10:02,068 Stage-63 map = 79%,  reduce = 0%, Cumulative CPU 165.53 sec
2023-01-05 07:10:07,133 Stage-63 map = 80%,  reduce = 0%, Cumulative CPU 168.07 sec
2023-01-05 07:10:07,619 Stage-14 map = 100%,  reduce = 95%, Cumulative CPU 82.03 sec
2023-01-05 07:10:09,365 Stage-2 map = 100%,  reduce = 67%, Cumulative CPU 34.19 sec
2023-01-05 07:10:13,209 Stage-63 map = 81%,  reduce = 0%, Cumulative CPU 170.44 sec
2023-01-05 07:10:13,699 Stage-14 map = 100%,  reduce = 98%, Cumulative CPU 86.92 sec
2023-01-05 07:10:15,445 Stage-2 map = 100%,  reduce = 72%, Cumulative CPU 40.66 sec
2023-01-05 07:10:20,790 Stage-14 map = 100%,  reduce = 100%, Cumulative CPU 98.13 sec
2023-01-05 07:10:21,530 Stage-2 map = 100%,  reduce = 75%, Cumulative CPU 43.61 sec
MapReduce Total cumulative CPU time: 1 minutes 40 seconds 800 msec
Ended Job = job_1672890466700_0031
Launching Job 17 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:10:25,395 Stage-63 map = 82%,  reduce = 0%, Cumulative CPU 175.4 sec
Starting Job = job_1672890466700_0036, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0036/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0036
2023-01-05 07:10:26,628 Stage-2 map = 100%,  reduce = 84%, Cumulative CPU 49.54 sec
2023-01-05 07:10:32,711 Stage-2 map = 100%,  reduce = 96%, Cumulative CPU 57.39 sec
Hadoop job information for Stage-24: number of mappers: 2; number of reducers: 1
2023-01-05 07:10:36,410 Stage-24 map = 0%,  reduce = 0%
2023-01-05 07:10:37,583 Stage-63 map = 83%,  reduce = 0%, Cumulative CPU 180.26 sec
2023-01-05 07:10:38,804 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 65.71 sec
MapReduce Total cumulative CPU time: 1 minutes 5 seconds 710 msec
Ended Job = job_1672890466700_0032
Launching Job 18 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0037, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0037/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0037
2023-01-05 07:10:45,553 Stage-24 map = 50%,  reduce = 0%, Cumulative CPU 8.99 sec
2023-01-05 07:10:48,616 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 18.9 sec
2023-01-05 07:10:49,764 Stage-63 map = 84%,  reduce = 0%, Cumulative CPU 186.99 sec
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2023-01-05 07:10:50,870 Stage-9 map = 0%,  reduce = 0%
2023-01-05 07:10:56,065 Stage-63 map = 85%,  reduce = 0%, Cumulative CPU 191.54 sec
2023-01-05 07:10:58,999 Stage-9 map = 50%,  reduce = 0%, Cumulative CPU 7.07 sec
2023-01-05 07:11:02,347 Stage-63 map = 86%,  reduce = 0%, Cumulative CPU 195.18 sec
2023-01-05 07:11:03,833 Stage-24 map = 100%,  reduce = 88%, Cumulative CPU 34.27 sec
2023-01-05 07:11:09,129 Stage-9 map = 83%,  reduce = 0%, Cumulative CPU 21.52 sec
2023-01-05 07:11:09,919 Stage-24 map = 100%,  reduce = 100%, Cumulative CPU 41.77 sec
MapReduce Total cumulative CPU time: 41 seconds 770 msec
Ended Job = job_1672890466700_0033
Launching Job 19 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:11:13,503 Stage-63 map = 88%,  reduce = 0%, Cumulative CPU 201.77 sec
2023-01-05 07:11:14,190 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 26.24 sec
Starting Job = job_1672890466700_0038, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0038/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0038
2023-01-05 07:11:18,244 Stage-9 map = 100%,  reduce = 67%, Cumulative CPU 29.81 sec
2023-01-05 07:11:19,629 Stage-63 map = 89%,  reduce = 0%, Cumulative CPU 204.08 sec
2023-01-05 07:11:24,315 Stage-9 map = 100%,  reduce = 69%, Cumulative CPU 38.28 sec
Hadoop job information for Stage-19: number of mappers: 2; number of reducers: 1
2023-01-05 07:11:25,075 Stage-19 map = 0%,  reduce = 0%
2023-01-05 07:11:26,048 Stage-63 map = 91%,  reduce = 0%, Cumulative CPU 208.39 sec
2023-01-05 07:11:30,388 Stage-9 map = 100%,  reduce = 73%, Cumulative CPU 44.05 sec
2023-01-05 07:11:32,154 Stage-63 map = 93%,  reduce = 0%, Cumulative CPU 212.33 sec
2023-01-05 07:11:36,256 Stage-19 map = 50%,  reduce = 0%, Cumulative CPU 8.58 sec
2023-01-05 07:11:36,464 Stage-9 map = 100%,  reduce = 77%, Cumulative CPU 50.39 sec
2023-01-05 07:11:38,231 Stage-63 map = 94%,  reduce = 0%, Cumulative CPU 215.03 sec
2023-01-05 07:11:40,322 Stage-19 map = 100%,  reduce = 0%, Cumulative CPU 22.45 sec
2023-01-05 07:11:42,542 Stage-9 map = 100%,  reduce = 82%, Cumulative CPU 57.58 sec
2023-01-05 07:11:43,299 Stage-63 map = 96%,  reduce = 0%, Cumulative CPU 218.74 sec
2023-01-05 07:11:48,648 Stage-9 map = 100%,  reduce = 87%, Cumulative CPU 64.65 sec
2023-01-05 07:11:49,396 Stage-63 map = 98%,  reduce = 0%, Cumulative CPU 223.03 sec
2023-01-05 07:11:54,720 Stage-9 map = 100%,  reduce = 92%, Cumulative CPU 71.8 sec
2023-01-05 07:11:55,478 Stage-63 map = 99%,  reduce = 0%, Cumulative CPU 228.58 sec
2023-01-05 07:11:55,561 Stage-19 map = 100%,  reduce = 76%, Cumulative CPU 38.82 sec
2023-01-05 07:12:00,806 Stage-9 map = 100%,  reduce = 97%, Cumulative CPU 79.11 sec
2023-01-05 07:12:01,659 Stage-19 map = 100%,  reduce = 85%, Cumulative CPU 45.59 sec
2023-01-05 07:12:02,579 Stage-63 map = 100%,  reduce = 0%, Cumulative CPU 234.4 sec
MapReduce Total cumulative CPU time: 3 minutes 54 seconds 400 msec
Ended Job = job_1672890466700_0030
Stage-85 is selected by condition resolver.
Stage-86 is filtered out by condition resolver.
Stage-29 is filtered out by condition resolver.
2023-01-05 07:12:05,872 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 87.4 sec
MapReduce Total cumulative CPU time: 1 minutes 27 seconds 400 msec
Ended Job = job_1672890466700_0034
Launching Job 21 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:12:07,826 Stage-19 map = 100%,  reduce = 94%, Cumulative CPU 52.24 sec
Starting Job = job_1672890466700_0039, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0039/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0039
2023-01-05 07:12:13,938 Stage-19 map = 100%,  reduce = 100%, Cumulative CPU 59.99 sec
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 07:12:14	Starting to launch local task to process map join;	maximum memory = 239075328
MapReduce Total cumulative CPU time: 1 minutes 0 seconds 830 msec
Ended Job = job_1672890466700_0035
Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 1
2023-01-05 07:12:16,302 Stage-15 map = 0%,  reduce = 0%
Launching Job 22 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2023-01-05 07:12:17,416 Stage-3 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0040, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0040/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0040
Hive Runtime Error: Map local work exhausted memory
2023-01-05 07:12:25,919 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 5.32 sec
2023-01-05 07:12:26,928 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 6.38 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-85

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 23 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Hadoop job information for Stage-25: number of mappers: 1; number of reducers: 1
2023-01-05 07:12:27,489 Stage-25 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0041, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0041/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0041
2023-01-05 07:12:33,060 Stage-15 map = 100%,  reduce = 100%, Cumulative CPU 10.23 sec
2023-01-05 07:12:33,064 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 10.01 sec
MapReduce Total cumulative CPU time: 10 seconds 230 msec
Ended Job = job_1672890466700_0036
MapReduce Total cumulative CPU time: 10 seconds 10 msec
Ended Job = job_1672890466700_0037
2023-01-05 07:12:35,819 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 3.7 sec
2023-01-05 07:12:40,925 Stage-25 map = 100%,  reduce = 100%, Cumulative CPU 5.97 sec
MapReduce Total cumulative CPU time: 5 seconds 970 msec
Ended Job = job_1672890466700_0038
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 1
2023-01-05 07:12:44,632 Stage-10 map = 0%,  reduce = 0%
Hadoop job information for Stage-20: number of mappers: 1; number of reducers: 1
2023-01-05 07:12:44,833 Stage-20 map = 0%,  reduce = 0%
2023-01-05 07:12:57,238 Stage-20 map = 100%,  reduce = 0%, Cumulative CPU 6.44 sec
Hadoop job information for Stage-29: number of mappers: 2; number of reducers: 1
2023-01-05 07:12:58,776 Stage-29 map = 0%,  reduce = 0%
2023-01-05 07:12:58,906 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 7.64 sec
2023-01-05 07:13:07,143 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 14.47 sec
MapReduce Total cumulative CPU time: 14 seconds 470 msec
Ended Job = job_1672890466700_0039
2023-01-05 07:13:10,167 Stage-29 map = 50%,  reduce = 0%, Cumulative CPU 8.19 sec
2023-01-05 07:13:13,517 Stage-20 map = 100%,  reduce = 100%, Cumulative CPU 15.49 sec
2023-01-05 07:13:14,226 Stage-29 map = 100%,  reduce = 0%, Cumulative CPU 18.7 sec
MapReduce Total cumulative CPU time: 15 seconds 490 msec
Ended Job = job_1672890466700_0040
2023-01-05 07:13:25,394 Stage-29 map = 100%,  reduce = 89%, Cumulative CPU 33.89 sec
2023-01-05 07:13:31,485 Stage-29 map = 100%,  reduce = 100%, Cumulative CPU 40.4 sec
MapReduce Total cumulative CPU time: 40 seconds 410 msec
Ended Job = job_1672890466700_0041
Launching Job 24 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0042, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0042/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0042
Hadoop job information for Stage-30: number of mappers: 1; number of reducers: 1
2023-01-05 07:13:41,519 Stage-30 map = 0%,  reduce = 0%
2023-01-05 07:13:48,615 Stage-30 map = 100%,  reduce = 0%, Cumulative CPU 3.41 sec
2023-01-05 07:13:55,760 Stage-30 map = 100%,  reduce = 100%, Cumulative CPU 8.48 sec
MapReduce Total cumulative CPU time: 8 seconds 480 msec
Ended Job = job_1672890466700_0042
Stage-64 is filtered out by condition resolver.
Stage-65 is filtered out by condition resolver.
Stage-66 is filtered out by condition resolver.
Stage-67 is filtered out by condition resolver.
Stage-68 is filtered out by condition resolver.
Stage-69 is filtered out by condition resolver.
Stage-4 is selected by condition resolver.
Launching Job 25 out of 38
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0043, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0043/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0043
Hadoop job information for Stage-4: number of mappers: 6; number of reducers: 1
2023-01-05 07:14:05,181 Stage-4 map = 0%,  reduce = 0%
2023-01-05 07:14:17,352 Stage-4 map = 17%,  reduce = 0%, Cumulative CPU 3.78 sec
2023-01-05 07:14:18,364 Stage-4 map = 33%,  reduce = 0%, Cumulative CPU 8.15 sec
2023-01-05 07:14:20,389 Stage-4 map = 67%,  reduce = 0%, Cumulative CPU 15.79 sec
2023-01-05 07:14:21,401 Stage-4 map = 83%,  reduce = 0%, Cumulative CPU 20.45 sec
2023-01-05 07:14:22,415 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 25.62 sec
2023-01-05 07:14:26,465 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 32.72 sec
MapReduce Total cumulative CPU time: 32 seconds 720 msec
Ended Job = job_1672890466700_0043
Launching Job 26 out of 38
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0044, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0044/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0044
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 07:14:35,321 Stage-5 map = 0%,  reduce = 0%
2023-01-05 07:14:39,392 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.07 sec
2023-01-05 07:14:43,460 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.2 sec
MapReduce Total cumulative CPU time: 2 seconds 200 msec
Ended Job = job_1672890466700_0044
MapReduce Jobs Launched:
Stage-Stage-51: Map: 4   Cumulative CPU: 250.91 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-43: Map: 3   Cumulative CPU: 298.9 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-59: Map: 2   Cumulative CPU: 258.17 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-47: Map: 4   Cumulative CPU: 235.3 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-55: Map: 3   Cumulative CPU: 255.18 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-14: Map: 2  Reduce: 1   Cumulative CPU: 100.8 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 65.71 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-24: Map: 2  Reduce: 1   Cumulative CPU: 41.77 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-63: Map: 2   Cumulative CPU: 234.4 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-9: Map: 2  Reduce: 1   Cumulative CPU: 87.4 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 2  Reduce: 1   Cumulative CPU: 60.83 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-15: Map: 1  Reduce: 1   Cumulative CPU: 10.23 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 10.01 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-25: Map: 1  Reduce: 1   Cumulative CPU: 5.97 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-10: Map: 1  Reduce: 1   Cumulative CPU: 14.47 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-20: Map: 1  Reduce: 1   Cumulative CPU: 15.49 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-29: Map: 2  Reduce: 1   Cumulative CPU: 40.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-30: Map: 1  Reduce: 1   Cumulative CPU: 8.48 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 6  Reduce: 1   Cumulative CPU: 32.72 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.2 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 33 minutes 49 seconds 350 msec
OK
AAAAAAAAAACKGAAA	Betty               	Good                          	KAZAKHSTAN
AAAAAAAAAAHBHAAA	Patrick             	Gary                          	NULL
AAAAAAAAAAHHDAAA	Steven              	Gardner                       	SAINT HELENA
AAAAAAAAABBJFAAA	Jacob               	Palmer                        	MOZAMBIQUE
AAAAAAAAABKJBAAA	NULL	Crawford                      	MALAYSIA
AAAAAAAAABLIHAAA	Steve               	Craig                         	QATAR
AAAAAAAAABMCFAAA	Thomas              	Williamson                    	CAMEROON
AAAAAAAAACCEHAAA	NULL	Gonzales                      	NULL
AAAAAAAAACDOGAAA	Elizabeth           	Green                         	GUADELOUPE
AAAAAAAAACJIEAAA	Nakia               	Harkins                       	NEW CALEDONIA
AAAAAAAAADPKFAAA	Hilda               	Delvalle                      	MALAWI
AAAAAAAAAEDOCAAA	Evangeline          	Longo                         	POLAND
AAAAAAAAAEJLCAAA	David               	Tucker                        	NULL
AAAAAAAAAFAPFAAA	Beverly             	Ortega                        	URUGUAY
AAAAAAAAAFJGAAAA	Betty               	Campos                        	NIGERIA
AAAAAAAAAFMNBAAA	Roger               	Coleman                       	ALGERIA
AAAAAAAAAGDEBAAA	Kevin               	Burgos                        	NETHERLANDS ANTILLES
AAAAAAAAAGNGHAAA	Gene                	King                          	LATVIA
AAAAAAAAAHKDEAAA	Debra               	Webb                          	GUADELOUPE
AAAAAAAAAHOLCAAA	John                	Holmes                        	VIRGIN ISLANDS, U.S.
AAAAAAAAAIBCFAAA	Frank               	Ott                           	RWANDA
AAAAAAAAAICOCAAA	Audra               	Burke                         	NETHERLANDS
AAAAAAAAAIOKDAAA	Naomi               	Weaver                        	GAMBIA
AAAAAAAAAJCPDAAA	Donald              	Moore                         	SPAIN
AAAAAAAAAJNOAAAA	Mae                 	Camacho                       	BENIN
AAAAAAAAAKDDGAAA	Sarah               	Williamson                    	GUINEA-BISSAU
AAAAAAAAAKKGGAAA	Robert              	Pace                          	JORDAN
AAAAAAAAAKPKCAAA	Diana               	Albright                      	MALI
AAAAAAAAALIEHAAA	Dawn                	Castro                        	COMOROS
AAAAAAAAALIPAAAA	NULL	Williams                      	NULL
AAAAAAAAALOOEAAA	Briana              	Broyles                       	BELARUS
AAAAAAAAAMNHBAAA	Carl                	Mason                         	PAKISTAN
AAAAAAAAANENAAAA	NULL	Miller                        	NULL
AAAAAAAAANHPAAAA	Sharon              	Mitchell                      	LATVIA
AAAAAAAABALPCAAA	Heather             	Harper                        	CAYMAN ISLANDS
AAAAAAAABBAIAAAA	William             	Davis                         	FINLAND
AAAAAAAABCFHDAAA	Glen                	Edwards                       	INDIA
AAAAAAAABDNLAAAA	Teresa              	Newman                        	GEORGIA
AAAAAAAABFAEEAAA	NULL	NULL	BHUTAN
AAAAAAAABFCLFAAA	Betty               	Mcfarlane                     	GHANA
AAAAAAAABFKADAAA	Cheryl              	Brown                         	RUSSIAN FEDERATION
AAAAAAAABFLNAAAA	Ernestine           	Jones                         	RUSSIAN FEDERATION
AAAAAAAABGDAAAAA	Betty               	Kelly                         	NICARAGUA
AAAAAAAABGDJBAAA	Heather             	Cooley                        	GABON
AAAAAAAABGJAHAAA	Jaclyn              	Serrano                       	GUYANA
AAAAAAAABGMNFAAA	Jerome              	Mccoy                         	FIJI
AAAAAAAABHDPCAAA	Anita               	Dayton                        	TONGA
AAAAAAAABIEJAAAA	Barbara             	Mason                         	UZBEKISTAN
AAAAAAAABJELFAAA	Rose                	Smith                         	KAZAKHSTAN
AAAAAAAABJIKDAAA	William             	Dubois                        	BELARUS
AAAAAAAABJNPAAAA	Jessica             	Collins                       	MALAWI
AAAAAAAABKGECAAA	Angie               	Smith                         	PAKISTAN
AAAAAAAABLGDCAAA	James               	Hayes                         	KUWAIT
AAAAAAAABLLOAAAA	Ashley              	NULL	NULL
AAAAAAAABLOLFAAA	Geraldine           	Diamond                       	COSTA RICA
AAAAAAAABMJDCAAA	James               	Nguyen                        	MADAGASCAR
AAAAAAAABMOGDAAA	Anna                	Pennington                    	DENMARK
AAAAAAAABNFNAAAA	Wayne               	Rodriguez                     	SOLOMON ISLANDS
AAAAAAAABNGIEAAA	Edward              	Nathan                        	NIGERIA
AAAAAAAABNIJHAAA	Ernest              	Young                         	ECUADOR
AAAAAAAABNKBDAAA	Anthony             	Herman                        	AFGHANISTAN
AAAAAAAABNKOEAAA	Randall             	Miller                        	ARGENTINA
AAAAAAAABOABFAAA	Randall             	Morgan                        	IRELAND
AAAAAAAABOHCDAAA	Tessie              	Gibson                        	CAYMAN ISLANDS
AAAAAAAABOPABAAA	Johnnie             	Garcia                        	NORFOLK ISLAND
AAAAAAAABPHBGAAA	Gary                	Bergeron                      	ARGENTINA
AAAAAAAABPLDEAAA	Dennis              	Frey                          	GHANA
AAAAAAAACAJCBAAA	Kimberley           	Rogers                        	SAINT HELENA
AAAAAAAACAJNDAAA	Pauline             	Baker                         	ERITREA
AAAAAAAACANPEAAA	Lidia               	Hughes                        	FRANCE
AAAAAAAACAOPDAAA	William             	Moss                          	BURKINA FASO
AAAAAAAACAPNBAAA	Ashley              	Angelo                        	BULGARIA
AAAAAAAACBBGDAAA	Bradley             	Lockwood                      	BELIZE
AAAAAAAACBCJHAAA	Karen               	Livingston                    	MALAWI
AAAAAAAACBFOEAAA	NULL	NULL	CZECH REPUBLIC
AAAAAAAACBNKEAAA	Loren               	Labonte                       	ANDORRA
AAAAAAAACBNLCAAA	Joyce               	Johnson                       	COMOROS
AAAAAAAACBOFCAAA	Clara               	Smith                         	MALTA
AAAAAAAACBPBHAAA	William             	Lewis                         	ARMENIA
AAAAAAAACCHMFAAA	William             	Brady                         	MONTSERRAT
AAAAAAAACCJABAAA	James               	Wood                          	EGYPT
AAAAAAAACCKLEAAA	Mark                	Fields                        	ERITREA
AAAAAAAACEBIDAAA	Sha                 	Prichard                      	BANGLADESH
AAAAAAAACEIKGAAA	Timothy             	Tobin                         	SOLOMON ISLANDS
AAAAAAAACEKHDAAA	Nora                	Lewis                         	NAURU
AAAAAAAACEMFBAAA	Jonna               	Mattson                       	CAMBODIA
AAAAAAAACFFFHAAA	Myra                	West                          	BANGLADESH
AAAAAAAACFJBBAAA	Celeste             	Sanders                       	TONGA
AAAAAAAACFOIAAAA	Willie              	Rogers                        	ESTONIA
AAAAAAAACGENGAAA	Kimberly            	Peak                          	PUERTO RICO
AAAAAAAACGHKFAAA	Juanita             	Bryson                        	MOROCCO
AAAAAAAACGMNDAAA	Janet               	Blackman                      	SYRIAN ARAB REPUBLIC
AAAAAAAACGOJFAAA	Alisha              	Thomas                        	KUWAIT
AAAAAAAACHAOAAAA	Teresa              	Hewitt                        	SIERRA LEONE
AAAAAAAACIAHEAAA	Craig               	Mann                          	ANDORRA
AAAAAAAACIFACAAA	Joel                	Miller                        	KYRGYZSTAN
AAAAAAAACIFHBAAA	Linda               	Holloway                      	SENEGAL
AAAAAAAACIGMEAAA	James               	Reyes                         	MOROCCO
AAAAAAAACIHNGAAA	Tim                 	Whitman                       	JORDAN
AAAAAAAACIJJCAAA	Hector              	Nunez                         	ISRAEL
Time taken: 870.33 seconds, Fetched: 100 row(s)
timediff:875.928651854
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query5.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 3b100c90-0c5c-44b2-ae85-9bd8cad358e9

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 2360a17f-a342-4f73-ac82-05a8b26be796
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_ext_sales_price, ss_net_profit, ss_store_sk
No Stats for tpcds_bin_partitioned_orc_10@store_returns, Columns: sr_return_amt, sr_net_loss, sr_store_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_store_id, s_store_sk
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_catalog_page_sk, cs_net_profit, cs_ext_sales_price
No Stats for tpcds_bin_partitioned_orc_10@catalog_returns, Columns: cr_net_loss, cr_return_amount, cr_catalog_page_sk
No Stats for tpcds_bin_partitioned_orc_10@catalog_page, Columns: cp_catalog_page_sk, cp_catalog_page_id
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_ext_sales_price, ws_web_site_sk, ws_net_profit
No Stats for tpcds_bin_partitioned_orc_10@web_returns, Columns: wr_order_number, wr_net_loss, wr_return_amt, wr_item_sk
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_order_number, ws_item_sk
No Stats for tpcds_bin_partitioned_orc_10@web_site, Columns: web_site_sk, web_site_id
Query ID = hdfs_20230105071450_2c06e6b5-3a8a-4fce-b01f-6184872fc121
Total jobs = 9
Launching Job 1 out of 9
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.



2023-01-05 07:15:39	Dump the side-table for tag: 1 with group count: 102 into file: file:/tmp/hdfs/3b100c90-0c5c-44b2-ae85-9bd8cad358e9/hive_2023-01-05_07-14-50_820_7348710886379401300-1/-local-10021/HashTable-Stage-24/MapJoin-mapfile11--.hashtable
2023-01-05 07:15:39	End of local task; Time Taken: 2.619 sec.

Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 9
Number of reduce tasks is set to 0 since there's no reduce operator
Execution completed successfully
MapredLocal task succeeded
Launching Job 3 out of 9
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0045, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0045/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0045
Hadoop job information for Stage-19: number of mappers: 3; number of reducers: 2
2023-01-05 07:15:54,716 Stage-19 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0046, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0046/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0046
Starting Job = job_1672890466700_0047, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0047/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0047
Hadoop job information for Stage-24: number of mappers: 5; number of reducers: 0
2023-01-05 07:16:01,392 Stage-24 map = 0%,  reduce = 0%
Hadoop job information for Stage-26: number of mappers: 4; number of reducers: 0
2023-01-05 07:16:07,613 Stage-26 map = 0%,  reduce = 0%
2023-01-05 07:16:20,053 Stage-19 map = 1%,  reduce = 0%, Cumulative CPU 49.37 sec
2023-01-05 07:16:36,355 Stage-24 map = 1%,  reduce = 0%, Cumulative CPU 62.33 sec
2023-01-05 07:16:42,488 Stage-24 map = 2%,  reduce = 0%, Cumulative CPU 101.59 sec
2023-01-05 07:16:43,954 Stage-26 map = 1%,  reduce = 0%, Cumulative CPU 66.83 sec
2023-01-05 07:16:46,602 Stage-24 map = 3%,  reduce = 0%, Cumulative CPU 111.14 sec
2023-01-05 07:16:49,204 Stage-26 map = 2%,  reduce = 0%, Cumulative CPU 100.38 sec
2023-01-05 07:16:49,692 Stage-24 map = 4%,  reduce = 0%, Cumulative CPU 127.68 sec
2023-01-05 07:16:50,238 Stage-19 map = 2%,  reduce = 0%, Cumulative CPU 86.32 sec
2023-01-05 07:16:54,389 Stage-26 map = 3%,  reduce = 0%, Cumulative CPU 114.38 sec
2023-01-05 07:16:55,835 Stage-24 map = 5%,  reduce = 0%, Cumulative CPU 145.62 sec
2023-01-05 07:17:00,717 Stage-26 map = 4%,  reduce = 0%, Cumulative CPU 130.51 sec
2023-01-05 07:17:01,185 Stage-24 map = 6%,  reduce = 0%, Cumulative CPU 164.61 sec
2023-01-05 07:17:05,306 Stage-24 map = 7%,  reduce = 0%, Cumulative CPU 170.75 sec
2023-01-05 07:17:05,952 Stage-26 map = 5%,  reduce = 0%, Cumulative CPU 143.38 sec
2023-01-05 07:17:07,369 Stage-24 map = 8%,  reduce = 0%, Cumulative CPU 182.75 sec
2023-01-05 07:17:09,156 Stage-26 map = 6%,  reduce = 0%, Cumulative CPU 156.94 sec
2023-01-05 07:17:13,379 Stage-26 map = 7%,  reduce = 0%, Cumulative CPU 167.03 sec
2023-01-05 07:17:13,498 Stage-24 map = 9%,  reduce = 0%, Cumulative CPU 201.24 sec
2023-01-05 07:17:14,440 Stage-19 map = 3%,  reduce = 0%, Cumulative CPU 133.29 sec
2023-01-05 07:17:16,570 Stage-24 map = 10%,  reduce = 0%, Cumulative CPU 204.35 sec
2023-01-05 07:17:18,655 Stage-26 map = 8%,  reduce = 0%, Cumulative CPU 180.56 sec
2023-01-05 07:17:19,609 Stage-24 map = 11%,  reduce = 0%, Cumulative CPU 219.45 sec
2023-01-05 07:17:24,124 Stage-26 map = 9%,  reduce = 0%, Cumulative CPU 193.11 sec
2023-01-05 07:17:24,752 Stage-24 map = 12%,  reduce = 0%, Cumulative CPU 231.7 sec
2023-01-05 07:17:28,847 Stage-24 map = 13%,  reduce = 0%, Cumulative CPU 241.57 sec
2023-01-05 07:17:30,567 Stage-26 map = 10%,  reduce = 0%, Cumulative CPU 213.93 sec
2023-01-05 07:17:31,911 Stage-24 map = 14%,  reduce = 0%, Cumulative CPU 257.67 sec
2023-01-05 07:17:32,538 Stage-19 map = 4%,  reduce = 0%, Cumulative CPU 167.31 sec
2023-01-05 07:17:36,057 Stage-24 map = 15%,  reduce = 0%, Cumulative CPU 264.24 sec
2023-01-05 07:17:36,956 Stage-26 map = 11%,  reduce = 0%, Cumulative CPU 229.64 sec
2023-01-05 07:17:39,052 Stage-26 map = 12%,  reduce = 0%, Cumulative CPU 238.68 sec
2023-01-05 07:17:41,180 Stage-24 map = 16%,  reduce = 0%, Cumulative CPU 279.96 sec
2023-01-05 07:17:43,208 Stage-24 map = 17%,  reduce = 0%, Cumulative CPU 295.54 sec
2023-01-05 07:17:45,349 Stage-26 map = 13%,  reduce = 0%, Cumulative CPU 255.76 sec
2023-01-05 07:17:47,311 Stage-24 map = 18%,  reduce = 0%, Cumulative CPU 301.97 sec
2023-01-05 07:17:49,375 Stage-24 map = 19%,  reduce = 0%, Cumulative CPU 314.29 sec
2023-01-05 07:17:50,618 Stage-19 map = 5%,  reduce = 0%, Cumulative CPU 202.42 sec
2023-01-05 07:17:51,631 Stage-26 map = 14%,  reduce = 0%, Cumulative CPU 272.13 sec
2023-01-05 07:17:55,497 Stage-24 map = 20%,  reduce = 0%, Cumulative CPU 331.97 sec
2023-01-05 07:17:55,843 Stage-26 map = 15%,  reduce = 0%, Cumulative CPU 282.19 sec
2023-01-05 07:17:59,593 Stage-24 map = 21%,  reduce = 0%, Cumulative CPU 338.69 sec
2023-01-05 07:18:02,149 Stage-26 map = 16%,  reduce = 0%, Cumulative CPU 298.49 sec
2023-01-05 07:18:02,218 Stage-19 map = 6%,  reduce = 0%, Cumulative CPU 225.66 sec
2023-01-05 07:18:05,717 Stage-24 map = 22%,  reduce = 0%, Cumulative CPU 357.36 sec
2023-01-05 07:18:07,345 Stage-26 map = 17%,  reduce = 0%, Cumulative CPU 312.21 sec
2023-01-05 07:18:07,795 Stage-24 map = 23%,  reduce = 0%, Cumulative CPU 370.13 sec
2023-01-05 07:18:12,617 Stage-26 map = 18%,  reduce = 0%, Cumulative CPU 325.57 sec
2023-01-05 07:18:12,917 Stage-24 map = 24%,  reduce = 0%, Cumulative CPU 382.26 sec
2023-01-05 07:18:14,749 Stage-26 map = 19%,  reduce = 0%, Cumulative CPU 338.77 sec
2023-01-05 07:18:18,023 Stage-24 map = 25%,  reduce = 0%, Cumulative CPU 394.6 sec
2023-01-05 07:18:20,272 Stage-19 map = 7%,  reduce = 0%, Cumulative CPU 260.3 sec
2023-01-05 07:18:21,119 Stage-26 map = 20%,  reduce = 0%, Cumulative CPU 356.96 sec
2023-01-05 07:18:23,122 Stage-24 map = 26%,  reduce = 0%, Cumulative CPU 409.99 sec
2023-01-05 07:18:26,199 Stage-24 map = 27%,  reduce = 0%, Cumulative CPU 425.77 sec
2023-01-05 07:18:27,444 Stage-26 map = 21%,  reduce = 0%, Cumulative CPU 374.13 sec
2023-01-05 07:18:31,309 Stage-24 map = 28%,  reduce = 0%, Cumulative CPU 438.11 sec
2023-01-05 07:18:32,765 Stage-26 map = 22%,  reduce = 0%, Cumulative CPU 391.72 sec
2023-01-05 07:18:32,848 Stage-19 map = 8%,  reduce = 0%, Cumulative CPU 283.9 sec
2023-01-05 07:18:35,404 Stage-24 map = 29%,  reduce = 0%, Cumulative CPU 451.15 sec
2023-01-05 07:18:37,463 Stage-24 map = 30%,  reduce = 0%, Cumulative CPU 463.22 sec
2023-01-05 07:18:43,469 Stage-26 map = 23%,  reduce = 0%, Cumulative CPU 415.82 sec
2023-01-05 07:18:43,587 Stage-24 map = 31%,  reduce = 0%, Cumulative CPU 482.37 sec
2023-01-05 07:18:46,618 Stage-19 map = 9%,  reduce = 0%, Cumulative CPU 310.05 sec
2023-01-05 07:18:48,697 Stage-24 map = 32%,  reduce = 0%, Cumulative CPU 495.16 sec
2023-01-05 07:18:49,717 Stage-26 map = 24%,  reduce = 0%, Cumulative CPU 437.19 sec
2023-01-05 07:18:54,813 Stage-24 map = 33%,  reduce = 0%, Cumulative CPU 514.5 sec
2023-01-05 07:18:55,856 Stage-24 map = 34%,  reduce = 0%, Cumulative CPU 521.02 sec
2023-01-05 07:18:57,114 Stage-26 map = 25%,  reduce = 0%, Cumulative CPU 461.54 sec
2023-01-05 07:18:57,147 Stage-19 map = 10%,  reduce = 0%, Cumulative CPU 328.07 sec
2023-01-05 07:19:01,990 Stage-24 map = 35%,  reduce = 0%, Cumulative CPU 540.14 sec
2023-01-05 07:19:07,092 Stage-24 map = 36%,  reduce = 0%, Cumulative CPU 552.6 sec
2023-01-05 07:19:07,572 Stage-26 map = 26%,  reduce = 0%, Cumulative CPU 487.36 sec
2023-01-05 07:19:08,783 Stage-19 map = 11%,  reduce = 0%, Cumulative CPU 350.07 sec
2023-01-05 07:19:12,190 Stage-24 map = 37%,  reduce = 0%, Cumulative CPU 566.11 sec
2023-01-05 07:19:13,975 Stage-26 map = 27%,  reduce = 0%, Cumulative CPU 508.86 sec
2023-01-05 07:19:17,302 Stage-24 map = 38%,  reduce = 0%, Cumulative CPU 581.32 sec
2023-01-05 07:19:20,375 Stage-24 map = 39%,  reduce = 0%, Cumulative CPU 595.74 sec
2023-01-05 07:19:20,462 Stage-19 map = 12%,  reduce = 0%, Cumulative CPU 367.96 sec
2023-01-05 07:19:21,454 Stage-26 map = 28%,  reduce = 0%, Cumulative CPU 532.05 sec
2023-01-05 07:19:26,492 Stage-24 map = 40%,  reduce = 0%, Cumulative CPU 615.39 sec
2023-01-05 07:19:27,768 Stage-26 map = 29%,  reduce = 0%, Cumulative CPU 549.46 sec
2023-01-05 07:19:29,046 Stage-19 map = 13%,  reduce = 0%, Cumulative CPU 386.27 sec
2023-01-05 07:19:31,817 Stage-24 map = 41%,  reduce = 0%, Cumulative CPU 634.93 sec
2023-01-05 07:19:33,065 Stage-26 map = 30%,  reduce = 0%, Cumulative CPU 567.34 sec
2023-01-05 07:19:35,910 Stage-24 map = 42%,  reduce = 0%, Cumulative CPU 641.53 sec
2023-01-05 07:19:38,473 Stage-19 map = 14%,  reduce = 0%, Cumulative CPU 400.39 sec
2023-01-05 07:19:39,412 Stage-26 map = 31%,  reduce = 0%, Cumulative CPU 585.58 sec
2023-01-05 07:19:42,052 Stage-24 map = 43%,  reduce = 0%, Cumulative CPU 660.35 sec
2023-01-05 07:19:44,095 Stage-24 map = 44%,  reduce = 0%, Cumulative CPU 673.07 sec
2023-01-05 07:19:45,657 Stage-26 map = 32%,  reduce = 0%, Cumulative CPU 602.43 sec
2023-01-05 07:19:46,919 Stage-19 map = 15%,  reduce = 0%, Cumulative CPU 419.86 sec
2023-01-05 07:19:49,202 Stage-24 map = 45%,  reduce = 0%, Cumulative CPU 686.13 sec
2023-01-05 07:19:50,943 Stage-26 map = 33%,  reduce = 0%, Cumulative CPU 619.5 sec
2023-01-05 07:19:54,322 Stage-24 map = 46%,  reduce = 0%, Cumulative CPU 699.56 sec
2023-01-05 07:19:56,378 Stage-24 map = 47%,  reduce = 0%, Cumulative CPU 712.41 sec
2023-01-05 07:19:57,216 Stage-26 map = 34%,  reduce = 0%, Cumulative CPU 637.0 sec
2023-01-05 07:19:57,516 Stage-19 map = 16%,  reduce = 0%, Cumulative CPU 439.27 sec
2023-01-05 07:20:00,469 Stage-24 map = 48%,  reduce = 0%, Cumulative CPU 725.24 sec
2023-01-05 07:20:03,545 Stage-26 map = 35%,  reduce = 0%, Cumulative CPU 654.01 sec
2023-01-05 07:20:05,587 Stage-24 map = 49%,  reduce = 0%, Cumulative CPU 735.88 sec
2023-01-05 07:20:08,658 Stage-24 map = 50%,  reduce = 0%, Cumulative CPU 752.12 sec
2023-01-05 07:20:09,850 Stage-26 map = 36%,  reduce = 0%, Cumulative CPU 670.72 sec
2023-01-05 07:20:11,160 Stage-19 map = 17%,  reduce = 0%, Cumulative CPU 465.5 sec
2023-01-05 07:20:12,742 Stage-24 map = 51%,  reduce = 0%, Cumulative CPU 764.82 sec
2023-01-05 07:20:15,121 Stage-26 map = 37%,  reduce = 0%, Cumulative CPU 687.56 sec
2023-01-05 07:20:17,855 Stage-24 map = 52%,  reduce = 0%, Cumulative CPU 778.07 sec
2023-01-05 07:20:19,888 Stage-24 map = 53%,  reduce = 0%, Cumulative CPU 790.92 sec
2023-01-05 07:20:21,447 Stage-26 map = 38%,  reduce = 0%, Cumulative CPU 705.15 sec
2023-01-05 07:20:23,654 Stage-19 map = 18%,  reduce = 0%, Cumulative CPU 487.71 sec
2023-01-05 07:20:24,989 Stage-24 map = 54%,  reduce = 0%, Cumulative CPU 803.87 sec
2023-01-05 07:20:27,738 Stage-26 map = 39%,  reduce = 0%, Cumulative CPU 722.15 sec
2023-01-05 07:20:30,085 Stage-24 map = 55%,  reduce = 0%, Cumulative CPU 817.85 sec
2023-01-05 07:20:32,112 Stage-24 map = 56%,  reduce = 0%, Cumulative CPU 830.65 sec
2023-01-05 07:20:33,052 Stage-26 map = 40%,  reduce = 0%, Cumulative CPU 739.14 sec
2023-01-05 07:20:35,153 Stage-19 map = 19%,  reduce = 0%, Cumulative CPU 509.84 sec
2023-01-05 07:20:38,243 Stage-24 map = 57%,  reduce = 0%, Cumulative CPU 849.29 sec
2023-01-05 07:20:38,316 Stage-26 map = 41%,  reduce = 0%, Cumulative CPU 749.94 sec
2023-01-05 07:20:44,400 Stage-24 map = 58%,  reduce = 0%, Cumulative CPU 868.03 sec
2023-01-05 07:20:44,613 Stage-26 map = 42%,  reduce = 0%, Cumulative CPU 768.2 sec
2023-01-05 07:20:47,671 Stage-19 map = 20%,  reduce = 0%, Cumulative CPU 532.44 sec
2023-01-05 07:20:48,489 Stage-24 map = 59%,  reduce = 0%, Cumulative CPU 880.61 sec
2023-01-05 07:20:49,872 Stage-26 map = 43%,  reduce = 0%, Cumulative CPU 781.53 sec
2023-01-05 07:20:53,618 Stage-24 map = 60%,  reduce = 0%, Cumulative CPU 890.29 sec
2023-01-05 07:20:55,164 Stage-26 map = 44%,  reduce = 0%, Cumulative CPU 794.62 sec
2023-01-05 07:20:56,900 Stage-24 map = 61%,  reduce = 0%, Cumulative CPU 906.11 sec
2023-01-05 07:20:57,275 Stage-26 map = 45%,  reduce = 0%, Cumulative CPU 808.45 sec
2023-01-05 07:20:59,204 Stage-19 map = 21%,  reduce = 0%, Cumulative CPU 555.62 sec
2023-01-05 07:21:01,001 Stage-24 map = 62%,  reduce = 0%, Cumulative CPU 918.13 sec
2023-01-05 07:21:03,621 Stage-26 map = 46%,  reduce = 0%, Cumulative CPU 825.06 sec
2023-01-05 07:21:06,123 Stage-24 map = 63%,  reduce = 0%, Cumulative CPU 931.69 sec
2023-01-05 07:21:08,173 Stage-24 map = 64%,  reduce = 0%, Cumulative CPU 943.77 sec
2023-01-05 07:21:08,915 Stage-26 map = 47%,  reduce = 0%, Cumulative CPU 835.18 sec
2023-01-05 07:21:13,286 Stage-24 map = 65%,  reduce = 0%, Cumulative CPU 956.21 sec
2023-01-05 07:21:14,153 Stage-26 map = 48%,  reduce = 0%, Cumulative CPU 848.38 sec
2023-01-05 07:21:15,045 Stage-19 map = 22%,  reduce = 0%, Cumulative CPU 587.37 sec
2023-01-05 07:21:15,172 Stage-26 map = 49%,  reduce = 0%, Cumulative CPU 858.3 sec
2023-01-05 07:21:18,428 Stage-24 map = 66%,  reduce = 0%, Cumulative CPU 969.15 sec
2023-01-05 07:21:20,412 Stage-26 map = 50%,  reduce = 0%, Cumulative CPU 868.02 sec
2023-01-05 07:21:20,490 Stage-24 map = 67%,  reduce = 0%, Cumulative CPU 981.48 sec
2023-01-05 07:21:24,597 Stage-24 map = 68%,  reduce = 0%, Cumulative CPU 994.82 sec
2023-01-05 07:21:25,714 Stage-26 map = 51%,  reduce = 0%, Cumulative CPU 878.26 sec
2023-01-05 07:21:27,820 Stage-26 map = 52%,  reduce = 0%, Cumulative CPU 891.62 sec
2023-01-05 07:21:29,718 Stage-24 map = 69%,  reduce = 0%, Cumulative CPU 1004.37 sec
2023-01-05 07:21:30,764 Stage-24 map = 70%,  reduce = 0%, Cumulative CPU 1013.6 sec
2023-01-05 07:21:31,967 Stage-26 map = 53%,  reduce = 0%, Cumulative CPU 897.9 sec
2023-01-05 07:21:33,875 Stage-19 map = 23%,  reduce = 0%, Cumulative CPU 624.15 sec
2023-01-05 07:21:35,886 Stage-24 map = 71%,  reduce = 0%, Cumulative CPU 1026.21 sec
2023-01-05 07:21:37,255 Stage-26 map = 54%,  reduce = 0%, Cumulative CPU 910.22 sec
2023-01-05 07:21:38,319 Stage-26 map = 55%,  reduce = 0%, Cumulative CPU 916.95 sec
2023-01-05 07:21:38,973 Stage-24 map = 72%,  reduce = 0%, Cumulative CPU 1039.2 sec
2023-01-05 07:21:42,018 Stage-24 map = 73%,  reduce = 0%, Cumulative CPU 1044.12 sec
2023-01-05 07:21:44,076 Stage-24 map = 74%,  reduce = 0%, Cumulative CPU 1053.32 sec
2023-01-05 07:21:44,553 Stage-26 map = 56%,  reduce = 0%, Cumulative CPU 931.33 sec
2023-01-05 07:21:45,346 Stage-19 map = 24%,  reduce = 0%, Cumulative CPU 647.09 sec
2023-01-05 07:21:49,244 Stage-24 map = 75%,  reduce = 0%, Cumulative CPU 1063.19 sec
2023-01-05 07:21:50,756 Stage-26 map = 57%,  reduce = 0%, Cumulative CPU 944.25 sec
2023-01-05 07:21:54,358 Stage-24 map = 76%,  reduce = 0%, Cumulative CPU 1075.63 sec
2023-01-05 07:21:55,393 Stage-24 map = 77%,  reduce = 0%, Cumulative CPU 1081.95 sec
2023-01-05 07:21:55,972 Stage-26 map = 58%,  reduce = 0%, Cumulative CPU 956.11 sec
2023-01-05 07:21:56,995 Stage-26 map = 59%,  reduce = 0%, Cumulative CPU 959.66 sec
2023-01-05 07:21:59,497 Stage-24 map = 78%,  reduce = 0%, Cumulative CPU 1091.36 sec
2023-01-05 07:21:59,885 Stage-19 map = 25%,  reduce = 0%, Cumulative CPU 676.42 sec
2023-01-05 07:22:02,197 Stage-26 map = 60%,  reduce = 0%, Cumulative CPU 972.01 sec
2023-01-05 07:22:02,575 Stage-24 map = 79%,  reduce = 0%, Cumulative CPU 1106.73 sec
2023-01-05 07:22:03,236 Stage-26 map = 61%,  reduce = 0%, Cumulative CPU 981.62 sec
2023-01-05 07:22:06,655 Stage-24 map = 80%,  reduce = 0%, Cumulative CPU 1118.37 sec
2023-01-05 07:22:08,698 Stage-24 map = 81%,  reduce = 0%, Cumulative CPU 1123.64 sec
2023-01-05 07:22:09,419 Stage-26 map = 62%,  reduce = 0%, Cumulative CPU 994.41 sec
2023-01-05 07:22:12,791 Stage-24 map = 82%,  reduce = 0%, Cumulative CPU 1134.41 sec
2023-01-05 07:22:14,720 Stage-26 map = 63%,  reduce = 0%, Cumulative CPU 1003.68 sec
2023-01-05 07:22:15,523 Stage-19 map = 26%,  reduce = 0%, Cumulative CPU 708.59 sec
2023-01-05 07:22:17,888 Stage-24 map = 83%,  reduce = 0%, Cumulative CPU 1144.76 sec
2023-01-05 07:22:18,909 Stage-24 map = 84%,  reduce = 0%, Cumulative CPU 1154.56 sec
2023-01-05 07:22:19,919 Stage-26 map = 64%,  reduce = 0%, Cumulative CPU 1013.75 sec
2023-01-05 07:22:20,936 Stage-26 map = 65%,  reduce = 0%, Cumulative CPU 1020.0 sec
2023-01-05 07:22:24,054 Stage-24 map = 85%,  reduce = 0%, Cumulative CPU 1167.68 sec
2023-01-05 07:22:25,072 Stage-26 map = 66%,  reduce = 0%, Cumulative CPU 1028.51 sec
2023-01-05 07:22:27,102 Stage-24 map = 86%,  reduce = 0%, Cumulative CPU 1177.88 sec
2023-01-05 07:22:28,039 Stage-19 map = 27%,  reduce = 0%, Cumulative CPU 732.7 sec
2023-01-05 07:22:28,179 Stage-26 map = 67%,  reduce = 0%, Cumulative CPU 1040.74 sec
2023-01-05 07:22:32,222 Stage-24 map = 87%,  reduce = 0%, Cumulative CPU 1192.25 sec
2023-01-05 07:22:33,444 Stage-26 map = 68%,  reduce = 0%, Cumulative CPU 1056.73 sec
2023-01-05 07:22:36,333 Stage-24 map = 88%,  reduce = 0%, Cumulative CPU 1202.09 sec
2023-01-05 07:22:37,637 Stage-26 map = 69%,  reduce = 0%, Cumulative CPU 1059.69 sec
2023-01-05 07:22:38,380 Stage-24 map = 89%,  reduce = 0%, Cumulative CPU 1213.2 sec
2023-01-05 07:22:39,414 Stage-19 map = 28%,  reduce = 0%, Cumulative CPU 756.0 sec
2023-01-05 07:22:39,688 Stage-26 map = 70%,  reduce = 0%, Cumulative CPU 1072.3 sec
2023-01-05 07:22:42,689 Stage-24 map = 90%,  reduce = 0%, Cumulative CPU 1227.52 sec
2023-01-05 07:22:44,726 Stage-24 map = 91%,  reduce = 0%, Cumulative CPU 1231.17 sec
2023-01-05 07:22:44,831 Stage-26 map = 71%,  reduce = 0%, Cumulative CPU 1081.57 sec
2023-01-05 07:22:48,818 Stage-24 map = 92%,  reduce = 0%, Cumulative CPU 1243.56 sec
2023-01-05 07:22:49,934 Stage-26 map = 72%,  reduce = 0%, Cumulative CPU 1091.43 sec
2023-01-05 07:22:50,957 Stage-26 map = 73%,  reduce = 0%, Cumulative CPU 1098.12 sec
2023-01-05 07:22:51,729 Stage-19 map = 29%,  reduce = 0%, Cumulative CPU 780.61 sec
2023-01-05 07:22:53,907 Stage-24 map = 93%,  reduce = 0%, Cumulative CPU 1249.9 sec
2023-01-05 07:22:57,141 Stage-26 map = 74%,  reduce = 0%, Cumulative CPU 1115.64 sec
2023-01-05 07:22:58,179 Stage-26 map = 75%,  reduce = 0%, Cumulative CPU 1122.05 sec
2023-01-05 07:23:00,034 Stage-24 map = 94%,  reduce = 0%, Cumulative CPU 1264.76 sec
2023-01-05 07:23:03,103 Stage-24 map = 95%,  reduce = 0%, Cumulative CPU 1277.7 sec
2023-01-05 07:23:03,360 Stage-26 map = 76%,  reduce = 0%, Cumulative CPU 1139.54 sec
2023-01-05 07:23:04,152 Stage-19 map = 30%,  reduce = 0%, Cumulative CPU 805.54 sec
2023-01-05 07:23:08,537 Stage-26 map = 77%,  reduce = 0%, Cumulative CPU 1148.04 sec
2023-01-05 07:23:09,200 Stage-24 map = 96%,  reduce = 0%, Cumulative CPU 1294.18 sec
2023-01-05 07:23:09,553 Stage-26 map = 78%,  reduce = 0%, Cumulative CPU 1158.29 sec
2023-01-05 07:23:10,290 Stage-19 map = 31%,  reduce = 0%, Cumulative CPU 817.71 sec
2023-01-05 07:23:12,279 Stage-24 map = 97%,  reduce = 0%, Cumulative CPU 1302.58 sec
2023-01-05 07:23:14,677 Stage-26 map = 79%,  reduce = 0%, Cumulative CPU 1170.51 sec
2023-01-05 07:23:18,427 Stage-24 map = 98%,  reduce = 0%, Cumulative CPU 1309.07 sec
2023-01-05 07:23:18,469 Stage-19 map = 32%,  reduce = 0%, Cumulative CPU 834.92 sec
2023-01-05 07:23:19,794 Stage-26 map = 80%,  reduce = 0%, Cumulative CPU 1181.83 sec
2023-01-05 07:23:21,837 Stage-26 map = 81%,  reduce = 0%, Cumulative CPU 1197.13 sec
2023-01-05 07:23:27,653 Stage-19 map = 33%,  reduce = 0%, Cumulative CPU 857.45 sec
2023-01-05 07:23:27,955 Stage-26 map = 82%,  reduce = 0%, Cumulative CPU 1214.04 sec
2023-01-05 07:23:33,072 Stage-26 map = 83%,  reduce = 0%, Cumulative CPU 1224.45 sec
2023-01-05 07:23:33,773 Stage-19 map = 34%,  reduce = 0%, Cumulative CPU 872.08 sec
2023-01-05 07:23:36,765 Stage-24 map = 99%,  reduce = 0%, Cumulative CPU 1322.57 sec
2023-01-05 07:23:39,156 Stage-26 map = 84%,  reduce = 0%, Cumulative CPU 1239.43 sec
2023-01-05 07:23:39,874 Stage-19 map = 35%,  reduce = 0%, Cumulative CPU 885.35 sec
2023-01-05 07:23:44,249 Stage-26 map = 85%,  reduce = 0%, Cumulative CPU 1254.39 sec
2023-01-05 07:23:45,981 Stage-19 map = 36%,  reduce = 0%, Cumulative CPU 898.02 sec
2023-01-05 07:23:48,011 Stage-19 map = 37%,  reduce = 0%, Cumulative CPU 901.43 sec
2023-01-05 07:23:49,332 Stage-26 map = 86%,  reduce = 0%, Cumulative CPU 1265.43 sec
2023-01-05 07:23:51,366 Stage-26 map = 87%,  reduce = 0%, Cumulative CPU 1275.94 sec
2023-01-05 07:23:54,107 Stage-19 map = 38%,  reduce = 0%, Cumulative CPU 913.85 sec
2023-01-05 07:23:57,480 Stage-26 map = 88%,  reduce = 0%, Cumulative CPU 1292.42 sec
2023-01-05 07:24:00,213 Stage-19 map = 39%,  reduce = 0%, Cumulative CPU 926.82 sec
2023-01-05 07:24:01,553 Stage-26 map = 89%,  reduce = 0%, Cumulative CPU 1297.16 sec
2023-01-05 07:24:04,293 Stage-19 map = 40%,  reduce = 0%, Cumulative CPU 935.39 sec
2023-01-05 07:24:07,335 Stage-24 map = 100%,  reduce = 0%, Cumulative CPU 1343.61 sec
2023-01-05 07:24:07,654 Stage-26 map = 90%,  reduce = 0%, Cumulative CPU 1312.76 sec
MapReduce Total cumulative CPU time: 22 minutes 23 seconds 610 msec
Ended Job = job_1672890466700_0046


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 07:24:09,731 Stage-26 map = 91%,  reduce = 0%, Cumulative CPU 1323.98 sec
2023-01-05 07:24:10,473 Stage-19 map = 41%,  reduce = 0%, Cumulative CPU 951.89 sec
2023-01-05 07:24:13,806 Stage-26 map = 92%,  reduce = 0%, Cumulative CPU 1328.81 sec
2023-01-05 07:24:15,647 Stage-19 map = 42%,  reduce = 0%, Cumulative CPU 962.4 sec
2023-01-05 07:24:16	Starting to launch local task to process map join;	maximum memory = 239075328


Execution completed successfully
MapredLocal task succeeded
2023-01-05 07:24:20,063 Stage-26 map = 93%,  reduce = 0%, Cumulative CPU 1343.89 sec
Launching Job 4 out of 9
Number of reduce tasks not specified. Estimated from input data size: 7
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:24:21,778 Stage-19 map = 43%,  reduce = 0%, Cumulative CPU 985.34 sec
Starting Job = job_1672890466700_0048, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0048/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0048
2023-01-05 07:24:24,823 Stage-19 map = 54%,  reduce = 0%, Cumulative CPU 994.55 sec
2023-01-05 07:24:27,145 Stage-26 map = 94%,  reduce = 0%, Cumulative CPU 1363.54 sec
2023-01-05 07:24:27,866 Stage-19 map = 55%,  reduce = 0%, Cumulative CPU 999.95 sec
Hadoop job information for Stage-3: number of mappers: 5; number of reducers: 7
2023-01-05 07:24:29,441 Stage-3 map = 0%,  reduce = 0%
2023-01-05 07:24:35,999 Stage-19 map = 56%,  reduce = 0%, Cumulative CPU 1017.13 sec
2023-01-05 07:24:39,335 Stage-26 map = 95%,  reduce = 0%, Cumulative CPU 1369.8 sec
2023-01-05 07:24:42,103 Stage-19 map = 56%,  reduce = 6%, Cumulative CPU 1026.37 sec
2023-01-05 07:24:48,178 Stage-19 map = 57%,  reduce = 6%, Cumulative CPU 1034.53 sec
2023-01-05 07:24:51,216 Stage-19 map = 57%,  reduce = 11%, Cumulative CPU 1035.51 sec
2023-01-05 07:24:54,102 Stage-3 map = 5%,  reduce = 0%, Cumulative CPU 22.01 sec
2023-01-05 07:24:56,571 Stage-26 map = 96%,  reduce = 0%, Cumulative CPU 1374.73 sec
2023-01-05 07:24:59,420 Stage-3 map = 11%,  reduce = 0%, Cumulative CPU 41.57 sec
2023-01-05 07:25:02,720 Stage-3 map = 27%,  reduce = 0%, Cumulative CPU 42.59 sec
2023-01-05 07:25:05,999 Stage-3 map = 32%,  reduce = 0%, Cumulative CPU 53.73 sec
2023-01-05 07:25:10,528 Stage-19 map = 58%,  reduce = 11%, Cumulative CPU 1052.49 sec
2023-01-05 07:25:11,790 Stage-3 map = 48%,  reduce = 0%, Cumulative CPU 65.87 sec
2023-01-05 07:25:14,995 Stage-3 map = 52%,  reduce = 0%, Cumulative CPU 68.0 sec
2023-01-05 07:25:16,037 Stage-3 map = 63%,  reduce = 0%, Cumulative CPU 70.9 sec
2023-01-05 07:25:20,579 Stage-3 map = 63%,  reduce = 3%, Cumulative CPU 74.85 sec
2023-01-05 07:25:21,003 Stage-26 map = 97%,  reduce = 0%, Cumulative CPU 1381.93 sec
2023-01-05 07:25:21,627 Stage-3 map = 68%,  reduce = 3%, Cumulative CPU 77.85 sec
2023-01-05 07:25:22,667 Stage-3 map = 88%,  reduce = 3%, Cumulative CPU 81.18 sec
2023-01-05 07:25:23,848 Stage-19 map = 59%,  reduce = 11%, Cumulative CPU 1067.01 sec
2023-01-05 07:25:26,899 Stage-3 map = 88%,  reduce = 23%, Cumulative CPU 85.15 sec
2023-01-05 07:25:29,923 Stage-19 map = 60%,  reduce = 11%, Cumulative CPU 1077.72 sec
2023-01-05 07:25:31,227 Stage-3 map = 88%,  reduce = 27%, Cumulative CPU 88.67 sec
2023-01-05 07:25:32,357 Stage-3 map = 100%,  reduce = 32%, Cumulative CPU 92.07 sec
2023-01-05 07:25:33,214 Stage-26 map = 98%,  reduce = 0%, Cumulative CPU 1389.34 sec
2023-01-05 07:25:33,392 Stage-3 map = 100%,  reduce = 44%, Cumulative CPU 93.74 sec
2023-01-05 07:25:35,423 Stage-3 map = 100%,  reduce = 64%, Cumulative CPU 101.64 sec
2023-01-05 07:25:36,439 Stage-3 map = 100%,  reduce = 79%, Cumulative CPU 107.52 sec
2023-01-05 07:25:37,453 Stage-3 map = 100%,  reduce = 85%, Cumulative CPU 110.58 sec
2023-01-05 07:25:38,468 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 116.0 sec
2023-01-05 07:25:39,300 Stage-26 map = 99%,  reduce = 0%, Cumulative CPU 1391.38 sec
MapReduce Total cumulative CPU time: 1 minutes 56 seconds 0 msec
Ended Job = job_1672890466700_0048
2023-01-05 07:25:42,246 Stage-19 map = 61%,  reduce = 11%, Cumulative CPU 1094.43 sec
2023-01-05 07:25:48,331 Stage-19 map = 62%,  reduce = 11%, Cumulative CPU 1105.24 sec
2023-01-05 07:25:49,444 Stage-26 map = 100%,  reduce = 0%, Cumulative CPU 1396.48 sec
MapReduce Total cumulative CPU time: 23 minutes 16 seconds 480 msec
Ended Job = job_1672890466700_0047


SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2023-01-05 07:25:54,456 Stage-19 map = 63%,  reduce = 11%, Cumulative CPU 1114.66 sec
2023-01-05 07:26:00	Dump the side-table for tag: 1 with group count: 12000 into file: file:/tmp/hdfs/3b100c90-0c5c-44b2-ae85-9bd8cad358e9/hive_2023-01-05_07-14-50_820_7348710886379401300-1/-local-10023/HashTable-Stage-11/MapJoin-mapfile21--.hashtable2023-01-05 07:26:00	Uploaded 1 File to: file:/tmp/hdfs/3b100c90-0c5c-44b2-ae85-9bd8cad358e9/hive_2023-01-05_07-14-50_820_7348710886379401300-1/-local-10023/HashTable-Stage-11/MapJoin-mapfile21--.hashtable (458101 bytes)
2023-01-05 07:26:00,560 Stage-19 map = 64%,  reduce = 11%, Cumulative CPU 1125.43 sec
Execution completed successfully
MapredLocal task succeeded
Launching Job 5 out of 9
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0049, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0049/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0049
2023-01-05 07:26:06,649 Stage-19 map = 65%,  reduce = 11%, Cumulative CPU 1137.87 sec
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 1
2023-01-05 07:26:16,102 Stage-11 map = 0%,  reduce = 0%
2023-01-05 07:26:16,792 Stage-19 map = 66%,  reduce = 11%, Cumulative CPU 1156.96 sec
2023-01-05 07:26:22,880 Stage-19 map = 67%,  reduce = 11%, Cumulative CPU 1171.0 sec
2023-01-05 07:26:28,755 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 7.38 sec
2023-01-05 07:26:30,965 Stage-19 map = 79%,  reduce = 11%, Cumulative CPU 1191.51 sec
2023-01-05 07:26:32,987 Stage-19 map = 79%,  reduce = 17%, Cumulative CPU 1191.69 sec
2023-01-05 07:26:35,872 Stage-11 map = 100%,  reduce = 100%, Cumulative CPU 10.02 sec
MapReduce Total cumulative CPU time: 10 seconds 20 msec
Ended Job = job_1672890466700_0049
2023-01-05 07:26:37,048 Stage-19 map = 79%,  reduce = 22%, Cumulative CPU 1198.09 sec
2023-01-05 07:26:46,160 Stage-19 map = 80%,  reduce = 22%, Cumulative CPU 1211.12 sec
2023-01-05 07:26:58,290 Stage-19 map = 81%,  reduce = 22%, Cumulative CPU 1226.24 sec
2023-01-05 07:27:10,418 Stage-19 map = 82%,  reduce = 22%, Cumulative CPU 1241.02 sec
2023-01-05 07:27:22,540 Stage-19 map = 83%,  reduce = 22%, Cumulative CPU 1255.86 sec
2023-01-05 07:27:34,662 Stage-19 map = 84%,  reduce = 22%, Cumulative CPU 1270.66 sec
2023-01-05 07:27:46,787 Stage-19 map = 85%,  reduce = 22%, Cumulative CPU 1285.51 sec
2023-01-05 07:28:03,967 Stage-19 map = 86%,  reduce = 22%, Cumulative CPU 1308.25 sec
2023-01-05 07:28:16,091 Stage-19 map = 87%,  reduce = 22%, Cumulative CPU 1323.42 sec
2023-01-05 07:28:28,237 Stage-19 map = 88%,  reduce = 22%, Cumulative CPU 1338.26 sec
2023-01-05 07:28:46,418 Stage-19 map = 89%,  reduce = 22%, Cumulative CPU 1360.55 sec
2023-01-05 07:29:00,566 Stage-19 map = 100%,  reduce = 31%, Cumulative CPU 1378.24 sec
2023-01-05 07:29:03,596 Stage-19 map = 100%,  reduce = 70%, Cumulative CPU 1382.41 sec
2023-01-05 07:29:05,616 Stage-19 map = 100%,  reduce = 100%, Cumulative CPU 1388.04 sec
MapReduce Total cumulative CPU time: 23 minutes 8 seconds 40 msec
Ended Job = job_1672890466700_0045
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]


2023-01-05 07:29:15	Dump the side-table for tag: 1 with group count: 42 into file: file:/tmp/hdfs/3b100c90-0c5c-44b2-ae85-9bd8cad358e9/hive_2023-01-05_07-14-50_820_7348710886379401300-1/-local-10017/HashTable-Stage-28/MapJoin-mapfile51--.hashtable2023-01-05 07:29:15	End of local task; Time Taken: 0.988 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 6 out of 9
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0050, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0050/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0050
Hadoop job information for Stage-28: number of mappers: 3; number of reducers: 0
2023-01-05 07:29:26,785 Stage-28 map = 0%,  reduce = 0%
2023-01-05 07:29:43,556 Stage-28 map = 33%,  reduce = 0%, Cumulative CPU 13.67 sec
2023-01-05 07:29:44,574 Stage-28 map = 35%,  reduce = 0%, Cumulative CPU 30.72 sec
2023-01-05 07:29:50,671 Stage-28 map = 36%,  reduce = 0%, Cumulative CPU 36.63 sec
2023-01-05 07:29:51,687 Stage-28 map = 39%,  reduce = 0%, Cumulative CPU 57.48 sec
2023-01-05 07:29:55,759 Stage-28 map = 40%,  reduce = 0%, Cumulative CPU 63.34 sec
2023-01-05 07:29:57,795 Stage-28 map = 42%,  reduce = 0%, Cumulative CPU 66.44 sec
2023-01-05 07:30:01,868 Stage-28 map = 43%,  reduce = 0%, Cumulative CPU 70.51 sec
2023-01-05 07:30:02,884 Stage-28 map = 45%,  reduce = 0%, Cumulative CPU 74.06 sec
2023-01-05 07:30:07,973 Stage-28 map = 47%,  reduce = 0%, Cumulative CPU 77.82 sec
2023-01-05 07:30:08,988 Stage-28 map = 49%,  reduce = 0%, Cumulative CPU 80.77 sec
2023-01-05 07:30:14,072 Stage-28 map = 50%,  reduce = 0%, Cumulative CPU 84.27 sec
2023-01-05 07:30:15,089 Stage-28 map = 53%,  reduce = 0%, Cumulative CPU 87.1 sec
2023-01-05 07:30:20,169 Stage-28 map = 54%,  reduce = 0%, Cumulative CPU 90.29 sec
2023-01-05 07:30:21,191 Stage-28 map = 56%,  reduce = 0%, Cumulative CPU 92.18 sec
2023-01-05 07:30:26,266 Stage-28 map = 57%,  reduce = 0%, Cumulative CPU 95.97 sec
2023-01-05 07:30:27,280 Stage-28 map = 59%,  reduce = 0%, Cumulative CPU 98.77 sec
2023-01-05 07:30:32,353 Stage-28 map = 60%,  reduce = 0%, Cumulative CPU 103.4 sec
2023-01-05 07:30:33,372 Stage-28 map = 61%,  reduce = 0%, Cumulative CPU 108.33 sec
2023-01-05 07:30:38,447 Stage-28 map = 62%,  reduce = 0%, Cumulative CPU 113.36 sec
2023-01-05 07:30:39,469 Stage-28 map = 64%,  reduce = 0%, Cumulative CPU 118.36 sec
2023-01-05 07:30:44,554 Stage-28 map = 65%,  reduce = 0%, Cumulative CPU 123.28 sec
2023-01-05 07:30:45,566 Stage-28 map = 66%,  reduce = 0%, Cumulative CPU 127.87 sec
2023-01-05 07:30:50,641 Stage-28 map = 67%,  reduce = 0%, Cumulative CPU 132.74 sec
2023-01-05 07:30:51,656 Stage-28 map = 69%,  reduce = 0%, Cumulative CPU 136.81 sec
2023-01-05 07:30:55,909 Stage-28 map = 70%,  reduce = 0%, Cumulative CPU 140.46 sec
2023-01-05 07:30:56,923 Stage-28 map = 72%,  reduce = 0%, Cumulative CPU 143.77 sec
2023-01-05 07:31:01,997 Stage-28 map = 73%,  reduce = 0%, Cumulative CPU 147.1 sec
2023-01-05 07:31:03,012 Stage-28 map = 75%,  reduce = 0%, Cumulative CPU 149.69 sec
2023-01-05 07:31:08,112 Stage-28 map = 76%,  reduce = 0%, Cumulative CPU 154.1 sec
2023-01-05 07:31:09,132 Stage-28 map = 78%,  reduce = 0%, Cumulative CPU 156.87 sec
2023-01-05 07:31:14,215 Stage-28 map = 79%,  reduce = 0%, Cumulative CPU 159.19 sec
2023-01-05 07:31:15,230 Stage-28 map = 81%,  reduce = 0%, Cumulative CPU 161.26 sec
2023-01-05 07:31:20,309 Stage-28 map = 83%,  reduce = 0%, Cumulative CPU 164.83 sec
2023-01-05 07:31:21,324 Stage-28 map = 85%,  reduce = 0%, Cumulative CPU 167.62 sec
2023-01-05 07:31:26,411 Stage-28 map = 88%,  reduce = 0%, Cumulative CPU 173.1 sec
2023-01-05 07:31:32,517 Stage-28 map = 89%,  reduce = 0%, Cumulative CPU 178.05 sec
2023-01-05 07:31:44,718 Stage-28 map = 90%,  reduce = 0%, Cumulative CPU 186.78 sec
2023-01-05 07:31:50,819 Stage-28 map = 91%,  reduce = 0%, Cumulative CPU 191.93 sec
2023-01-05 07:31:55,903 Stage-28 map = 93%,  reduce = 0%, Cumulative CPU 196.17 sec
2023-01-05 07:32:01,998 Stage-28 map = 94%,  reduce = 0%, Cumulative CPU 199.58 sec
2023-01-05 07:32:08,087 Stage-28 map = 95%,  reduce = 0%, Cumulative CPU 202.54 sec
2023-01-05 07:32:14,179 Stage-28 map = 97%,  reduce = 0%, Cumulative CPU 206.11 sec
2023-01-05 07:32:20,269 Stage-28 map = 98%,  reduce = 0%, Cumulative CPU 209.24 sec
2023-01-05 07:32:26,372 Stage-28 map = 99%,  reduce = 0%, Cumulative CPU 214.31 sec
2023-01-05 07:32:34,498 Stage-28 map = 100%,  reduce = 0%, Cumulative CPU 220.67 sec
MapReduce Total cumulative CPU time: 3 minutes 40 seconds 670 msec
Ended Job = job_1672890466700_0050
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 07:32:44	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 07:32:45	End of local task; Time Taken: 1.193 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 9
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0051, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0051/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0051
Hadoop job information for Stage-18: number of mappers: 2; number of reducers: 2
2023-01-05 07:32:53,949 Stage-18 map = 0%,  reduce = 0%
2023-01-05 07:32:59,056 Stage-18 map = 50%,  reduce = 0%, Cumulative CPU 4.53 sec
2023-01-05 07:33:11,304 Stage-18 map = 71%,  reduce = 0%, Cumulative CPU 15.72 sec
2023-01-05 07:33:14,366 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 17.48 sec
2023-01-05 07:33:15,385 Stage-18 map = 100%,  reduce = 8%, Cumulative CPU 17.91 sec
2023-01-05 07:33:16,407 Stage-18 map = 100%,  reduce = 100%, Cumulative CPU 22.05 sec
MapReduce Total cumulative CPU time: 22 seconds 50 msec
Ended Job = job_1672890466700_0051
Launching Job 8 out of 9
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0052, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0052/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0052
Hadoop job information for Stage-4: number of mappers: 3; number of reducers: 1
2023-01-05 07:33:25,933 Stage-4 map = 0%,  reduce = 0%
2023-01-05 07:33:32,035 Stage-4 map = 33%,  reduce = 0%, Cumulative CPU 1.24 sec
2023-01-05 07:33:36,098 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 5.41 sec
2023-01-05 07:33:37,112 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.22 sec
MapReduce Total cumulative CPU time: 6 seconds 220 msec
Ended Job = job_1672890466700_0052
Launching Job 9 out of 9
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0053, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0053/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0053
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 07:33:44,740 Stage-5 map = 0%,  reduce = 0%
2023-01-05 07:33:48,808 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 0.97 sec
2023-01-05 07:33:52,872 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.16 sec
MapReduce Total cumulative CPU time: 2 seconds 160 msec
Ended Job = job_1672890466700_0053
MapReduce Jobs Launched:
Stage-Stage-24: Map: 5   Cumulative CPU: 1343.61 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-3: Map: 5  Reduce: 7   Cumulative CPU: 116.0 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-26: Map: 4   Cumulative CPU: 1396.48 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-11: Map: 1  Reduce: 1   Cumulative CPU: 10.02 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 3  Reduce: 2   Cumulative CPU: 1388.04 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-28: Map: 3   Cumulative CPU: 220.67 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-18: Map: 2  Reduce: 2   Cumulative CPU: 22.05 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 3  Reduce: 1   Cumulative CPU: 6.22 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.16 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 15 minutes 5 seconds 250 msec
OK
NULL	NULL	1141741031.35	30791039.24	-314419012.96
catalog channel	NULL	394326392.72	9666566.28	-44693847.72
catalog channel	catalog_pageAAAAAAAAAAABAAAA	0.00	33332.80	-23940.17
catalog channel	catalog_pageAAAAAAAAAABBAAAA	0.00	1960.89	-1588.49
catalog channel	catalog_pageAAAAAAAAAADBAAAA	1021035.63	1795.65	-60632.12
catalog channel	catalog_pageAAAAAAAAABABAAAA	1727047.13	12974.82	-210550.19
catalog channel	catalog_pageAAAAAAAAABBBAAAA	0.00	2584.60	-863.84
catalog channel	catalog_pageAAAAAAAAABCBAAAA	0.00	51.84	-160.98
catalog channel	catalog_pageAAAAAAAAABDBAAAA	876804.29	0.00	-117449.52
catalog channel	catalog_pageAAAAAAAAACABAAAA	1961789.58	26082.48	-208652.67
catalog channel	catalog_pageAAAAAAAAACBBAAAA	0.00	2534.62	-828.48
catalog channel	catalog_pageAAAAAAAAACCBAAAA	0.00	1367.30	-846.93
catalog channel	catalog_pageAAAAAAAAACJAAAAA	0.00	41.18	-100.81
catalog channel	catalog_pageAAAAAAAAADABAAAA	1887982.83	7719.98	-83850.02
catalog channel	catalog_pageAAAAAAAAADBBAAAA	0.00	8029.37	-3061.19
catalog channel	catalog_pageAAAAAAAAADCBAAAA	0.00	5822.40	-4601.36
catalog channel	catalog_pageAAAAAAAAADJAAAAA	0.00	7671.88	-3319.40
catalog channel	catalog_pageAAAAAAAAAEABAAAA	1712653.76	22192.32	-186536.83
catalog channel	catalog_pageAAAAAAAAAEBBAAAA	0.00	314.91	-110.06
catalog channel	catalog_pageAAAAAAAAAECBAAAA	0.00	15.64	-175.05
catalog channel	catalog_pageAAAAAAAAAEIAAAAA	0.00	275.74	-126.74
catalog channel	catalog_pageAAAAAAAAAEJAAAAA	0.00	7202.20	-4045.79
catalog channel	catalog_pageAAAAAAAAAFABAAAA	1582247.35	17231.62	-187323.33
catalog channel	catalog_pageAAAAAAAAAFIAAAAA	0.00	13936.57	-3704.77
catalog channel	catalog_pageAAAAAAAAAFJAAAAA	0.00	13.23	-69.82
catalog channel	catalog_pageAAAAAAAAAGABAAAA	2001863.21	20769.14	-256921.30
catalog channel	catalog_pageAAAAAAAAAGBBAAAA	0.00	10850.32	-3766.69
catalog channel	catalog_pageAAAAAAAAAGCBAAAA	0.00	165.95	-505.63
catalog channel	catalog_pageAAAAAAAAAGIAAAAA	0.00	258.23	-120.85
catalog channel	catalog_pageAAAAAAAAAGJAAAAA	0.00	2304.96	-1889.93
catalog channel	catalog_pageAAAAAAAAAHABAAAA	1870826.87	11527.78	-272905.70
catalog channel	catalog_pageAAAAAAAAAHBBAAAA	0.00	2901.82	-1229.28
catalog channel	catalog_pageAAAAAAAAAHCBAAAA	0.00	6618.84	-1862.54
catalog channel	catalog_pageAAAAAAAAAHIAAAAA	0.00	6037.54	-2642.84
catalog channel	catalog_pageAAAAAAAAAHJAAAAA	0.00	10819.02	-7512.62
catalog channel	catalog_pageAAAAAAAAAIABAAAA	785592.26	2349.72	-162126.49
catalog channel	catalog_pageAAAAAAAAAICBAAAA	0.00	261.43	-94.98
catalog channel	catalog_pageAAAAAAAAAIIAAAAA	0.00	3774.66	-1279.25
catalog channel	catalog_pageAAAAAAAAAIJAAAAA	0.00	3216.39	-287.19
catalog channel	catalog_pageAAAAAAAAAJABAAAA	923474.98	486.60	-49389.83
catalog channel	catalog_pageAAAAAAAAAJBBAAAA	0.00	1103.76	-7583.76
catalog channel	catalog_pageAAAAAAAAAJCBAAAA	0.00	3.42	-55.62
catalog channel	catalog_pageAAAAAAAAAJIAAAAA	0.00	1218.76	-636.78
catalog channel	catalog_pageAAAAAAAAAKABAAAA	788669.51	12265.62	-119384.26
catalog channel	catalog_pageAAAAAAAAAKBBAAAA	0.00	1551.24	-733.16
catalog channel	catalog_pageAAAAAAAAAKIAAAAA	0.00	2382.68	-2643.50
catalog channel	catalog_pageAAAAAAAAAKJAAAAA	0.00	2302.65	-344.66
catalog channel	catalog_pageAAAAAAAAAKMAAAAA	0.00	310.59	-157.84
catalog channel	catalog_pageAAAAAAAAAKPAAAAA	0.00	17376.65	-14351.42
catalog channel	catalog_pageAAAAAAAAALABAAAA	739727.26	863.25	-101659.93
catalog channel	catalog_pageAAAAAAAAALCBAAAA	872214.19	2202.13	-60258.44
catalog channel	catalog_pageAAAAAAAAALPAAAAA	0.00	51843.33	-25392.83
catalog channel	catalog_pageAAAAAAAAAMABAAAA	829156.92	10212.46	-129046.69
catalog channel	catalog_pageAAAAAAAAAMBBAAAA	0.00	411.45	-557.52
catalog channel	catalog_pageAAAAAAAAAMCBAAAA	964779.41	0.00	-30181.79
catalog channel	catalog_pageAAAAAAAAAMMAAAAA	0.00	4884.11	-4898.67
catalog channel	catalog_pageAAAAAAAAAMPAAAAA	0.00	47822.48	-24039.60
catalog channel	catalog_pageAAAAAAAAANABAAAA	872816.19	10083.31	-66943.23
catalog channel	catalog_pageAAAAAAAAANBBAAAA	0.00	385.98	-201.81
catalog channel	catalog_pageAAAAAAAAANCBAAAA	897976.26	0.00	-142305.06
catalog channel	catalog_pageAAAAAAAAANMAAAAA	0.00	36.64	-406.84
catalog channel	catalog_pageAAAAAAAAANPAAAAA	0.00	64431.04	-30848.01
catalog channel	catalog_pageAAAAAAAAAOABAAAA	896196.52	3847.95	-160692.25
catalog channel	catalog_pageAAAAAAAAAOBBAAAA	0.00	3492.02	-1655.64
catalog channel	catalog_pageAAAAAAAAAOCBAAAA	888658.53	0.00	-64360.73
catalog channel	catalog_pageAAAAAAAAAOMAAAAA	0.00	129.99	-83.66
catalog channel	catalog_pageAAAAAAAAAOPAAAAA	0.00	20873.11	-11841.70
catalog channel	catalog_pageAAAAAAAAAPABAAAA	0.00	3963.03	-8718.63
catalog channel	catalog_pageAAAAAAAAAPBBAAAA	0.00	3205.84	-1788.02
catalog channel	catalog_pageAAAAAAAAAPCBAAAA	750848.58	0.00	-109329.29
catalog channel	catalog_pageAAAAAAAAAPPAAAAA	0.00	25082.83	-17858.97
catalog channel	catalog_pageAAAAAAAABAABAAAA	0.00	73774.59	-33858.60
catalog channel	catalog_pageAAAAAAAABABBAAAA	0.00	1436.58	-674.07
catalog channel	catalog_pageAAAAAAAABACBAAAA	0.00	1227.06	-530.20
catalog channel	catalog_pageAAAAAAAABADBAAAA	853879.65	0.00	-114587.03
catalog channel	catalog_pageAAAAAAAABANAAAAA	0.00	349.11	-65.98
catalog channel	catalog_pageAAAAAAAABBABAAAA	1830654.43	11027.85	-183101.82
catalog channel	catalog_pageAAAAAAAABBBBAAAA	0.00	3350.92	-965.38
catalog channel	catalog_pageAAAAAAAABBDBAAAA	980895.40	0.00	-131954.89
catalog channel	catalog_pageAAAAAAAABCABAAAA	1809954.63	18413.08	-90119.45
catalog channel	catalog_pageAAAAAAAABCBBAAAA	0.00	619.43	-372.67
catalog channel	catalog_pageAAAAAAAABCJAAAAA	0.00	7848.40	-3233.81
catalog channel	catalog_pageAAAAAAAABDABAAAA	1651466.31	19037.94	-275826.43
catalog channel	catalog_pageAAAAAAAABDBBAAAA	0.00	14709.00	-1260.15
catalog channel	catalog_pageAAAAAAAABDCBAAAA	0.00	374.08	-296.07
catalog channel	catalog_pageAAAAAAAABDJAAAAA	0.00	3559.35	-3029.54
catalog channel	catalog_pageAAAAAAAABDMAAAAA	0.00	171.12	-1812.89
catalog channel	catalog_pageAAAAAAAABEABAAAA	1711752.48	7935.13	-127373.32
catalog channel	catalog_pageAAAAAAAABEBBAAAA	0.00	6651.90	-4413.73
catalog channel	catalog_pageAAAAAAAABECBAAAA	0.00	1675.38	-2605.72
catalog channel	catalog_pageAAAAAAAABEIAAAAA	0.00	11761.27	-7781.90
catalog channel	catalog_pageAAAAAAAABEJAAAAA	0.00	315.21	-327.86
catalog channel	catalog_pageAAAAAAAABEMAAAAA	0.00	66.24	-137.64
catalog channel	catalog_pageAAAAAAAABFABAAAA	1805057.31	9012.97	-223349.14
catalog channel	catalog_pageAAAAAAAABFBBAAAA	0.00	128.82	-144.08
catalog channel	catalog_pageAAAAAAAABFCBAAAA	0.00	3891.54	-1112.19
catalog channel	catalog_pageAAAAAAAABFIAAAAA	0.00	3279.69	-3810.05
catalog channel	catalog_pageAAAAAAAABFJAAAAA	0.00	8488.73	-1921.83
catalog channel	catalog_pageAAAAAAAABGABAAAA	1728285.02	29405.32	-326593.12
catalog channel	catalog_pageAAAAAAAABGBBAAAA	0.00	5038.79	-3195.80
Time taken: 1143.862 seconds, Fetched: 100 row(s)
timediff:1149.417129355
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query6.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = f2dd9549-d971-4c12-b79d-596185414851

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = c69d5098-8b6a-4bc4-a233-fe4427d69ea7
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_state, ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_current_addr_sk, c_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk, ss_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_month_seq, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_category, i_current_price, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_moy, d_month_seq, d_year
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_category, i_current_price
Warning: Map Join MAPJOIN[168][bigTable=?] in task 'Stage-28:MAPRED' is a cross product
Warning: Map Join MAPJOIN[175][bigTable=?] in task 'Stage-29:MAPRED' is a cross product
Warning: Shuffle Join JOIN[35][tables = [$hdt$_4, $hdt$_5]] in Stage 'Stage-8:MAPRED' is a cross product
Query ID = hdfs_20230105073400_1992a78d-9f9d-4be0-af16-2d458288e07e
Total jobs = 21
Launching Job 1 out of 21
Launching Job 2 out of 21
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks not specified. Estimated from input data size: 1
Launching Job 3 out of 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0055, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0055/
Starting Job = job_1672890466700_0054, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0054/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0055
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0054
Starting Job = job_1672890466700_0056, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0056/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0056
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 1
2023-01-05 07:34:21,026 Stage-11 map = 0%,  reduce = 0%
Hadoop job information for Stage-16: number of mappers: 1; number of reducers: 1
2023-01-05 07:34:21,797 Stage-16 map = 0%,  reduce = 0%
2023-01-05 07:34:19	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 07:34:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	11845440percentage:	0.4952023-01-05 07:34:21	Dump the side-table for tag: 1 with group count: 250000 into file: file:/tmp/hdfs/f2dd9549-d971-4c12-b79d-596185414851/hive_2023-01-05_07-34-00_195_6374023958784900748-1/-local-10036/HashTable-Stage-23/MapJoin-mapfile41--.hashtable2023-01-05 07:34:21	Uploaded 1 File to: file:/tmp/hdfs/f2dd9549-d971-4c12-b79d-596185414851/hive_2023-01-05_07-34-00_195_6374023958784900748-1/-local-10036/HashTable-Stage-23/MapJoin-mapfile41--.hashtable (6191766 bytes)
Execution completed successfully
MapredLocal task succeeded
Hadoop job information for Stage-7: number of mappers: 1; number of reducers: 1
2023-01-05 07:34:22,713 Stage-7 map = 0%,  reduce = 0%
Launching Job 4 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
2023-01-05 07:34:25,262 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 2.5 sec
Starting Job = job_1672890466700_0057, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0057/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0057
2023-01-05 07:34:26,839 Stage-7 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec
2023-01-05 07:34:27,894 Stage-16 map = 100%,  reduce = 0%, Cumulative CPU 1.46 sec
2023-01-05 07:34:29,358 Stage-11 map = 100%,  reduce = 100%, Cumulative CPU 3.36 sec
MapReduce Total cumulative CPU time: 3 seconds 360 msec
Ended Job = job_1672890466700_0055
2023-01-05 07:34:31,929 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 4.81 sec
Launching Job 5 out of 21
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
MapReduce Total cumulative CPU time: 4 seconds 810 msec
Ended Job = job_1672890466700_0056
Starting Job = job_1672890466700_0058, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0058/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0058
2023-01-05 07:34:33,988 Stage-16 map = 100%,  reduce = 100%, Cumulative CPU 2.68 sec
MapReduce Total cumulative CPU time: 2 seconds 680 msec
Ended Job = job_1672890466700_0054
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
Hadoop job information for Stage-23: number of mappers: 1; number of reducers: 0
2023-01-05 07:34:40,947 Stage-23 map = 0%,  reduce = 0%
Hadoop job information for Stage-12: number of mappers: 1; number of reducers: 1
2023-01-05 07:34:42,404 Stage-12 map = 0%,  reduce = 0%


2023-01-05 07:34:43	End of local task; Time Taken: 1.347 sec.
Execution completed successfully
MapredLocal task succeeded
2023-01-05 07:34:45,473 Stage-12 map = 100%,  reduce = 0%, Cumulative CPU 0.62 sec
Launching Job 6 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0059, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0059/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0059
2023-01-05 07:34:50,142 Stage-23 map = 100%,  reduce = 0%, Cumulative CPU 7.48 sec
2023-01-05 07:34:50,602 Stage-12 map = 100%,  reduce = 100%, Cumulative CPU 2.01 sec
MapReduce Total cumulative CPU time: 7 seconds 480 msec
Ended Job = job_1672890466700_0057
MapReduce Total cumulative CPU time: 2 seconds 10 msec
Ended Job = job_1672890466700_0058
Hadoop job information for Stage-31: number of mappers: 1; number of reducers: 0
2023-01-05 07:34:53,867 Stage-31 map = 0%,  reduce = 0%
Stage-39 is selected by condition resolver.
Stage-40 is filtered out by condition resolver.
Stage-8 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2023-01-05 07:34:59,986 Stage-31 map = 100%,  reduce = 0%, Cumulative CPU 0.73 sec
2023-01-05 07:35:01	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 07:35:01	End of local task; Time Taken: 0.394 sec.
MapReduce Total cumulative CPU time: 730 msec
Ended Job = job_1672890466700_0059
Execution completed successfully
MapredLocal task succeeded
Launching Job 8 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0060, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0060/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0060
Hadoop job information for Stage-28: number of mappers: 1; number of reducers: 0
2023-01-05 07:35:12,856 Stage-28 map = 0%,  reduce = 0%
2023-01-05 07:35:18,941 Stage-28 map = 100%,  reduce = 0%, Cumulative CPU 0.9 sec
MapReduce Total cumulative CPU time: 900 msec
Ended Job = job_1672890466700_0060

SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 07:35:28	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 07:35:30	End of local task; Time Taken: 1.901 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 9 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0061, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0061/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0061
Hadoop job information for Stage-27: number of mappers: 1; number of reducers: 0
2023-01-05 07:35:37,988 Stage-27 map = 0%,  reduce = 0%
2023-01-05 07:35:42,065 Stage-27 map = 100%,  reduce = 0%, Cumulative CPU 2.59 sec
MapReduce Total cumulative CPU time: 2 seconds 590 msec
Ended Job = job_1672890466700_0061
Stage-36 is filtered out by condition resolver.
Stage-37 is selected by condition resolver.
Stage-10 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 07:35:53	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 07:35:54	End of local task; Time Taken: 0.56 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 11 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0062, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0062/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0062
Hadoop job information for Stage-25: number of mappers: 4; number of reducers: 0
2023-01-05 07:36:08,597 Stage-25 map = 0%,  reduce = 0%
2023-01-05 07:36:24,154 Stage-25 map = 3%,  reduce = 0%, Cumulative CPU 10.76 sec
2023-01-05 07:36:26,186 Stage-25 map = 10%,  reduce = 0%, Cumulative CPU 41.54 sec
2023-01-05 07:36:30,255 Stage-25 map = 12%,  reduce = 0%, Cumulative CPU 43.03 sec
2023-01-05 07:36:32,294 Stage-25 map = 17%,  reduce = 0%, Cumulative CPU 51.32 sec
2023-01-05 07:36:36,371 Stage-25 map = 19%,  reduce = 0%, Cumulative CPU 53.22 sec
2023-01-05 07:36:38,402 Stage-25 map = 23%,  reduce = 0%, Cumulative CPU 61.7 sec
2023-01-05 07:36:42,471 Stage-25 map = 25%,  reduce = 0%, Cumulative CPU 63.53 sec
2023-01-05 07:36:44,504 Stage-25 map = 30%,  reduce = 0%, Cumulative CPU 73.31 sec
2023-01-05 07:36:48,578 Stage-25 map = 32%,  reduce = 0%, Cumulative CPU 75.67 sec
2023-01-05 07:36:49,598 Stage-25 map = 33%,  reduce = 0%, Cumulative CPU 78.8 sec
2023-01-05 07:36:50,616 Stage-25 map = 37%,  reduce = 0%, Cumulative CPU 83.27 sec
2023-01-05 07:36:54,680 Stage-25 map = 38%,  reduce = 0%, Cumulative CPU 86.5 sec
2023-01-05 07:36:55,698 Stage-25 map = 39%,  reduce = 0%, Cumulative CPU 89.48 sec
2023-01-05 07:36:56,711 Stage-25 map = 43%,  reduce = 0%, Cumulative CPU 93.94 sec
2023-01-05 07:37:00,775 Stage-25 map = 44%,  reduce = 0%, Cumulative CPU 97.08 sec
2023-01-05 07:37:01,795 Stage-25 map = 45%,  reduce = 0%, Cumulative CPU 100.24 sec
2023-01-05 07:37:02,823 Stage-25 map = 49%,  reduce = 0%, Cumulative CPU 104.94 sec
2023-01-05 07:37:05,874 Stage-25 map = 50%,  reduce = 0%, Cumulative CPU 107.85 sec
2023-01-05 07:37:07,907 Stage-25 map = 53%,  reduce = 0%, Cumulative CPU 114.62 sec
2023-01-05 07:37:08,922 Stage-25 map = 55%,  reduce = 0%, Cumulative CPU 116.66 sec
2023-01-05 07:37:11,968 Stage-25 map = 56%,  reduce = 0%, Cumulative CPU 120.66 sec
2023-01-05 07:37:14,004 Stage-25 map = 59%,  reduce = 0%, Cumulative CPU 126.24 sec
2023-01-05 07:37:15,020 Stage-25 map = 61%,  reduce = 0%, Cumulative CPU 127.9 sec
2023-01-05 07:37:18,277 Stage-25 map = 62%,  reduce = 0%, Cumulative CPU 130.78 sec
2023-01-05 07:37:20,315 Stage-25 map = 68%,  reduce = 0%, Cumulative CPU 138.83 sec
2023-01-05 07:37:24,378 Stage-25 map = 69%,  reduce = 0%, Cumulative CPU 141.82 sec
2023-01-05 07:37:26,409 Stage-25 map = 73%,  reduce = 0%, Cumulative CPU 148.76 sec
2023-01-05 07:37:30,467 Stage-25 map = 75%,  reduce = 0%, Cumulative CPU 150.99 sec
2023-01-05 07:37:32,501 Stage-25 map = 80%,  reduce = 0%, Cumulative CPU 158.78 sec
2023-01-05 07:37:36,569 Stage-25 map = 82%,  reduce = 0%, Cumulative CPU 161.61 sec
2023-01-05 07:37:38,595 Stage-25 map = 86%,  reduce = 0%, Cumulative CPU 169.18 sec
2023-01-05 07:37:42,665 Stage-25 map = 88%,  reduce = 0%, Cumulative CPU 171.72 sec
2023-01-05 07:37:43,678 Stage-25 map = 91%,  reduce = 0%, Cumulative CPU 173.88 sec
2023-01-05 07:37:44,695 Stage-25 map = 94%,  reduce = 0%, Cumulative CPU 178.28 sec
2023-01-05 07:37:48,764 Stage-25 map = 95%,  reduce = 0%, Cumulative CPU 179.85 sec
2023-01-05 07:37:49,777 Stage-25 map = 98%,  reduce = 0%, Cumulative CPU 184.57 sec
2023-01-05 07:37:52,819 Stage-25 map = 100%,  reduce = 0%, Cumulative CPU 185.7 sec
MapReduce Total cumulative CPU time: 3 minutes 5 seconds 700 msec
Ended Job = job_1672890466700_0062
Stage-34 is filtered out by condition resolver.
Stage-35 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2023-01-05 07:38:01	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 07:38:03	Processing rows:	300000	Hashtable size:	299999	Memory usage:	137528832	percentage:	0.575
2023-01-05 07:38:07	Processing rows:	500000	Hashtable size:	499999	Memory usage:	204502144	percentage:	0.8552023-01-05 07:38:07	Dump the side-table for tag: 0 with group count: 500000 into file: file:/tmp/hdfs/f2dd9549-d971-4c12-b79d-596185414851/hive_2023-01-05_07-34-00_195_6374023958784900748-1/-local-10022/HashTable-Stage-21/MapJoin-mapfile30--.hashtable
2023-01-05 07:38:14	End of local task; Time Taken: 12.65 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 13 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0063, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0063/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0063
Hadoop job information for Stage-21: number of mappers: 1; number of reducers: 0
2023-01-05 07:38:24,547 Stage-21 map = 0%,  reduce = 0%
2023-01-05 07:38:37,265 Stage-21 map = 100%,  reduce = 0%, Cumulative CPU 14.91 sec
MapReduce Total cumulative CPU time: 14 seconds 910 msec
Ended Job = job_1672890466700_0063
Stage-32 is selected by condition resolver.
Stage-33 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 07:38:47	Dump the side-table for tag: 1 with group count: 31 into file: file:/tmp/hdfs/f2dd9549-d971-4c12-b79d-596185414851/hive_2023-01-05_07-34-00_195_6374023958784900748-1/-local-10016/HashTable-Stage-17/MapJoin-mapfile01--.hashtable2023-01-05 07:38:47	Uploaded 1 File to: file:/tmp/hdfs/f2dd9549-d971-4c12-b79d-596185414851/hive_2023-01-05_07-34-00_195_6374023958784900748-1/-local-10016/HashTable-Stage-17/MapJoin-mapfile01--.hashtable (914 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 15 out of 21
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0064, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0064/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0064
Hadoop job information for Stage-17: number of mappers: 1; number of reducers: 0
2023-01-05 07:38:57,500 Stage-17 map = 0%,  reduce = 0%
2023-01-05 07:39:03,594 Stage-17 map = 100%,  reduce = 0%, Cumulative CPU 3.18 sec
MapReduce Total cumulative CPU time: 3 seconds 180 msec
Ended Job = job_1672890466700_0064
Launching Job 16 out of 21
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0065, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0065/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0065
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 07:39:12,941 Stage-4 map = 0%,  reduce = 0%
2023-01-05 07:39:15,999 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 0.62 sec
2023-01-05 07:39:20,071 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.04 sec
MapReduce Total cumulative CPU time: 2 seconds 40 msec
Ended Job = job_1672890466700_0065
Launching Job 17 out of 21
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0066, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0066/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0066
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 07:39:30,454 Stage-5 map = 0%,  reduce = 0%
2023-01-05 07:39:36,762 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 0.88 sec
2023-01-05 07:39:41,849 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 1.67 sec
MapReduce Total cumulative CPU time: 1 seconds 670 msec
Ended Job = job_1672890466700_0066
MapReduce Jobs Launched:
Stage-Stage-11: Map: 1  Reduce: 1   Cumulative CPU: 3.36 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-7: Map: 1  Reduce: 1   Cumulative CPU: 4.81 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-16: Map: 1  Reduce: 1   Cumulative CPU: 2.68 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-23: Map: 1   Cumulative CPU: 7.48 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-12: Map: 1  Reduce: 1   Cumulative CPU: 2.01 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-31: Map: 1   Cumulative CPU: 0.73 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-28: Map: 1   Cumulative CPU: 0.9 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-27: Map: 1   Cumulative CPU: 2.59 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-25: Map: 4   Cumulative CPU: 185.7 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-21: Map: 1   Cumulative CPU: 14.91 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-17: Map: 1   Cumulative CPU: 3.18 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 2.04 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 1.67 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 52 seconds 60 msec
OK
HI	26
DE	34
RI	35
CT	56
NH	96
VT	110
ME	117
AZ	126
MA	133
NV	136
WY	148
AK	159
NJ	163
MD	177
UT	236
NM	246
OR	246
SC	302
WA	325
ID	357
MT	407
ND	411
WV	442
CO	454
CA	462
NY	477
LA	481
SD	501
PA	513
AL	523
FL	528
WI	541
AR	615
MS	621
MI	626
OK	636
OH	694
MN	699
IN	716
TN	737
NULL	739
NC	756
NE	778
KS	790
IL	793
IA	808
MO	888
VA	1002
KY	1005
GA	1279
TX	1990
Time taken: 343.966 seconds, Fetched: 51 row(s)
timediff:349.510415693
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query7.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 8af00431-678e-4dec-b946-f43a73a28bdc

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 6bd595f9-c604-4014-94f9-e245fafa6fbc
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_cdemo_sk, ss_promo_sk, ss_item_sk, ss_coupon_amt, ss_quantity, ss_list_price, ss_sales_price
No Stats for tpcds_bin_partitioned_orc_10@customer_demographics, Columns: cd_demo_sk, cd_education_status, cd_marital_status, cd_gender
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_id, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@promotion, Columns: p_channel_event, p_channel_email, p_promo_sk
Query ID = hdfs_20230105073949_f27d059f-9725-4c66-82e8-a29a5fd1c265
Total jobs = 2


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2023-01-05 07:40:08	Uploaded 1 File to: file:/tmp/hdfs/8af00431-678e-4dec-b946-f43a73a28bdc/hive_2023-01-05_07-39-49_832_9207298050844728759-1/-local-10009/HashTable-Stage-5/MapJoin-mapfile01--.hashtable (3931446 bytes)

2023-01-05 07:40:08	Uploaded 1 File to: file:/tmp/hdfs/8af00431-678e-4dec-b946-f43a73a28bdc/hive_2023-01-05_07-39-49_832_9207298050844728759-1/-local-10009/HashTable-Stage-5/MapJoin-mapfile11--.hashtable (9887 bytes)2023-01-05 07:40:08	Dump the side-table for tag: 1 with group count: 365 into file: file:/tmp/hdfs/8af00431-678e-4dec-b946-f43a73a28bdc/hive_2023-01-05_07-39-49_832_9207298050844728759-1/-local-10009/HashTable-Stage-5/MapJoin-mapfile21--.hashtable2023-01-05 07:40:08	Uploaded 1 File to: file:/tmp/hdfs/8af00431-678e-4dec-b946-f43a73a28bdc/hive_2023-01-05_07-39-49_832_9207298050844728759-1/-local-10009/HashTable-Stage-5/MapJoin-mapfile21--.hashtable (7963 bytes)
2023-01-05 07:40:08	Dump the side-table for tag: 1 with group count: 27440 into file: file:/tmp/hdfs/8af00431-678e-4dec-b946-f43a73a28bdc/hive_2023-01-05_07-39-49_832_9207298050844728759-1/-local-10009/HashTable-Stage-5/MapJoin-mapfile31--.hashtable
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0067, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0067/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0067
Hadoop job information for Stage-5: number of mappers: 4; number of reducers: 4
2023-01-05 07:40:24,204 Stage-5 map = 0%,  reduce = 0%
2023-01-05 07:40:39,718 Stage-5 map = 1%,  reduce = 0%, Cumulative CPU 10.93 sec
2023-01-05 07:40:41,757 Stage-5 map = 2%,  reduce = 0%, Cumulative CPU 34.15 sec
2023-01-05 07:40:42,773 Stage-5 map = 3%,  reduce = 0%, Cumulative CPU 48.07 sec
2023-01-05 07:40:46,854 Stage-5 map = 4%,  reduce = 0%, Cumulative CPU 50.33 sec
2023-01-05 07:40:48,898 Stage-5 map = 5%,  reduce = 0%, Cumulative CPU 54.91 sec
2023-01-05 07:40:52,967 Stage-5 map = 6%,  reduce = 0%, Cumulative CPU 57.33 sec
2023-01-05 07:40:53,986 Stage-5 map = 7%,  reduce = 0%, Cumulative CPU 61.15 sec
2023-01-05 07:40:58,049 Stage-5 map = 8%,  reduce = 0%, Cumulative CPU 61.96 sec
2023-01-05 07:41:00,077 Stage-5 map = 9%,  reduce = 0%, Cumulative CPU 65.85 sec
2023-01-05 07:41:06,163 Stage-5 map = 10%,  reduce = 0%, Cumulative CPU 70.18 sec
2023-01-05 07:41:10,230 Stage-5 map = 11%,  reduce = 0%, Cumulative CPU 71.03 sec
2023-01-05 07:41:12,276 Stage-5 map = 12%,  reduce = 0%, Cumulative CPU 76.35 sec
2023-01-05 07:41:15,325 Stage-5 map = 13%,  reduce = 0%, Cumulative CPU 77.25 sec
2023-01-05 07:41:18,373 Stage-5 map = 14%,  reduce = 0%, Cumulative CPU 82.36 sec
2023-01-05 07:41:23,459 Stage-5 map = 15%,  reduce = 0%, Cumulative CPU 84.69 sec
2023-01-05 07:41:24,472 Stage-5 map = 16%,  reduce = 0%, Cumulative CPU 87.7 sec
2023-01-05 07:41:30,560 Stage-5 map = 18%,  reduce = 0%, Cumulative CPU 95.45 sec
2023-01-05 07:41:36,634 Stage-5 map = 19%,  reduce = 0%, Cumulative CPU 99.85 sec
2023-01-05 07:41:40,689 Stage-5 map = 20%,  reduce = 0%, Cumulative CPU 102.12 sec
2023-01-05 07:41:42,714 Stage-5 map = 21%,  reduce = 0%, Cumulative CPU 104.05 sec
2023-01-05 07:41:47,788 Stage-5 map = 22%,  reduce = 0%, Cumulative CPU 108.8 sec
2023-01-05 07:41:51,837 Stage-5 map = 23%,  reduce = 0%, Cumulative CPU 110.3 sec
2023-01-05 07:41:53,865 Stage-5 map = 24%,  reduce = 0%, Cumulative CPU 113.18 sec
2023-01-05 07:41:58,930 Stage-5 map = 25%,  reduce = 0%, Cumulative CPU 116.7 sec
2023-01-05 07:42:00,971 Stage-5 map = 26%,  reduce = 0%, Cumulative CPU 118.85 sec
2023-01-05 07:42:06,073 Stage-5 map = 27%,  reduce = 0%, Cumulative CPU 124.23 sec
2023-01-05 07:42:07,085 Stage-5 map = 28%,  reduce = 0%, Cumulative CPU 125.01 sec
2023-01-05 07:42:12,201 Stage-5 map = 30%,  reduce = 0%, Cumulative CPU 131.88 sec
2023-01-05 07:42:17,268 Stage-5 map = 31%,  reduce = 0%, Cumulative CPU 136.48 sec
2023-01-05 07:42:21,363 Stage-5 map = 32%,  reduce = 0%, Cumulative CPU 142.73 sec
2023-01-05 07:42:24,408 Stage-5 map = 33%,  reduce = 0%, Cumulative CPU 145.97 sec
2023-01-05 07:42:29,482 Stage-5 map = 34%,  reduce = 0%, Cumulative CPU 149.26 sec
2023-01-05 07:42:30,499 Stage-5 map = 35%,  reduce = 0%, Cumulative CPU 150.53 sec
2023-01-05 07:42:35,567 Stage-5 map = 36%,  reduce = 0%, Cumulative CPU 154.71 sec
2023-01-05 07:42:39,633 Stage-5 map = 37%,  reduce = 0%, Cumulative CPU 158.36 sec
2023-01-05 07:42:42,682 Stage-5 map = 38%,  reduce = 0%, Cumulative CPU 161.28 sec
2023-01-05 07:42:46,732 Stage-5 map = 39%,  reduce = 0%, Cumulative CPU 164.67 sec
2023-01-05 07:42:48,759 Stage-5 map = 40%,  reduce = 0%, Cumulative CPU 168.07 sec
2023-01-05 07:42:52,821 Stage-5 map = 41%,  reduce = 0%, Cumulative CPU 173.03 sec
2023-01-05 07:42:54,858 Stage-5 map = 42%,  reduce = 0%, Cumulative CPU 175.48 sec
2023-01-05 07:42:58,918 Stage-5 map = 43%,  reduce = 0%, Cumulative CPU 179.74 sec
2023-01-05 07:43:00,945 Stage-5 map = 44%,  reduce = 0%, Cumulative CPU 182.01 sec
2023-01-05 07:43:06,014 Stage-5 map = 45%,  reduce = 0%, Cumulative CPU 185.83 sec
2023-01-05 07:43:10,068 Stage-5 map = 46%,  reduce = 0%, Cumulative CPU 189.3 sec
2023-01-05 07:43:12,094 Stage-5 map = 47%,  reduce = 0%, Cumulative CPU 191.65 sec
2023-01-05 07:43:16,148 Stage-5 map = 48%,  reduce = 0%, Cumulative CPU 194.69 sec
2023-01-05 07:43:18,173 Stage-5 map = 49%,  reduce = 0%, Cumulative CPU 196.36 sec
2023-01-05 07:43:23,234 Stage-5 map = 50%,  reduce = 0%, Cumulative CPU 200.72 sec
2023-01-05 07:43:28,300 Stage-5 map = 51%,  reduce = 0%, Cumulative CPU 203.91 sec
2023-01-05 07:43:30,330 Stage-5 map = 52%,  reduce = 0%, Cumulative CPU 208.88 sec
2023-01-05 07:43:33,370 Stage-5 map = 53%,  reduce = 0%, Cumulative CPU 210.38 sec
2023-01-05 07:43:36,406 Stage-5 map = 54%,  reduce = 0%, Cumulative CPU 215.12 sec
2023-01-05 07:43:39,444 Stage-5 map = 55%,  reduce = 0%, Cumulative CPU 216.48 sec
2023-01-05 07:43:42,495 Stage-5 map = 56%,  reduce = 0%, Cumulative CPU 221.02 sec
2023-01-05 07:43:47,556 Stage-5 map = 57%,  reduce = 0%, Cumulative CPU 223.24 sec
2023-01-05 07:43:48,568 Stage-5 map = 58%,  reduce = 0%, Cumulative CPU 228.27 sec
2023-01-05 07:43:53,646 Stage-5 map = 59%,  reduce = 0%, Cumulative CPU 229.8 sec
2023-01-05 07:43:54,660 Stage-5 map = 60%,  reduce = 0%, Cumulative CPU 232.89 sec
2023-01-05 07:43:58,710 Stage-5 map = 61%,  reduce = 0%, Cumulative CPU 234.57 sec
2023-01-05 07:44:03,770 Stage-5 map = 62%,  reduce = 0%, Cumulative CPU 238.29 sec
2023-01-05 07:44:06,804 Stage-5 map = 63%,  reduce = 0%, Cumulative CPU 242.18 sec
2023-01-05 07:44:09,836 Stage-5 map = 72%,  reduce = 0%, Cumulative CPU 243.71 sec
2023-01-05 07:44:11,860 Stage-5 map = 81%,  reduce = 0%, Cumulative CPU 245.09 sec
2023-01-05 07:44:15,963 Stage-5 map = 82%,  reduce = 0%, Cumulative CPU 246.93 sec
2023-01-05 07:44:16,977 Stage-5 map = 91%,  reduce = 0%, Cumulative CPU 247.91 sec
2023-01-05 07:44:25,070 Stage-5 map = 91%,  reduce = 13%, Cumulative CPU 249.66 sec
2023-01-05 07:44:26,080 Stage-5 map = 100%,  reduce = 19%, Cumulative CPU 251.57 sec
2023-01-05 07:44:27,112 Stage-5 map = 100%,  reduce = 25%, Cumulative CPU 251.98 sec
2023-01-05 07:44:28,133 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 260.58 sec
MapReduce Total cumulative CPU time: 4 minutes 20 seconds 580 msec
Ended Job = job_1672890466700_0067
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0068, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0068/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0068
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2023-01-05 07:44:39,053 Stage-6 map = 0%,  reduce = 0%
2023-01-05 07:44:43,138 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-01-05 07:44:48,203 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1672890466700_0068
MapReduce Jobs Launched:
Stage-Stage-5: Map: 4  Reduce: 4   Cumulative CPU: 260.58 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-6: Map: 1  Reduce: 1   Cumulative CPU: 1.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 21 seconds 990 msec
OK
AAAAAAAAAAAABAAA	53.2	71.8600000000000000000000	46.2200000000000000000000	56.2060000000000000000000
AAAAAAAAAAABBAAA	46.666666666666664	67.7433333333333333333333	0.0000000000000000000000	24.2066666666666666666667
AAAAAAAAAAACAAAA	4.0	91.7400000000000000000000	0.0000000000000000000000	37.6100000000000000000000
AAAAAAAAAAAEBAAA	95.0	105.6450000000000000000000	32.7400000000000000000000	44.1550000000000000000000
AAAAAAAAAAAFAAAA	54.0	43.6400000000000000000000	0.0000000000000000000000	7.4100000000000000000000
AAAAAAAAAAAGBAAA	43.0	59.9600000000000000000000	0.0000000000000000000000	26.1700000000000000000000
AAAAAAAAAAAHAAAA	68.66666666666667	47.6233333333333333333333	2653.7466666666666666666667	41.1333333333333333333333
AAAAAAAAAAAHBAAA	82.5	93.7000000000000000000000	0.0000000000000000000000	38.4100000000000000000000
AAAAAAAAAAAIAAAA	71.0	96.8500000000000000000000	0.0000000000000000000000	85.2200000000000000000000
AAAAAAAAAAAKAAAA	42.0	27.4700000000000000000000	0.0000000000000000000000	20.3200000000000000000000
AAAAAAAAAAANAAAA	19.0	143.5850000000000000000000	0.0000000000000000000000	88.2100000000000000000000
AAAAAAAAAAAOAAAA	18.0	37.9400000000000000000000	0.0000000000000000000000	14.4100000000000000000000
AAAAAAAAAABAAAAA	6.0	25.4800000000000000000000	114.9400000000000000000000	20.3800000000000000000000
AAAAAAAAAABABAAA	13.0	86.7300000000000000000000	49.4900000000000000000000	55.2100000000000000000000
AAAAAAAAAABBAAAA	81.5	48.3350000000000000000000	0.0000000000000000000000	30.3600000000000000000000
AAAAAAAAAABCBAAA	69.0	42.5400000000000000000000	0.0000000000000000000000	33.6000000000000000000000
AAAAAAAAAABEAAAA	51.5	100.8850000000000000000000	0.0000000000000000000000	66.9900000000000000000000
AAAAAAAAAABFBAAA	56.2	73.1380000000000000000000	111.2120000000000000000000	30.4780000000000000000000
AAAAAAAAAABGAAAA	75.0	94.4875000000000000000000	185.6575000000000000000000	36.3850000000000000000000
AAAAAAAAAABGBAAA	14.0	8.2000000000000000000000	0.0000000000000000000000	1.4700000000000000000000
AAAAAAAAAABHAAAA	71.66666666666667	59.8200000000000000000000	521.0433333333333333333333	29.9033333333333333333333
AAAAAAAAAABIBAAA	52.666666666666664	110.1400000000000000000000	1420.6766666666666666666667	48.7266666666666666666667
AAAAAAAAAABKAAAA	64.0	50.8500000000000000000000	446.3800000000000000000000	24.9100000000000000000000
AAAAAAAAAABMAAAA	47.0	102.7200000000000000000000	0.0000000000000000000000	63.6800000000000000000000
AAAAAAAAAABNAAAA	57.0	25.0000000000000000000000	0.0000000000000000000000	5.6600000000000000000000
AAAAAAAAAABPAAAA	53.0	40.8200000000000000000000	1278.8800000000000000000000	29.7900000000000000000000
AAAAAAAAAACCAAAA	48.333333333333336	35.0700000000000000000000	0.0000000000000000000000	10.5533333333333333333333
AAAAAAAAAACDAAAA	83.0	128.5200000000000000000000	0.0000000000000000000000	56.5400000000000000000000
AAAAAAAAAACEBAAA	70.5	51.0950000000000000000000	0.0000000000000000000000	20.9750000000000000000000
AAAAAAAAAACFAAAA	56.75	64.9850000000000000000000	295.0425000000000000000000	32.7875000000000000000000
AAAAAAAAAACGAAAA	4.0	82.0600000000000000000000	0.0000000000000000000000	80.4100000000000000000000
AAAAAAAAAACHBAAA	51.0	82.1300000000000000000000	0.0000000000000000000000	59.9500000000000000000000
AAAAAAAAAACIBAAA	58.5	55.5700000000000000000000	0.0000000000000000000000	53.5000000000000000000000
AAAAAAAAAACJAAAA	31.0	73.1100000000000000000000	0.0000000000000000000000	36.9500000000000000000000
AAAAAAAAAACLAAAA	43.666666666666664	36.9333333333333333333333	0.0000000000000000000000	23.5300000000000000000000
AAAAAAAAAACMAAAA	50.0	46.5900000000000000000000	809.9300000000000000000000	21.8900000000000000000000
AAAAAAAAAACOAAAA	88.0	11.5400000000000000000000	0.0000000000000000000000	1.5000000000000000000000
AAAAAAAAAACPAAAA	68.0	106.5225000000000000000000	6.8475000000000000000000	49.5800000000000000000000
AAAAAAAAAADABAAA	10.5	75.0350000000000000000000	0.0000000000000000000000	23.8150000000000000000000
AAAAAAAAAADBBAAA	58.0	28.9900000000000000000000	0.0000000000000000000000	11.0100000000000000000000
AAAAAAAAAADCAAAA	33.0	7.1400000000000000000000	0.0000000000000000000000	0.9900000000000000000000
AAAAAAAAAADEAAAA	8.5	38.1900000000000000000000	0.0000000000000000000000	17.7550000000000000000000
AAAAAAAAAADEBAAA	73.0	152.2300000000000000000000	0.0000000000000000000000	36.5300000000000000000000
AAAAAAAAAADFAAAA	84.0	69.1600000000000000000000	NULL	NULL
AAAAAAAAAADGBAAA	34.0	48.8800000000000000000000	0.0000000000000000000000	11.7300000000000000000000
AAAAAAAAAADHAAAA	71.0	86.8400000000000000000000	0.0000000000000000000000	54.7000000000000000000000
AAAAAAAAAADHBAAA	22.0	109.8700000000000000000000	0.0000000000000000000000	51.3700000000000000000000
AAAAAAAAAADIAAAA	48.0	39.4675000000000000000000	270.2775000000000000000000	32.1125000000000000000000
AAAAAAAAAADKAAAA	58.0	79.0050000000000000000000	0.0000000000000000000000	29.9050000000000000000000
AAAAAAAAAADNAAAA	51.0	82.6550000000000000000000	17.3900000000000000000000	27.0050000000000000000000
AAAAAAAAAAEABAAA	44.0	86.3500000000000000000000	0.0000000000000000000000	53.5300000000000000000000
AAAAAAAAAAECBAAA	21.0	106.7000000000000000000000	658.7600000000000000000000	64.0200000000000000000000
AAAAAAAAAAEDAAAA	43.0	84.5333333333333333333333	0.0000000000000000000000	36.4666666666666666666667
AAAAAAAAAAEDBAAA	56.0	13.3900000000000000000000	0.0000000000000000000000	7.4900000000000000000000
AAAAAAAAAAEFBAAA	48.0	63.5700000000000000000000	58.9666666666666666666667	16.0700000000000000000000
AAAAAAAAAAEGAAAA	38.5	71.2375000000000000000000	754.3875000000000000000000	32.2425000000000000000000
AAAAAAAAAAEGBAAA	68.0	83.4200000000000000000000	0.0000000000000000000000	64.2300000000000000000000
AAAAAAAAAAEHAAAA	89.0	21.2600000000000000000000	0.0000000000000000000000	9.3500000000000000000000
AAAAAAAAAAEIBAAA	57.0	76.0600000000000000000000	1402.7050000000000000000000	51.8825000000000000000000
AAAAAAAAAAEJAAAA	50.0	26.1500000000000000000000	0.0000000000000000000000	10.7033333333333333333333
AAAAAAAAAAEKAAAA	41.5	112.3450000000000000000000	0.0000000000000000000000	64.2450000000000000000000
AAAAAAAAAAEMAAAA	64.5	111.3400000000000000000000	0.0000000000000000000000	73.2900000000000000000000
AAAAAAAAAAENAAAA	34.0	27.9200000000000000000000	0.0000000000000000000000	21.2200000000000000000000
AAAAAAAAAAEPAAAA	99.0	10.7500000000000000000000	0.0000000000000000000000	9.6700000000000000000000
AAAAAAAAAAFAAAAA	61.0	109.0300000000000000000000	0.0000000000000000000000	72.5500000000000000000000
AAAAAAAAAAFBBAAA	86.0	128.4600000000000000000000	3106.2600000000000000000000	97.6200000000000000000000
AAAAAAAAAAFEBAAA	5.0	45.4900000000000000000000	3.6800000000000000000000	24.5600000000000000000000
AAAAAAAAAAFFAAAA	23.25	56.8900000000000000000000	15.1800000000000000000000	14.8450000000000000000000
AAAAAAAAAAFGAAAA	54.0	90.6300000000000000000000	0.0000000000000000000000	84.2800000000000000000000
AAAAAAAAAAFIAAAA	54.0	80.0566666666666666666667	10.2800000000000000000000	35.0666666666666666666667
AAAAAAAAAAFIBAAA	51.5	121.5350000000000000000000	0.0000000000000000000000	80.4850000000000000000000
AAAAAAAAAAFJAAAA	75.0	109.6800000000000000000000	0.0000000000000000000000	24.1200000000000000000000
AAAAAAAAAAFLAAAA	50.0	115.3450000000000000000000	0.0000000000000000000000	97.1750000000000000000000
AAAAAAAAAAFMAAAA	100.0	65.8100000000000000000000	0.0000000000000000000000	19.0800000000000000000000
AAAAAAAAAAGABAAA	39.0	46.1900000000000000000000	0.0000000000000000000000	20.0650000000000000000000
AAAAAAAAAAGBBAAA	49.25	106.6000000000000000000000	0.0000000000000000000000	80.4650000000000000000000
AAAAAAAAAAGDBAAA	41.0	81.1900000000000000000000	0.0000000000000000000000	31.1650000000000000000000
AAAAAAAAAAGEAAAA	51.0	100.3700000000000000000000	0.0000000000000000000000	57.2100000000000000000000
AAAAAAAAAAGEBAAA	62.5	112.3550000000000000000000	161.9850000000000000000000	59.2650000000000000000000
AAAAAAAAAAGFAAAA	50.5	72.6750000000000000000000	0.0000000000000000000000	47.4750000000000000000000
AAAAAAAAAAGGBAAA	4.0	101.4600000000000000000000	0.0000000000000000000000	82.1800000000000000000000
AAAAAAAAAAGHAAAA	17.0	61.2200000000000000000000	0.0000000000000000000000	17.7500000000000000000000
AAAAAAAAAAGHBAAA	36.0	25.9100000000000000000000	126.8000000000000000000000	5.1800000000000000000000
AAAAAAAAAAGIAAAA	34.5	56.6400000000000000000000	3.3850000000000000000000	27.0500000000000000000000
AAAAAAAAAAGLAAAA	23.0	82.6100000000000000000000	0.0000000000000000000000	1.6500000000000000000000
AAAAAAAAAAGNAAAA	51.5	82.5550000000000000000000	0.0000000000000000000000	37.1500000000000000000000
AAAAAAAAAAGOAAAA	51.333333333333336	36.6066666666666666666667	154.1766666666666666666667	23.5233333333333333333333
AAAAAAAAAAHAAAAA	61.5	62.1350000000000000000000	0.0000000000000000000000	50.3250000000000000000000
AAAAAAAAAAHBAAAA	16.0	15.2200000000000000000000	0.0000000000000000000000	14.7600000000000000000000
AAAAAAAAAAHDAAAA	56.0	44.8750000000000000000000	0.0000000000000000000000	31.0400000000000000000000
AAAAAAAAAAHEAAAA	58.0	3.3400000000000000000000	0.0000000000000000000000	1.4300000000000000000000
AAAAAAAAAAHFBAAA	44.0	141.0600000000000000000000	0.0000000000000000000000	42.6800000000000000000000
AAAAAAAAAAHGBAAA	94.0	82.0400000000000000000000	0.0000000000000000000000	81.2100000000000000000000
AAAAAAAAAAHHAAAA	32.5	49.2300000000000000000000	0.0000000000000000000000	8.2050000000000000000000
AAAAAAAAAAHJAAAA	43.0	120.3000000000000000000000	0.0000000000000000000000	63.3950000000000000000000
AAAAAAAAAAHKAAAA	49.25	54.0475000000000000000000	34.8275000000000000000000	35.0950000000000000000000
AAAAAAAAAAHMAAAA	42.5	65.2100000000000000000000	0.0000000000000000000000	12.7400000000000000000000
AAAAAAAAAAHNAAAA	79.0	98.5500000000000000000000	0.0000000000000000000000	65.0400000000000000000000
AAAAAAAAAAHPAAAA	77.0	51.0800000000000000000000	0.0000000000000000000000	49.0300000000000000000000
AAAAAAAAAAIAAAAA	58.0	59.2700000000000000000000	0.0000000000000000000000	53.9300000000000000000000
Time taken: 300.166 seconds, Fetched: 100 row(s)
timediff:305.751212592
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query8.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = f07f9733-dedb-4730-8239-536da0807db9

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 38537d82-9b10-4658-8756-5c3aaa208fd4
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_net_profit, ss_store_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_qoy, d_year
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_zip, s_store_name, s_store_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_zip
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_preferred_cust_flag, c_current_addr_sk
Query ID = hdfs_20230105074455_ba304469-401b-4f49-9fe3-16fe5ac0c1cf
Total jobs = 10
Launching Job 1 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Starting Job = job_1672890466700_0069, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0069/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0069
Hadoop job information for Stage-8: number of mappers: 1; number of reducers: 1
2023-01-05 07:45:15,114 Stage-8 map = 0%,  reduce = 0%
2023-01-05 07:45:18,524 Stage-8 map = 100%,  reduce = 0%, Cumulative CPU 1.99 sec
2023-01-05 07:45:18	Dump the side-table for tag: 1 with group count: 91 into file: file:/tmp/hdfs/f07f9733-dedb-4730-8239-536da0807db9/hive_2023-01-05_07-44-55_418_6965853775511266084-1/-local-10017/HashTable-Stage-17/MapJoin-mapfile21--.hashtable2023-01-05 07:45:18	Uploaded 1 File to: file:/tmp/hdfs/f07f9733-dedb-4730-8239-536da0807db9/hive_2023-01-05_07-44-55_418_6965853775511266084-1/-local-10017/HashTable-Stage-17/MapJoin-mapfile21--.hashtable (2179 bytes)
2023-01-05 07:45:18	End of local task; Time Taken: 2.802 sec.
Execution completed successfully
MapredLocal task succeeded
2023-01-05 07:45:16	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 07:45:20	Uploaded 1 File to: file:/tmp/hdfs/f07f9733-dedb-4730-8239-536da0807db9/hive_2023-01-05_07-44-55_418_6965853775511266084-1/-local-10019/HashTable-Stage-11/MapJoin-mapfile40--.hashtable (7974341 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 3 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:45:22,863 Stage-8 map = 100%,  reduce = 100%, Cumulative CPU 2.8 sec
MapReduce Total cumulative CPU time: 2 seconds 800 msec
Ended Job = job_1672890466700_0069
Starting Job = job_1672890466700_0071, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0071/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0071
Starting Job = job_1672890466700_0070, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0070/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0070
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 1
2023-01-05 07:45:30,342 Stage-11 map = 0%,  reduce = 0%
Hadoop job information for Stage-17: number of mappers: 4; number of reducers: 0
2023-01-05 07:45:35,170 Stage-17 map = 0%,  reduce = 0%
2023-01-05 07:45:38,525 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 5.87 sec
2023-01-05 07:45:47,097 Stage-11 map = 100%,  reduce = 100%, Cumulative CPU 9.98 sec
MapReduce Total cumulative CPU time: 9 seconds 980 msec
Ended Job = job_1672890466700_0071
2023-01-05 07:45:51,533 Stage-17 map = 2%,  reduce = 0%, Cumulative CPU 11.82 sec
Launching Job 4 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:45:53,594 Stage-17 map = 5%,  reduce = 0%, Cumulative CPU 24.05 sec
Starting Job = job_1672890466700_0072, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0072/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0072
2023-01-05 07:45:55,636 Stage-17 map = 6%,  reduce = 0%, Cumulative CPU 48.83 sec
2023-01-05 07:45:57,674 Stage-17 map = 7%,  reduce = 0%, Cumulative CPU 50.39 sec
2023-01-05 07:45:59,719 Stage-17 map = 8%,  reduce = 0%, Cumulative CPU 51.97 sec
2023-01-05 07:46:00,742 Stage-17 map = 9%,  reduce = 0%, Cumulative CPU 53.5 sec
2023-01-05 07:46:01,764 Stage-17 map = 10%,  reduce = 0%, Cumulative CPU 55.18 sec
Hadoop job information for Stage-12: number of mappers: 1; number of reducers: 1
2023-01-05 07:46:02,583 Stage-12 map = 0%,  reduce = 0%
2023-01-05 07:46:03,802 Stage-17 map = 11%,  reduce = 0%, Cumulative CPU 59.69 sec
2023-01-05 07:46:05,837 Stage-17 map = 13%,  reduce = 0%, Cumulative CPU 62.57 sec
2023-01-05 07:46:06,715 Stage-12 map = 100%,  reduce = 0%, Cumulative CPU 1.27 sec
2023-01-05 07:46:06,853 Stage-17 map = 15%,  reduce = 0%, Cumulative CPU 65.34 sec
2023-01-05 07:46:09,898 Stage-17 map = 16%,  reduce = 0%, Cumulative CPU 68.44 sec
2023-01-05 07:46:11,933 Stage-17 map = 17%,  reduce = 0%, Cumulative CPU 70.45 sec
2023-01-05 07:46:12,952 Stage-17 map = 19%,  reduce = 0%, Cumulative CPU 73.42 sec
2023-01-05 07:46:13,826 Stage-12 map = 100%,  reduce = 100%, Cumulative CPU 2.49 sec
MapReduce Total cumulative CPU time: 2 seconds 490 msec
Ended Job = job_1672890466700_0072
2023-01-05 07:46:16,003 Stage-17 map = 20%,  reduce = 0%, Cumulative CPU 76.61 sec
Launching Job 5 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:46:18,043 Stage-17 map = 22%,  reduce = 0%, Cumulative CPU 78.54 sec
2023-01-05 07:46:19,064 Stage-17 map = 24%,  reduce = 0%, Cumulative CPU 82.68 sec
Starting Job = job_1672890466700_0073, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0073/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0073
2023-01-05 07:46:22,120 Stage-17 map = 25%,  reduce = 0%, Cumulative CPU 85.32 sec
2023-01-05 07:46:24,160 Stage-17 map = 28%,  reduce = 0%, Cumulative CPU 87.9 sec
2023-01-05 07:46:25,179 Stage-17 map = 29%,  reduce = 0%, Cumulative CPU 90.93 sec
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2023-01-05 07:46:25,516 Stage-9 map = 0%,  reduce = 0%
2023-01-05 07:46:29,241 Stage-17 map = 31%,  reduce = 0%, Cumulative CPU 93.39 sec
2023-01-05 07:46:30,257 Stage-17 map = 32%,  reduce = 0%, Cumulative CPU 94.39 sec
2023-01-05 07:46:31,276 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 97.09 sec
2023-01-05 07:46:31,641 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 1.89 sec
2023-01-05 07:46:35,345 Stage-17 map = 35%,  reduce = 0%, Cumulative CPU 99.98 sec
2023-01-05 07:46:36,363 Stage-17 map = 37%,  reduce = 0%, Cumulative CPU 101.31 sec
2023-01-05 07:46:36,746 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec
2023-01-05 07:46:37,382 Stage-17 map = 38%,  reduce = 0%, Cumulative CPU 104.57 sec
MapReduce Total cumulative CPU time: 4 seconds 300 msec
Ended Job = job_1672890466700_0073
2023-01-05 07:46:39,412 Stage-17 map = 39%,  reduce = 0%, Cumulative CPU 105.74 sec
2023-01-05 07:46:41,442 Stage-17 map = 40%,  reduce = 0%, Cumulative CPU 107.3 sec
2023-01-05 07:46:42,461 Stage-17 map = 41%,  reduce = 0%, Cumulative CPU 108.74 sec
2023-01-05 07:46:43,478 Stage-17 map = 43%,  reduce = 0%, Cumulative CPU 111.88 sec
2023-01-05 07:46:47,542 Stage-17 map = 45%,  reduce = 0%, Cumulative CPU 116.62 sec
2023-01-05 07:46:48,558 Stage-17 map = 46%,  reduce = 0%, Cumulative CPU 118.34 sec
2023-01-05 07:46:49,572 Stage-17 map = 48%,  reduce = 0%, Cumulative CPU 121.43 sec
2023-01-05 07:46:53,638 Stage-17 map = 50%,  reduce = 0%, Cumulative CPU 126.02 sec
2023-01-05 07:46:54,657 Stage-17 map = 51%,  reduce = 0%, Cumulative CPU 129.38 sec
2023-01-05 07:46:55,671 Stage-17 map = 53%,  reduce = 0%, Cumulative CPU 131.92 sec
2023-01-05 07:46:59,741 Stage-17 map = 55%,  reduce = 0%, Cumulative CPU 136.2 sec
2023-01-05 07:47:00,761 Stage-17 map = 56%,  reduce = 0%, Cumulative CPU 138.81 sec
2023-01-05 07:47:01,776 Stage-17 map = 57%,  reduce = 0%, Cumulative CPU 141.01 sec
2023-01-05 07:47:03,807 Stage-17 map = 58%,  reduce = 0%, Cumulative CPU 142.35 sec
2023-01-05 07:47:05,836 Stage-17 map = 60%,  reduce = 0%, Cumulative CPU 145.83 sec
2023-01-05 07:47:06,855 Stage-17 map = 61%,  reduce = 0%, Cumulative CPU 148.95 sec
2023-01-05 07:47:07,872 Stage-17 map = 62%,  reduce = 0%, Cumulative CPU 150.65 sec
2023-01-05 07:47:09,899 Stage-17 map = 63%,  reduce = 0%, Cumulative CPU 151.65 sec
2023-01-05 07:47:11,928 Stage-17 map = 64%,  reduce = 0%, Cumulative CPU 154.84 sec
2023-01-05 07:47:12,953 Stage-17 map = 66%,  reduce = 0%, Cumulative CPU 159.3 sec
2023-01-05 07:47:15,999 Stage-17 map = 67%,  reduce = 0%, Cumulative CPU 160.73 sec
2023-01-05 07:47:18,036 Stage-17 map = 69%,  reduce = 0%, Cumulative CPU 163.21 sec
2023-01-05 07:47:19,063 Stage-17 map = 71%,  reduce = 0%, Cumulative CPU 166.9 sec
2023-01-05 07:47:22,106 Stage-17 map = 74%,  reduce = 0%, Cumulative CPU 170.25 sec
2023-01-05 07:47:25,143 Stage-17 map = 76%,  reduce = 0%, Cumulative CPU 174.16 sec
2023-01-05 07:47:27,169 Stage-17 map = 77%,  reduce = 0%, Cumulative CPU 175.31 sec
2023-01-05 07:47:30,207 Stage-17 map = 78%,  reduce = 0%, Cumulative CPU 177.24 sec
2023-01-05 07:47:31,220 Stage-17 map = 79%,  reduce = 0%, Cumulative CPU 178.28 sec
2023-01-05 07:47:33,248 Stage-17 map = 80%,  reduce = 0%, Cumulative CPU 179.35 sec
2023-01-05 07:47:36,285 Stage-17 map = 81%,  reduce = 0%, Cumulative CPU 180.36 sec
2023-01-05 07:47:37,298 Stage-17 map = 82%,  reduce = 0%, Cumulative CPU 181.48 sec
2023-01-05 07:47:39,324 Stage-17 map = 84%,  reduce = 0%, Cumulative CPU 182.77 sec
2023-01-05 07:47:42,362 Stage-17 map = 85%,  reduce = 0%, Cumulative CPU 183.87 sec
2023-01-05 07:47:43,375 Stage-17 map = 86%,  reduce = 0%, Cumulative CPU 185.19 sec
2023-01-05 07:47:45,403 Stage-17 map = 88%,  reduce = 0%, Cumulative CPU 186.89 sec
2023-01-05 07:47:51,688 Stage-17 map = 89%,  reduce = 0%, Cumulative CPU 190.46 sec
2023-01-05 07:47:54,727 Stage-17 map = 90%,  reduce = 0%, Cumulative CPU 191.4 sec
2023-01-05 07:47:57,769 Stage-17 map = 91%,  reduce = 0%, Cumulative CPU 194.15 sec
2023-01-05 07:48:00,807 Stage-17 map = 92%,  reduce = 0%, Cumulative CPU 194.98 sec
2023-01-05 07:48:03,848 Stage-17 map = 93%,  reduce = 0%, Cumulative CPU 198.24 sec
2023-01-05 07:48:06,888 Stage-17 map = 94%,  reduce = 0%, Cumulative CPU 199.19 sec
2023-01-05 07:48:09,931 Stage-17 map = 95%,  reduce = 0%, Cumulative CPU 202.63 sec
2023-01-05 07:48:11,958 Stage-17 map = 96%,  reduce = 0%, Cumulative CPU 204.05 sec
2023-01-05 07:48:12,970 Stage-17 map = 97%,  reduce = 0%, Cumulative CPU 205.27 sec
2023-01-05 07:48:19,043 Stage-17 map = 98%,  reduce = 0%, Cumulative CPU 206.56 sec
2023-01-05 07:48:25,116 Stage-17 map = 99%,  reduce = 0%, Cumulative CPU 207.86 sec
2023-01-05 07:48:28,160 Stage-17 map = 100%,  reduce = 0%, Cumulative CPU 208.75 sec
MapReduce Total cumulative CPU time: 3 minutes 28 seconds 750 msec
Ended Job = job_1672890466700_0070
Stage-20 is selected by condition resolver.
Stage-21 is filtered out by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 07:48:40	Dump the side-table for tag: 1 with group count: 9 into file: file:/tmp/hdfs/f07f9733-dedb-4730-8239-536da0807db9/hive_2023-01-05_07-44-55_418_6965853775511266084-1/-local-10013/HashTable-Stage-14/MapJoin-mapfile01--.hashtable2023-01-05 07:48:40	Uploaded 1 File to: file:/tmp/hdfs/f07f9733-dedb-4730-8239-536da0807db9/hive_2023-01-05_07-44-55_418_6965853775511266084-1/-local-10013/HashTable-Stage-14/MapJoin-mapfile01--.hashtable (440 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 10
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0074, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0074/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0074
Hadoop job information for Stage-14: number of mappers: 1; number of reducers: 0
2023-01-05 07:48:48,103 Stage-14 map = 0%,  reduce = 0%
2023-01-05 07:48:53,190 Stage-14 map = 100%,  reduce = 0%, Cumulative CPU 3.7 sec
MapReduce Total cumulative CPU time: 3 seconds 700 msec
Ended Job = job_1672890466700_0074
Launching Job 8 out of 10
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0075, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0075/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0075
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 07:49:02,236 Stage-4 map = 0%,  reduce = 0%
2023-01-05 07:49:07,349 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-01-05 07:49:13,474 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
MapReduce Total cumulative CPU time: 2 seconds 100 msec
Ended Job = job_1672890466700_0075
Launching Job 9 out of 10
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0076, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0076/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0076
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 07:49:22,379 Stage-5 map = 0%,  reduce = 0%
2023-01-05 07:49:28,520 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 0.93 sec
2023-01-05 07:49:32,606 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.1 sec
MapReduce Total cumulative CPU time: 2 seconds 100 msec
Ended Job = job_1672890466700_0076
MapReduce Jobs Launched:
Stage-Stage-8: Map: 1  Reduce: 1   Cumulative CPU: 2.8 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-11: Map: 1  Reduce: 1   Cumulative CPU: 9.98 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-12: Map: 1  Reduce: 1   Cumulative CPU: 2.49 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-9: Map: 2  Reduce: 1   Cumulative CPU: 4.3 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-17: Map: 4   Cumulative CPU: 208.75 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-14: Map: 1   Cumulative CPU: 3.7 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 2.1 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.1 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 56 seconds 220 msec
OK
able	-12366819.06
Time taken: 280.774 seconds, Fetched: 1 row(s)
timediff:286.223810505
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query9.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 94f9a396-7091-49d0-9785-01f98008ae12

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = a4a62d0b-d267-4678-a3cc-753ec12f89d8
Warning: Map Join MAPJOIN[236][bigTable=?] in task 'Stage-31:MAPRED' is a cross product
Warning: Shuffle Join JOIN[152][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10, $hdt$_11, $hdt$_12, $hdt$_13, $hdt$_14, $hdt$_15]] in Stage 'Stage-15:MAPRED' is a cross product
Warning: Map Join MAPJOIN[243][bigTable=?] in task 'Stage-33:MAPRED' is a cross product
Warning: Shuffle Join JOIN[149][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10, $hdt$_11, $hdt$_12, $hdt$_13, $hdt$_14]] in Stage 'Stage-14:MAPRED' is a cross product
Warning: Map Join MAPJOIN[250][bigTable=?] in task 'Stage-35:MAPRED' is a cross product
Warning: Shuffle Join JOIN[146][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10, $hdt$_11, $hdt$_12, $hdt$_13]] in Stage 'Stage-13:MAPRED' is a cross product
Warning: Map Join MAPJOIN[257][bigTable=?] in task 'Stage-37:MAPRED' is a cross product
Warning: Shuffle Join JOIN[143][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10, $hdt$_11, $hdt$_12]] in Stage 'Stage-12:MAPRED' is a cross product
Warning: Map Join MAPJOIN[264][bigTable=?] in task 'Stage-39:MAPRED' is a cross product
Warning: Shuffle Join JOIN[140][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10, $hdt$_11]] in Stage 'Stage-11:MAPRED' is a cross product
Warning: Map Join MAPJOIN[271][bigTable=?] in task 'Stage-41:MAPRED' is a cross product
Warning: Shuffle Join JOIN[137][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9, $hdt$_10]] in Stage 'Stage-10:MAPRED' is a cross product
Warning: Map Join MAPJOIN[278][bigTable=?] in task 'Stage-43:MAPRED' is a cross product
Warning: Shuffle Join JOIN[134][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8, $hdt$_9]] in Stage 'Stage-9:MAPRED' is a cross product
Warning: Map Join MAPJOIN[285][bigTable=?] in task 'Stage-45:MAPRED' is a cross product
Warning: Shuffle Join JOIN[131][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7, $hdt$_8]] in Stage 'Stage-8:MAPRED' is a cross product
Warning: Map Join MAPJOIN[292][bigTable=?] in task 'Stage-47:MAPRED' is a cross product
Warning: Shuffle Join JOIN[128][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6, $hdt$_7]] in Stage 'Stage-7:MAPRED' is a cross product
Warning: Map Join MAPJOIN[299][bigTable=?] in task 'Stage-49:MAPRED' is a cross product
Warning: Shuffle Join JOIN[125][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5, $hdt$_6]] in Stage 'Stage-6:MAPRED' is a cross product
Warning: Map Join MAPJOIN[306][bigTable=?] in task 'Stage-51:MAPRED' is a cross product
Warning: Shuffle Join JOIN[122][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4, $hdt$_5]] in Stage 'Stage-5:MAPRED' is a cross product
Warning: Map Join MAPJOIN[313][bigTable=?] in task 'Stage-53:MAPRED' is a cross product
Warning: Shuffle Join JOIN[119][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3, $hdt$_4]] in Stage 'Stage-4:MAPRED' is a cross product
Warning: Map Join MAPJOIN[320][bigTable=?] in task 'Stage-55:MAPRED' is a cross product
Warning: Shuffle Join JOIN[116][tables = [$hdt$_0, $hdt$_1, $hdt$_2, $hdt$_3]] in Stage 'Stage-3:MAPRED' is a cross product
Warning: Map Join MAPJOIN[327][bigTable=?] in task 'Stage-57:MAPRED' is a cross product
Warning: Shuffle Join JOIN[113][tables = [$hdt$_0, $hdt$_1, $hdt$_2]] in Stage 'Stage-2:MAPRED' is a cross product
Warning: Map Join MAPJOIN[336][bigTable=?] in task 'Stage-59:MAPRED' is a cross product
Warning: Shuffle Join JOIN[110][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product
Query ID = hdfs_20230105074941_312b6439-5e19-43f9-9c96-626dfc39fe4e
Total jobs = 45
Launching Job 1 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
Launching Job 2 out of 45
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Launching Job 3 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
Launching Job 4 out of 45
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Launching Job 5 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Launching Job 6 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Launching Job 7 out of 45
Launching Job 8 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0084, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0084/
Starting Job = job_1672890466700_0083, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0083/
Starting Job = job_1672890466700_0078, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0078/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0078
Starting Job = job_1672890466700_0082, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0082/
Starting Job = job_1672890466700_0077, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0077/
Starting Job = job_1672890466700_0080, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0080/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0077
Starting Job = job_1672890466700_0079, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0079/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0080
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0082
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0083
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0084
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0079
Starting Job = job_1672890466700_0081, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0081/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0081
Hadoop job information for Stage-16: number of mappers: 4; number of reducers: 1
2023-01-05 07:50:17,581 Stage-16 map = 0%,  reduce = 0%
Hadoop job information for Stage-22: number of mappers: 4; number of reducers: 1
Hadoop job information for Stage-17: number of mappers: 4; number of reducers: 1
2023-01-05 07:50:18,549 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:50:18,567 Stage-17 map = 0%,  reduce = 0%
2023-01-05 07:50:33,417 Stage-16 map = 1%,  reduce = 0%, Cumulative CPU 8.86 sec
2023-01-05 07:50:35,537 Stage-16 map = 2%,  reduce = 0%, Cumulative CPU 18.64 sec
2023-01-05 07:50:39,741 Stage-16 map = 3%,  reduce = 0%, Cumulative CPU 33.8 sec
2023-01-05 07:50:42,643 Stage-22 map = 1%,  reduce = 0%, Cumulative CPU 10.79 sec
2023-01-05 07:50:42,799 Stage-16 map = 4%,  reduce = 0%, Cumulative CPU 46.01 sec
2023-01-05 07:50:43,725 Stage-22 map = 2%,  reduce = 0%, Cumulative CPU 38.36 sec
2023-01-05 07:50:44,848 Stage-16 map = 5%,  reduce = 0%, Cumulative CPU 48.32 sec
2023-01-05 07:50:47,899 Stage-16 map = 6%,  reduce = 0%, Cumulative CPU 52.1 sec
2023-01-05 07:50:48,923 Stage-16 map = 7%,  reduce = 0%, Cumulative CPU 54.39 sec
2023-01-05 07:50:49,220 Stage-22 map = 3%,  reduce = 0%, Cumulative CPU 42.81 sec
2023-01-05 07:50:50,325 Stage-22 map = 5%,  reduce = 0%, Cumulative CPU 47.81 sec
2023-01-05 07:50:54,064 Stage-16 map = 9%,  reduce = 0%, Cumulative CPU 62.59 sec
2023-01-05 07:50:55,459 Stage-22 map = 6%,  reduce = 0%, Cumulative CPU 52.0 sec
2023-01-05 07:50:56,024 Stage-17 map = 1%,  reduce = 0%, Cumulative CPU 31.34 sec
2023-01-05 07:50:56,527 Stage-22 map = 7%,  reduce = 0%, Cumulative CPU 56.71 sec
2023-01-05 07:50:57,067 Stage-17 map = 2%,  reduce = 0%, Cumulative CPU 43.92 sec
2023-01-05 07:51:00,218 Stage-16 map = 10%,  reduce = 0%, Cumulative CPU 70.44 sec
2023-01-05 07:51:00,622 Stage-22 map = 8%,  reduce = 0%, Cumulative CPU 58.25 sec
2023-01-05 07:51:01,657 Stage-22 map = 9%,  reduce = 0%, Cumulative CPU 60.2 sec
2023-01-05 07:51:02,196 Stage-17 map = 3%,  reduce = 0%, Cumulative CPU 49.47 sec
2023-01-05 07:51:02,700 Stage-22 map = 10%,  reduce = 0%, Cumulative CPU 65.88 sec
2023-01-05 07:51:03,218 Stage-17 map = 4%,  reduce = 0%, Cumulative CPU 51.74 sec
2023-01-05 07:51:03,306 Stage-16 map = 11%,  reduce = 0%, Cumulative CPU 72.94 sec
2023-01-05 07:51:06,585 Stage-16 map = 13%,  reduce = 0%, Cumulative CPU 79.35 sec
2023-01-05 07:51:07,853 Stage-22 map = 12%,  reduce = 0%, Cumulative CPU 74.25 sec
2023-01-05 07:51:08,557 Stage-17 map = 5%,  reduce = 0%, Cumulative CPU 56.76 sec
2023-01-05 07:51:11,707 Stage-16 map = 14%,  reduce = 0%, Cumulative CPU 84.56 sec
2023-01-05 07:51:12,733 Stage-16 map = 15%,  reduce = 0%, Cumulative CPU 87.98 sec
2023-01-05 07:51:12,952 Stage-22 map = 13%,  reduce = 0%, Cumulative CPU 75.81 sec
2023-01-05 07:51:13,666 Stage-17 map = 6%,  reduce = 0%, Cumulative CPU 59.62 sec
2023-01-05 07:51:13,976 Stage-22 map = 14%,  reduce = 0%, Cumulative CPU 82.67 sec
2023-01-05 07:51:14,685 Stage-17 map = 7%,  reduce = 0%, Cumulative CPU 63.66 sec
2023-01-05 07:51:15,835 Stage-16 map = 16%,  reduce = 0%, Cumulative CPU 90.44 sec
2023-01-05 07:51:17,879 Stage-16 map = 17%,  reduce = 0%, Cumulative CPU 94.68 sec
2023-01-05 07:51:18,903 Stage-16 map = 19%,  reduce = 0%, Cumulative CPU 96.88 sec
2023-01-05 07:51:19,129 Stage-22 map = 16%,  reduce = 0%, Cumulative CPU 86.69 sec
2023-01-05 07:51:20,158 Stage-22 map = 17%,  reduce = 0%, Cumulative CPU 91.47 sec
2023-01-05 07:51:20,837 Stage-17 map = 8%,  reduce = 0%, Cumulative CPU 73.54 sec
2023-01-05 07:51:24,237 Stage-16 map = 22%,  reduce = 0%, Cumulative CPU 106.15 sec
2023-01-05 07:51:25,279 Stage-22 map = 20%,  reduce = 0%, Cumulative CPU 95.52 sec
2023-01-05 07:51:25,993 Stage-17 map = 9%,  reduce = 0%, Cumulative CPU 74.94 sec
2023-01-05 07:51:27,010 Stage-17 map = 10%,  reduce = 0%, Cumulative CPU 81.07 sec
2023-01-05 07:51:30,389 Stage-16 map = 25%,  reduce = 0%, Cumulative CPU 115.61 sec
2023-01-05 07:51:30,478 Stage-22 map = 22%,  reduce = 0%, Cumulative CPU 102.67 sec
2023-01-05 07:51:31,503 Stage-22 map = 23%,  reduce = 0%, Cumulative CPU 104.65 sec
2023-01-05 07:51:32,114 Stage-17 map = 11%,  reduce = 0%, Cumulative CPU 82.41 sec
2023-01-05 07:51:33,157 Stage-17 map = 12%,  reduce = 0%, Cumulative CPU 87.95 sec
2023-01-05 07:51:35,977 Stage-16 map = 26%,  reduce = 0%, Cumulative CPU 122.82 sec
2023-01-05 07:51:36,665 Stage-22 map = 24%,  reduce = 0%, Cumulative CPU 111.78 sec
2023-01-05 07:51:37,008 Stage-16 map = 27%,  reduce = 0%, Cumulative CPU 125.11 sec
2023-01-05 07:51:37,696 Stage-22 map = 25%,  reduce = 0%, Cumulative CPU 114.54 sec
2023-01-05 07:51:38,771 Stage-22 map = 26%,  reduce = 0%, Cumulative CPU 119.08 sec
2023-01-05 07:51:39,318 Stage-17 map = 13%,  reduce = 0%, Cumulative CPU 97.32 sec
2023-01-05 07:51:41,114 Stage-16 map = 28%,  reduce = 0%, Cumulative CPU 129.78 sec
2023-01-05 07:51:42,924 Stage-22 map = 27%,  reduce = 0%, Cumulative CPU 122.0 sec
2023-01-05 07:51:43,941 Stage-22 map = 28%,  reduce = 0%, Cumulative CPU 130.01 sec
2023-01-05 07:51:44,439 Stage-17 map = 15%,  reduce = 0%, Cumulative CPU 102.92 sec
2023-01-05 07:51:45,219 Stage-16 map = 29%,  reduce = 0%, Cumulative CPU 138.24 sec
2023-01-05 07:51:48,296 Stage-16 map = 30%,  reduce = 0%, Cumulative CPU 143.61 sec
2023-01-05 07:51:49,030 Stage-22 map = 29%,  reduce = 0%, Cumulative CPU 132.97 sec
2023-01-05 07:51:50,052 Stage-22 map = 30%,  reduce = 0%, Cumulative CPU 139.86 sec
2023-01-05 07:51:50,598 Stage-17 map = 16%,  reduce = 0%, Cumulative CPU 110.15 sec
2023-01-05 07:51:51,376 Stage-16 map = 31%,  reduce = 0%, Cumulative CPU 145.87 sec
2023-01-05 07:51:51,621 Stage-17 map = 17%,  reduce = 0%, Cumulative CPU 112.56 sec
2023-01-05 07:51:53,431 Stage-16 map = 32%,  reduce = 0%, Cumulative CPU 148.52 sec
2023-01-05 07:51:55,179 Stage-22 map = 31%,  reduce = 0%, Cumulative CPU 145.19 sec
2023-01-05 07:51:56,198 Stage-22 map = 32%,  reduce = 0%, Cumulative CPU 149.42 sec
2023-01-05 07:51:56,750 Stage-17 map = 18%,  reduce = 0%, Cumulative CPU 118.02 sec
2023-01-05 07:51:57,526 Stage-16 map = 33%,  reduce = 0%, Cumulative CPU 154.26 sec
2023-01-05 07:51:59,583 Stage-16 map = 34%,  reduce = 0%, Cumulative CPU 155.93 sec
2023-01-05 07:52:00,600 Stage-16 map = 35%,  reduce = 0%, Cumulative CPU 160.31 sec
2023-01-05 07:52:01,358 Stage-22 map = 33%,  reduce = 0%, Cumulative CPU 154.02 sec
2023-01-05 07:52:01,887 Stage-17 map = 19%,  reduce = 0%, Cumulative CPU 121.29 sec
2023-01-05 07:52:02,417 Stage-22 map = 35%,  reduce = 0%, Cumulative CPU 158.03 sec
2023-01-05 07:52:02,905 Stage-17 map = 20%,  reduce = 0%, Cumulative CPU 125.48 sec
2023-01-05 07:52:05,704 Stage-16 map = 36%,  reduce = 0%, Cumulative CPU 164.19 sec
2023-01-05 07:52:06,731 Stage-16 map = 37%,  reduce = 0%, Cumulative CPU 168.9 sec
2023-01-05 07:52:08,533 Stage-22 map = 37%,  reduce = 0%, Cumulative CPU 167.02 sec
2023-01-05 07:52:09,052 Stage-17 map = 21%,  reduce = 0%, Cumulative CPU 136.43 sec
2023-01-05 07:52:09,806 Stage-16 map = 38%,  reduce = 0%, Cumulative CPU 170.64 sec
2023-01-05 07:52:11,840 Stage-16 map = 39%,  reduce = 0%, Cumulative CPU 172.33 sec
2023-01-05 07:52:12,656 Stage-22 map = 38%,  reduce = 0%, Cumulative CPU 169.39 sec
2023-01-05 07:52:14,693 Stage-22 map = 39%,  reduce = 0%, Cumulative CPU 175.27 sec
2023-01-05 07:52:15,172 Stage-17 map = 23%,  reduce = 0%, Cumulative CPU 144.58 sec
2023-01-05 07:52:15,921 Stage-16 map = 40%,  reduce = 0%, Cumulative CPU 178.81 sec
2023-01-05 07:52:17,957 Stage-16 map = 41%,  reduce = 0%, Cumulative CPU 180.29 sec
2023-01-05 07:52:18,856 Stage-22 map = 40%,  reduce = 0%, Cumulative CPU 177.56 sec
2023-01-05 07:52:18,985 Stage-16 map = 42%,  reduce = 0%, Cumulative CPU 184.7 sec
2023-01-05 07:52:19,877 Stage-22 map = 42%,  reduce = 0%, Cumulative CPU 183.92 sec
2023-01-05 07:52:20,486 Stage-17 map = 24%,  reduce = 0%, Cumulative CPU 151.5 sec
2023-01-05 07:52:21,033 Stage-16 map = 43%,  reduce = 0%, Cumulative CPU 187.14 sec
2023-01-05 07:52:23,487 Stage-16 map = 44%,  reduce = 0%, Cumulative CPU 188.67 sec
2023-01-05 07:52:26,013 Stage-22 map = 44%,  reduce = 0%, Cumulative CPU 191.99 sec
2023-01-05 07:52:26,640 Stage-17 map = 26%,  reduce = 0%, Cumulative CPU 159.36 sec
2023-01-05 07:52:27,575 Stage-16 map = 45%,  reduce = 0%, Cumulative CPU 195.07 sec
2023-01-05 07:52:29,653 Stage-16 map = 47%,  reduce = 0%, Cumulative CPU 197.19 sec
2023-01-05 07:52:31,128 Stage-22 map = 45%,  reduce = 0%, Cumulative CPU 196.4 sec
2023-01-05 07:52:31,775 Stage-17 map = 27%,  reduce = 0%, Cumulative CPU 163.46 sec
2023-01-05 07:52:32,141 Stage-22 map = 47%,  reduce = 0%, Cumulative CPU 200.24 sec
2023-01-05 07:52:32,790 Stage-17 map = 28%,  reduce = 0%, Cumulative CPU 167.86 sec
2023-01-05 07:52:33,962 Stage-16 map = 49%,  reduce = 0%, Cumulative CPU 203.41 sec
2023-01-05 07:52:36,001 Stage-16 map = 50%,  reduce = 0%, Cumulative CPU 205.23 sec
2023-01-05 07:52:37,022 Stage-16 map = 62%,  reduce = 0%, Cumulative CPU 210.83 sec
2023-01-05 07:52:37,278 Stage-22 map = 49%,  reduce = 0%, Cumulative CPU 204.4 sec
2023-01-05 07:52:38,319 Stage-22 map = 51%,  reduce = 0%, Cumulative CPU 208.5 sec
2023-01-05 07:52:39,110 Stage-17 map = 30%,  reduce = 0%, Cumulative CPU 177.4 sec
2023-01-05 07:52:39,361 Stage-22 map = 60%,  reduce = 0%, Cumulative CPU 209.17 sec
2023-01-05 07:52:41,106 Stage-16 map = 63%,  reduce = 0%, Cumulative CPU 210.83 sec
2023-01-05 07:52:42,130 Stage-16 map = 64%,  reduce = 0%, Cumulative CPU 214.74 sec
2023-01-05 07:52:44,627 Stage-22 map = 62%,  reduce = 0%, Cumulative CPU 215.38 sec
2023-01-05 07:52:45,317 Stage-17 map = 32%,  reduce = 0%, Cumulative CPU 184.64 sec
2023-01-05 07:52:45,657 Stage-22 map = 73%,  reduce = 0%, Cumulative CPU 215.98 sec
2023-01-05 07:52:47,448 Stage-16 map = 65%,  reduce = 0%, Cumulative CPU 219.92 sec
2023-01-05 07:52:48,475 Stage-16 map = 66%,  reduce = 0%, Cumulative CPU 223.44 sec
2023-01-05 07:52:49,490 Stage-16 map = 75%,  reduce = 0%, Cumulative CPU 223.66 sec
2023-01-05 07:52:50,542 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 188.97 sec
2023-01-05 07:52:52,608 Stage-16 map = 75%,  reduce = 17%, Cumulative CPU 224.02 sec
Hadoop job information for Stage-21: number of mappers: 4; number of reducers: 1
2023-01-05 07:52:56,159 Stage-21 map = 0%,  reduce = 0%
2023-01-05 07:52:59,759 Stage-16 map = 76%,  reduce = 17%, Cumulative CPU 229.65 sec
2023-01-05 07:53:11,697 Stage-21 map = 1%,  reduce = 0%, Cumulative CPU 12.82 sec
2023-01-05 07:53:12,143 Stage-16 map = 77%,  reduce = 17%, Cumulative CPU 234.1 sec
2023-01-05 07:53:15,770 Stage-21 map = 2%,  reduce = 0%, Cumulative CPU 25.44 sec
2023-01-05 07:53:17,804 Stage-21 map = 3%,  reduce = 0%, Cumulative CPU 27.55 sec
2023-01-05 07:53:18,253 Stage-16 map = 86%,  reduce = 17%, Cumulative CPU 237.3 sec
2023-01-05 07:53:18,821 Stage-21 map = 4%,  reduce = 0%, Cumulative CPU 48.76 sec
2023-01-05 07:53:21,876 Stage-21 map = 5%,  reduce = 0%, Cumulative CPU 51.12 sec
2023-01-05 07:53:22,312 Stage-16 map = 86%,  reduce = 25%, Cumulative CPU 237.37 sec
2023-01-05 07:53:24,928 Stage-21 map = 6%,  reduce = 0%, Cumulative CPU 56.71 sec
2023-01-05 07:53:27,978 Stage-21 map = 7%,  reduce = 0%, Cumulative CPU 60.03 sec
2023-01-05 07:53:30,012 Stage-21 map = 8%,  reduce = 0%, Cumulative CPU 61.28 sec
2023-01-05 07:53:31,032 Stage-21 map = 10%,  reduce = 0%, Cumulative CPU 64.98 sec
2023-01-05 07:53:32,595 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:53:34,278 Stage-21 map = 11%,  reduce = 0%, Cumulative CPU 67.74 sec
2023-01-05 07:53:36,313 Stage-21 map = 13%,  reduce = 0%, Cumulative CPU 74.69 sec
2023-01-05 07:53:39,359 Stage-21 map = 14%,  reduce = 0%, Cumulative CPU 78.11 sec
2023-01-05 07:53:42,398 Stage-21 map = 16%,  reduce = 0%, Cumulative CPU 84.74 sec
2023-01-05 07:53:45,450 Stage-21 map = 17%,  reduce = 0%, Cumulative CPU 88.37 sec
2023-01-05 07:53:48,497 Stage-21 map = 20%,  reduce = 0%, Cumulative CPU 96.92 sec
2023-01-05 07:53:51,536 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:53:51,537 Stage-21 map = 21%,  reduce = 0%, Cumulative CPU 99.24 sec
2023-01-05 07:53:54,596 Stage-21 map = 23%,  reduce = 0%, Cumulative CPU 107.45 sec
2023-01-05 07:53:57,638 Stage-21 map = 24%,  reduce = 0%, Cumulative CPU 109.76 sec
2023-01-05 07:54:00,683 Stage-21 map = 26%,  reduce = 0%, Cumulative CPU 117.92 sec
2023-01-05 07:54:03,735 Stage-21 map = 27%,  reduce = 0%, Cumulative CPU 120.2 sec
2023-01-05 07:54:05,759 Stage-21 map = 28%,  reduce = 0%, Cumulative CPU 123.57 sec
2023-01-05 07:54:06,772 Stage-21 map = 29%,  reduce = 0%, Cumulative CPU 128.98 sec
2023-01-05 07:54:09,812 Stage-21 map = 30%,  reduce = 0%, Cumulative CPU 130.68 sec
2023-01-05 07:54:11,840 Stage-21 map = 31%,  reduce = 0%, Cumulative CPU 133.69 sec
2023-01-05 07:54:12,856 Stage-21 map = 32%,  reduce = 0%, Cumulative CPU 138.59 sec
2023-01-05 07:54:13,829 Stage-16 map = 87%,  reduce = 25%, Cumulative CPU 228.71 sec
2023-01-05 07:54:15,908 Stage-21 map = 33%,  reduce = 0%, Cumulative CPU 139.93 sec
2023-01-05 07:54:17,935 Stage-21 map = 34%,  reduce = 0%, Cumulative CPU 142.88 sec
2023-01-05 07:54:18,949 Stage-21 map = 35%,  reduce = 0%, Cumulative CPU 146.83 sec
2023-01-05 07:54:19,915 Stage-16 map = 88%,  reduce = 25%, Cumulative CPU 232.58 sec
2023-01-05 07:54:21,989 Stage-21 map = 36%,  reduce = 0%, Cumulative CPU 148.15 sec
2023-01-05 07:54:25,032 Stage-21 map = 38%,  reduce = 0%, Cumulative CPU 154.32 sec
2023-01-05 07:54:25,990 Stage-16 map = 89%,  reduce = 25%, Cumulative CPU 235.1 sec
2023-01-05 07:54:28,072 Stage-21 map = 39%,  reduce = 0%, Cumulative CPU 155.76 sec
2023-01-05 07:54:30,098 Stage-21 map = 40%,  reduce = 0%, Cumulative CPU 157.82 sec
2023-01-05 07:54:31,110 Stage-21 map = 41%,  reduce = 0%, Cumulative CPU 164.21 sec
2023-01-05 07:54:32,281 Stage-16 map = 91%,  reduce = 25%, Cumulative CPU 237.35 sec
2023-01-05 07:54:32,600 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:54:34,358 Stage-21 map = 42%,  reduce = 0%, Cumulative CPU 165.34 sec
2023-01-05 07:54:36,388 Stage-21 map = 44%,  reduce = 0%, Cumulative CPU 171.84 sec
2023-01-05 07:54:38,359 Stage-16 map = 100%,  reduce = 25%, Cumulative CPU 239.18 sec
2023-01-05 07:54:39,427 Stage-21 map = 45%,  reduce = 0%, Cumulative CPU 173.26 sec
2023-01-05 07:54:40,390 Stage-16 map = 100%,  reduce = 100%, Cumulative CPU 240.24 sec
MapReduce Total cumulative CPU time: 4 minutes 0 seconds 240 msec
Ended Job = job_1672890466700_0079
2023-01-05 07:54:42,467 Stage-21 map = 48%,  reduce = 0%, Cumulative CPU 180.77 sec
Launching Job 9 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:54:48,565 Stage-21 map = 51%,  reduce = 0%, Cumulative CPU 189.88 sec
2023-01-05 07:54:51,618 Stage-21 map = 52%,  reduce = 0%, Cumulative CPU 193.2 sec
2023-01-05 07:54:51,792 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
Hadoop job information for Stage-19: number of mappers: 4; number of reducers: 1
2023-01-05 07:54:54,550 Stage-19 map = 0%,  reduce = 0%
2023-01-05 07:54:54,738 Stage-21 map = 54%,  reduce = 0%, Cumulative CPU 199.91 sec
2023-01-05 07:54:57,802 Stage-21 map = 55%,  reduce = 0%, Cumulative CPU 203.39 sec
2023-01-05 07:55:00,104 Stage-21 map = 56%,  reduce = 0%, Cumulative CPU 205.51 sec
2023-01-05 07:55:01,124 Stage-21 map = 57%,  reduce = 0%, Cumulative CPU 209.88 sec
2023-01-05 07:55:04,307 Stage-21 map = 58%,  reduce = 0%, Cumulative CPU 212.28 sec
2023-01-05 07:55:06,392 Stage-21 map = 59%,  reduce = 0%, Cumulative CPU 215.11 sec
Starting Job = job_1672890466700_0085, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0085/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0085
2023-01-05 07:55:09,438 Stage-21 map = 60%,  reduce = 0%, Cumulative CPU 218.78 sec
2023-01-05 07:55:12,606 Stage-21 map = 61%,  reduce = 0%, Cumulative CPU 222.86 sec
2023-01-05 07:55:15,519 Stage-19 map = 2%,  reduce = 0%, Cumulative CPU 35.53 sec
2023-01-05 07:55:15,645 Stage-21 map = 70%,  reduce = 0%, Cumulative CPU 226.09 sec
2023-01-05 07:55:18,573 Stage-19 map = 3%,  reduce = 0%, Cumulative CPU 50.17 sec
2023-01-05 07:55:18,690 Stage-21 map = 80%,  reduce = 0%, Cumulative CPU 229.13 sec
2023-01-05 07:55:20,716 Stage-21 map = 91%,  reduce = 0%, Cumulative CPU 229.77 sec
2023-01-05 07:55:21,626 Stage-19 map = 5%,  reduce = 0%, Cumulative CPU 55.98 sec
2023-01-05 07:55:21,731 Stage-21 map = 92%,  reduce = 0%, Cumulative CPU 232.64 sec
2023-01-05 07:55:22,747 Stage-21 map = 100%,  reduce = 0%, Cumulative CPU 232.82 sec
2023-01-05 07:55:23,760 Stage-21 map = 100%,  reduce = 100%, Cumulative CPU 235.97 sec
2023-01-05 07:55:24,668 Stage-19 map = 6%,  reduce = 0%, Cumulative CPU 60.71 sec
MapReduce Total cumulative CPU time: 3 minutes 55 seconds 970 msec
Ended Job = job_1672890466700_0077
Launching Job 10 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:55:27,724 Stage-19 map = 8%,  reduce = 0%, Cumulative CPU 65.91 sec
2023-01-05 07:55:28,740 Stage-19 map = 10%,  reduce = 0%, Cumulative CPU 68.79 sec
2023-01-05 07:55:29,756 Stage-19 map = 11%,  reduce = 0%, Cumulative CPU 72.52 sec
2023-01-05 07:55:33,633 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:55:33,813 Stage-19 map = 13%,  reduce = 0%, Cumulative CPU 77.79 sec
2023-01-05 07:55:34,828 Stage-19 map = 14%,  reduce = 0%, Cumulative CPU 80.58 sec
2023-01-05 07:55:35,844 Stage-19 map = 15%,  reduce = 0%, Cumulative CPU 83.89 sec
Hadoop job information for Stage-20: number of mappers: 4; number of reducers: 1
2023-01-05 07:55:39,747 Stage-20 map = 0%,  reduce = 0%
2023-01-05 07:55:39,893 Stage-19 map = 17%,  reduce = 0%, Cumulative CPU 89.87 sec
2023-01-05 07:55:40,906 Stage-19 map = 18%,  reduce = 0%, Cumulative CPU 92.7 sec
2023-01-05 07:55:41,921 Stage-19 map = 19%,  reduce = 0%, Cumulative CPU 96.27 sec
Starting Job = job_1672890466700_0086, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0086/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0086
2023-01-05 07:55:45,994 Stage-19 map = 20%,  reduce = 0%, Cumulative CPU 102.3 sec
2023-01-05 07:55:47,014 Stage-19 map = 21%,  reduce = 0%, Cumulative CPU 104.41 sec
2023-01-05 07:55:48,038 Stage-19 map = 22%,  reduce = 0%, Cumulative CPU 106.91 sec
2023-01-05 07:55:52,206 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:55:52,225 Stage-19 map = 23%,  reduce = 0%, Cumulative CPU 111.8 sec
2023-01-05 07:55:54,306 Stage-19 map = 24%,  reduce = 0%, Cumulative CPU 116.71 sec
2023-01-05 07:55:59,443 Stage-20 map = 1%,  reduce = 0%, Cumulative CPU 11.19 sec
2023-01-05 07:55:59,608 Stage-19 map = 25%,  reduce = 0%, Cumulative CPU 122.85 sec
2023-01-05 07:56:03,913 Stage-19 map = 27%,  reduce = 0%, Cumulative CPU 132.26 sec
2023-01-05 07:56:05,589 Stage-20 map = 2%,  reduce = 0%, Cumulative CPU 40.01 sec
2023-01-05 07:56:10,085 Stage-19 map = 29%,  reduce = 0%, Cumulative CPU 141.77 sec
2023-01-05 07:56:11,098 Stage-19 map = 30%,  reduce = 0%, Cumulative CPU 143.99 sec
2023-01-05 07:56:11,894 Stage-20 map = 3%,  reduce = 0%, Cumulative CPU 62.52 sec
2023-01-05 07:56:12,912 Stage-20 map = 4%,  reduce = 0%, Cumulative CPU 64.46 sec
2023-01-05 07:56:16,170 Stage-19 map = 32%,  reduce = 0%, Cumulative CPU 151.67 sec
2023-01-05 07:56:17,989 Stage-20 map = 7%,  reduce = 0%, Cumulative CPU 74.91 sec
2023-01-05 07:56:18,199 Stage-19 map = 33%,  reduce = 0%, Cumulative CPU 156.26 sec
2023-01-05 07:56:21,444 Stage-19 map = 34%,  reduce = 0%, Cumulative CPU 158.15 sec
2023-01-05 07:56:23,476 Stage-19 map = 35%,  reduce = 0%, Cumulative CPU 163.61 sec
2023-01-05 07:56:24,094 Stage-20 map = 9%,  reduce = 0%, Cumulative CPU 82.98 sec
2023-01-05 07:56:25,116 Stage-20 map = 10%,  reduce = 0%, Cumulative CPU 85.6 sec
2023-01-05 07:56:27,527 Stage-19 map = 36%,  reduce = 0%, Cumulative CPU 168.46 sec
2023-01-05 07:56:28,541 Stage-19 map = 37%,  reduce = 0%, Cumulative CPU 170.94 sec
2023-01-05 07:56:30,198 Stage-20 map = 11%,  reduce = 0%, Cumulative CPU 90.96 sec
2023-01-05 07:56:30,568 Stage-19 map = 38%,  reduce = 0%, Cumulative CPU 175.44 sec
2023-01-05 07:56:31,211 Stage-20 map = 12%,  reduce = 0%, Cumulative CPU 92.99 sec
2023-01-05 07:56:33,628 Stage-19 map = 39%,  reduce = 0%, Cumulative CPU 179.66 sec
2023-01-05 07:56:34,461 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:56:35,663 Stage-19 map = 40%,  reduce = 0%, Cumulative CPU 182.45 sec
2023-01-05 07:56:36,280 Stage-20 map = 14%,  reduce = 0%, Cumulative CPU 99.18 sec
2023-01-05 07:56:36,679 Stage-19 map = 41%,  reduce = 0%, Cumulative CPU 184.55 sec
2023-01-05 07:56:39,725 Stage-19 map = 43%,  reduce = 0%, Cumulative CPU 188.36 sec
2023-01-05 07:56:41,566 Stage-20 map = 15%,  reduce = 0%, Cumulative CPU 105.93 sec
2023-01-05 07:56:42,580 Stage-20 map = 17%,  reduce = 0%, Cumulative CPU 108.96 sec
2023-01-05 07:56:42,761 Stage-19 map = 44%,  reduce = 0%, Cumulative CPU 193.62 sec
2023-01-05 07:56:45,806 Stage-19 map = 47%,  reduce = 0%, Cumulative CPU 198.57 sec
2023-01-05 07:56:47,650 Stage-20 map = 18%,  reduce = 0%, Cumulative CPU 115.46 sec
2023-01-05 07:56:48,671 Stage-20 map = 19%,  reduce = 0%, Cumulative CPU 117.47 sec
2023-01-05 07:56:48,850 Stage-19 map = 48%,  reduce = 0%, Cumulative CPU 203.63 sec
2023-01-05 07:56:51,893 Stage-19 map = 50%,  reduce = 0%, Cumulative CPU 208.66 sec
2023-01-05 07:56:52,540 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:56:52,908 Stage-19 map = 51%,  reduce = 0%, Cumulative CPU 211.1 sec
2023-01-05 07:56:53,953 Stage-20 map = 21%,  reduce = 0%, Cumulative CPU 123.88 sec
2023-01-05 07:56:54,968 Stage-20 map = 22%,  reduce = 0%, Cumulative CPU 125.87 sec
2023-01-05 07:56:57,979 Stage-19 map = 53%,  reduce = 0%, Cumulative CPU 218.81 sec
2023-01-05 07:56:58,992 Stage-19 map = 54%,  reduce = 0%, Cumulative CPU 221.58 sec
2023-01-05 07:57:00,005 Stage-19 map = 55%,  reduce = 0%, Cumulative CPU 223.71 sec
2023-01-05 07:57:00,052 Stage-20 map = 23%,  reduce = 0%, Cumulative CPU 132.85 sec
2023-01-05 07:57:01,064 Stage-20 map = 24%,  reduce = 0%, Cumulative CPU 135.06 sec
2023-01-05 07:57:04,264 Stage-19 map = 57%,  reduce = 0%, Cumulative CPU 229.15 sec
2023-01-05 07:57:06,295 Stage-19 map = 58%,  reduce = 0%, Cumulative CPU 232.62 sec
2023-01-05 07:57:06,347 Stage-20 map = 26%,  reduce = 0%, Cumulative CPU 143.24 sec
2023-01-05 07:57:09,335 Stage-19 map = 68%,  reduce = 0%, Cumulative CPU 234.06 sec
2023-01-05 07:57:11,369 Stage-19 map = 69%,  reduce = 0%, Cumulative CPU 237.97 sec
2023-01-05 07:57:11,414 Stage-20 map = 27%,  reduce = 0%, Cumulative CPU 145.28 sec
2023-01-05 07:57:12,405 Stage-19 map = 70%,  reduce = 0%, Cumulative CPU 239.53 sec
2023-01-05 07:57:12,428 Stage-20 map = 28%,  reduce = 0%, Cumulative CPU 151.81 sec
2023-01-05 07:57:15,509 Stage-19 map = 81%,  reduce = 0%, Cumulative CPU 243.57 sec
2023-01-05 07:57:17,510 Stage-20 map = 29%,  reduce = 0%, Cumulative CPU 156.48 sec
2023-01-05 07:57:17,535 Stage-19 map = 82%,  reduce = 0%, Cumulative CPU 245.7 sec
2023-01-05 07:57:18,524 Stage-20 map = 30%,  reduce = 0%, Cumulative CPU 161.36 sec
2023-01-05 07:57:21,591 Stage-19 map = 91%,  reduce = 0%, Cumulative CPU 249.81 sec
2023-01-05 07:57:23,598 Stage-20 map = 32%,  reduce = 0%, Cumulative CPU 166.92 sec
2023-01-05 07:57:24,611 Stage-20 map = 33%,  reduce = 0%, Cumulative CPU 173.74 sec
2023-01-05 07:57:24,627 Stage-19 map = 100%,  reduce = 0%, Cumulative CPU 251.16 sec
2023-01-05 07:57:25,642 Stage-19 map = 100%,  reduce = 100%, Cumulative CPU 252.71 sec
MapReduce Total cumulative CPU time: 4 minutes 12 seconds 710 msec
Ended Job = job_1672890466700_0078
Launching Job 11 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:57:29,674 Stage-20 map = 35%,  reduce = 0%, Cumulative CPU 177.44 sec
2023-01-05 07:57:30,687 Stage-20 map = 36%,  reduce = 0%, Cumulative CPU 183.68 sec
2023-01-05 07:57:35,248 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:57:35,961 Stage-20 map = 39%,  reduce = 0%, Cumulative CPU 190.2 sec
Hadoop job information for Stage-23: number of mappers: 4; number of reducers: 1
2023-01-05 07:57:41,563 Stage-23 map = 0%,  reduce = 0%
2023-01-05 07:57:42,147 Stage-20 map = 41%,  reduce = 0%, Cumulative CPU 197.95 sec
2023-01-05 07:57:43,160 Stage-20 map = 42%,  reduce = 0%, Cumulative CPU 200.53 sec
2023-01-05 07:57:48,318 Stage-20 map = 44%,  reduce = 0%, Cumulative CPU 205.39 sec
2023-01-05 07:57:49,331 Stage-20 map = 45%,  reduce = 0%, Cumulative CPU 208.01 sec
2023-01-05 07:57:53,025 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:57:54,465 Stage-20 map = 46%,  reduce = 0%, Cumulative CPU 214.33 sec
Starting Job = job_1672890466700_0087, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0087/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0087
2023-01-05 07:57:59,113 Stage-23 map = 1%,  reduce = 0%, Cumulative CPU 10.41 sec
2023-01-05 07:57:59,537 Stage-20 map = 47%,  reduce = 0%, Cumulative CPU 215.72 sec
2023-01-05 07:58:00,552 Stage-20 map = 48%,  reduce = 0%, Cumulative CPU 222.5 sec
2023-01-05 07:58:05,212 Stage-23 map = 2%,  reduce = 0%, Cumulative CPU 38.55 sec
2023-01-05 07:58:05,631 Stage-20 map = 50%,  reduce = 0%, Cumulative CPU 225.43 sec
2023-01-05 07:58:06,228 Stage-23 map = 3%,  reduce = 0%, Cumulative CPU 41.28 sec
2023-01-05 07:58:06,644 Stage-20 map = 51%,  reduce = 0%, Cumulative CPU 229.53 sec
2023-01-05 07:58:08,257 Stage-23 map = 4%,  reduce = 0%, Cumulative CPU 55.84 sec
2023-01-05 07:58:11,508 Stage-23 map = 5%,  reduce = 0%, Cumulative CPU 57.51 sec
2023-01-05 07:58:11,726 Stage-20 map = 52%,  reduce = 0%, Cumulative CPU 232.46 sec
2023-01-05 07:58:12,741 Stage-20 map = 54%,  reduce = 0%, Cumulative CPU 236.84 sec
2023-01-05 07:58:13,544 Stage-23 map = 6%,  reduce = 0%, Cumulative CPU 61.66 sec
2023-01-05 07:58:17,610 Stage-23 map = 7%,  reduce = 0%, Cumulative CPU 66.35 sec
2023-01-05 07:58:17,806 Stage-20 map = 56%,  reduce = 0%, Cumulative CPU 241.8 sec
2023-01-05 07:58:19,638 Stage-23 map = 9%,  reduce = 0%, Cumulative CPU 71.6 sec
2023-01-05 07:58:23,886 Stage-20 map = 58%,  reduce = 0%, Cumulative CPU 246.77 sec
2023-01-05 07:58:25,730 Stage-23 map = 11%,  reduce = 0%, Cumulative CPU 78.38 sec
2023-01-05 07:58:27,940 Stage-20 map = 69%,  reduce = 0%, Cumulative CPU 249.95 sec
2023-01-05 07:58:29,970 Stage-20 map = 71%,  reduce = 0%, Cumulative CPU 253.21 sec
2023-01-05 07:58:30,816 Stage-23 map = 12%,  reduce = 0%, Cumulative CPU 84.99 sec
2023-01-05 07:58:31,830 Stage-23 map = 13%,  reduce = 0%, Cumulative CPU 86.46 sec
2023-01-05 07:58:33,010 Stage-20 map = 80%,  reduce = 0%, Cumulative CPU 255.7 sec
2023-01-05 07:58:35,046 Stage-20 map = 89%,  reduce = 0%, Cumulative CPU 257.12 sec
2023-01-05 07:58:35,821 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:58:35,892 Stage-23 map = 14%,  reduce = 0%, Cumulative CPU 91.1 sec
2023-01-05 07:58:36,060 Stage-20 map = 90%,  reduce = 0%, Cumulative CPU 259.02 sec
2023-01-05 07:58:36,907 Stage-23 map = 15%,  reduce = 0%, Cumulative CPU 93.32 sec
2023-01-05 07:58:40,982 Stage-23 map = 16%,  reduce = 0%, Cumulative CPU 97.56 sec
2023-01-05 07:58:41,997 Stage-23 map = 17%,  reduce = 0%, Cumulative CPU 99.86 sec
2023-01-05 07:58:43,012 Stage-23 map = 18%,  reduce = 0%, Cumulative CPU 102.33 sec
2023-01-05 07:58:44,033 Stage-23 map = 19%,  reduce = 0%, Cumulative CPU 103.72 sec
2023-01-05 07:58:45,174 Stage-20 map = 90%,  reduce = 25%, Cumulative CPU 262.09 sec
2023-01-05 07:58:48,093 Stage-23 map = 20%,  reduce = 0%, Cumulative CPU 107.97 sec
2023-01-05 07:58:48,212 Stage-20 map = 91%,  reduce = 25%, Cumulative CPU 264.6 sec
2023-01-05 07:58:50,121 Stage-23 map = 21%,  reduce = 0%, Cumulative CPU 112.01 sec
2023-01-05 07:58:53,164 Stage-23 map = 22%,  reduce = 0%, Cumulative CPU 115.33 sec
2023-01-05 07:58:53,635 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:58:54,179 Stage-23 map = 23%,  reduce = 0%, Cumulative CPU 116.9 sec
2023-01-05 07:58:56,204 Stage-23 map = 24%,  reduce = 0%, Cumulative CPU 122.29 sec
2023-01-05 07:58:57,329 Stage-20 map = 100%,  reduce = 25%, Cumulative CPU 268.22 sec
2023-01-05 07:58:59,245 Stage-23 map = 25%,  reduce = 0%, Cumulative CPU 125.22 sec
2023-01-05 07:58:59,357 Stage-20 map = 100%,  reduce = 100%, Cumulative CPU 271.24 sec
2023-01-05 07:59:00,260 Stage-23 map = 26%,  reduce = 0%, Cumulative CPU 126.94 sec
2023-01-05 07:59:02,289 Stage-23 map = 27%,  reduce = 0%, Cumulative CPU 132.36 sec
2023-01-05 07:59:05,328 Stage-23 map = 28%,  reduce = 0%, Cumulative CPU 135.13 sec
2023-01-05 07:59:06,342 Stage-23 map = 29%,  reduce = 0%, Cumulative CPU 136.41 sec
MapReduce Total cumulative CPU time: 4 minutes 31 seconds 240 msec
Ended Job = job_1672890466700_0080
Launching Job 12 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 07:59:07,354 Stage-23 map = 30%,  reduce = 0%, Cumulative CPU 138.46 sec
2023-01-05 07:59:11,611 Stage-23 map = 31%,  reduce = 0%, Cumulative CPU 143.19 sec
2023-01-05 07:59:12,623 Stage-23 map = 32%,  reduce = 0%, Cumulative CPU 144.81 sec
2023-01-05 07:59:13,636 Stage-23 map = 34%,  reduce = 0%, Cumulative CPU 149.25 sec
2023-01-05 07:59:18,702 Stage-23 map = 35%,  reduce = 0%, Cumulative CPU 154.73 sec
2023-01-05 07:59:19,714 Stage-23 map = 36%,  reduce = 0%, Cumulative CPU 160.15 sec
Hadoop job information for Stage-18: number of mappers: 4; number of reducers: 1
2023-01-05 07:59:20,553 Stage-18 map = 0%,  reduce = 0%
2023-01-05 07:59:23,768 Stage-23 map = 37%,  reduce = 0%, Cumulative CPU 162.3 sec
2023-01-05 07:59:24,782 Stage-23 map = 38%,  reduce = 0%, Cumulative CPU 165.36 sec
2023-01-05 07:59:25,827 Stage-23 map = 39%,  reduce = 0%, Cumulative CPU 169.31 sec
2023-01-05 07:59:28,930 Stage-23 map = 40%,  reduce = 0%, Cumulative CPU 170.82 sec
2023-01-05 07:59:31,020 Stage-23 map = 41%,  reduce = 0%, Cumulative CPU 175.43 sec
Starting Job = job_1672890466700_0088, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0088/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0088
2023-01-05 07:59:35,897 Stage-22 map = 0%,  reduce = 0%
2023-01-05 07:59:36,162 Stage-23 map = 42%,  reduce = 0%, Cumulative CPU 180.85 sec
2023-01-05 07:59:41,238 Stage-23 map = 43%,  reduce = 0%, Cumulative CPU 185.66 sec
2023-01-05 07:59:41,394 Stage-18 map = 2%,  reduce = 0%, Cumulative CPU 37.91 sec
2023-01-05 07:59:43,265 Stage-23 map = 44%,  reduce = 0%, Cumulative CPU 189.67 sec
2023-01-05 07:59:43,432 Stage-18 map = 3%,  reduce = 0%, Cumulative CPU 50.38 sec
2023-01-05 07:59:47,325 Stage-23 map = 45%,  reduce = 0%, Cumulative CPU 193.72 sec
2023-01-05 07:59:47,508 Stage-18 map = 4%,  reduce = 0%, Cumulative CPU 55.05 sec
2023-01-05 07:59:48,524 Stage-18 map = 5%,  reduce = 0%, Cumulative CPU 56.45 sec
2023-01-05 07:59:48,542 Stage-23 map = 46%,  reduce = 0%, Cumulative CPU 196.54 sec
2023-01-05 07:59:50,779 Stage-23 map = 47%,  reduce = 0%, Cumulative CPU 199.33 sec
2023-01-05 07:59:53,806 Stage-18 map = 6%,  reduce = 0%, Cumulative CPU 63.65 sec
2023-01-05 07:59:53,820 Stage-23 map = 48%,  reduce = 0%, Cumulative CPU 200.81 sec
2023-01-05 07:59:54,003 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 07:59:54,819 Stage-18 map = 7%,  reduce = 0%, Cumulative CPU 65.12 sec
2023-01-05 07:59:55,833 Stage-18 map = 8%,  reduce = 0%, Cumulative CPU 66.57 sec
2023-01-05 07:59:55,853 Stage-23 map = 49%,  reduce = 0%, Cumulative CPU 204.79 sec
2023-01-05 07:59:56,867 Stage-23 map = 50%,  reduce = 0%, Cumulative CPU 206.44 sec
2023-01-05 07:59:58,894 Stage-23 map = 61%,  reduce = 0%, Cumulative CPU 208.01 sec
2023-01-05 07:59:59,891 Stage-18 map = 9%,  reduce = 0%, Cumulative CPU 72.04 sec
2023-01-05 08:00:00,906 Stage-18 map = 10%,  reduce = 0%, Cumulative CPU 74.53 sec
2023-01-05 08:00:01,148 Stage-23 map = 62%,  reduce = 0%, Cumulative CPU 211.07 sec
2023-01-05 08:00:01,926 Stage-18 map = 11%,  reduce = 0%, Cumulative CPU 76.31 sec
2023-01-05 08:00:02,224 Stage-23 map = 63%,  reduce = 0%, Cumulative CPU 213.29 sec
2023-01-05 08:00:05,989 Stage-18 map = 12%,  reduce = 0%, Cumulative CPU 79.85 sec
2023-01-05 08:00:07,350 Stage-23 map = 64%,  reduce = 0%, Cumulative CPU 216.4 sec
2023-01-05 08:00:08,020 Stage-18 map = 13%,  reduce = 0%, Cumulative CPU 85.47 sec
2023-01-05 08:00:08,368 Stage-23 map = 65%,  reduce = 0%, Cumulative CPU 218.23 sec
2023-01-05 08:00:11,271 Stage-18 map = 14%,  reduce = 0%, Cumulative CPU 88.85 sec
2023-01-05 08:00:12,284 Stage-18 map = 15%,  reduce = 0%, Cumulative CPU 91.41 sec
2023-01-05 08:00:13,439 Stage-23 map = 66%,  reduce = 0%, Cumulative CPU 221.04 sec
2023-01-05 08:00:14,457 Stage-23 map = 67%,  reduce = 0%, Cumulative CPU 222.69 sec
2023-01-05 08:00:14,513 Stage-18 map = 16%,  reduce = 0%, Cumulative CPU 93.14 sec
2023-01-05 08:00:17,550 Stage-18 map = 17%,  reduce = 0%, Cumulative CPU 97.92 sec
2023-01-05 08:00:18,515 Stage-23 map = 68%,  reduce = 8%, Cumulative CPU 225.27 sec
2023-01-05 08:00:18,589 Stage-18 map = 18%,  reduce = 0%, Cumulative CPU 100.45 sec
2023-01-05 08:00:20,546 Stage-23 map = 69%,  reduce = 8%, Cumulative CPU 228.72 sec
2023-01-05 08:00:23,659 Stage-18 map = 20%,  reduce = 0%, Cumulative CPU 107.02 sec
2023-01-05 08:00:24,817 Stage-23 map = 70%,  reduce = 8%, Cumulative CPU 231.63 sec
2023-01-05 08:00:25,687 Stage-18 map = 21%,  reduce = 0%, Cumulative CPU 111.41 sec
2023-01-05 08:00:26,847 Stage-23 map = 71%,  reduce = 8%, Cumulative CPU 235.11 sec
2023-01-05 08:00:27,863 Stage-23 map = 80%,  reduce = 8%, Cumulative CPU 236.63 sec
2023-01-05 08:00:29,743 Stage-18 map = 22%,  reduce = 0%, Cumulative CPU 116.41 sec
2023-01-05 08:00:30,919 Stage-23 map = 80%,  reduce = 17%, Cumulative CPU 236.67 sec
2023-01-05 08:00:31,781 Stage-18 map = 23%,  reduce = 0%, Cumulative CPU 120.88 sec
2023-01-05 08:00:32,945 Stage-23 map = 89%,  reduce = 17%, Cumulative CPU 239.44 sec
2023-01-05 08:00:36,043 Stage-18 map = 24%,  reduce = 0%, Cumulative CPU 127.65 sec
2023-01-05 08:00:36,089 Stage-22 map = 0%,  reduce = 0%
2023-01-05 08:00:36,998 Stage-23 map = 89%,  reduce = 25%, Cumulative CPU 239.5 sec
2023-01-05 08:00:37,058 Stage-18 map = 25%,  reduce = 0%, Cumulative CPU 131.02 sec
2023-01-05 08:00:38,012 Stage-23 map = 90%,  reduce = 25%, Cumulative CPU 241.95 sec
2023-01-05 08:00:38,070 Stage-18 map = 26%,  reduce = 0%, Cumulative CPU 132.81 sec
2023-01-05 08:00:41,109 Stage-18 map = 28%,  reduce = 0%, Cumulative CPU 137.01 sec
2023-01-05 08:00:44,150 Stage-18 map = 29%,  reduce = 0%, Cumulative CPU 141.31 sec
2023-01-05 08:00:47,231 Stage-18 map = 31%,  reduce = 0%, Cumulative CPU 145.82 sec
2023-01-05 08:00:48,247 Stage-18 map = 32%,  reduce = 0%, Cumulative CPU 148.58 sec
2023-01-05 08:00:49,355 Stage-23 map = 91%,  reduce = 25%, Cumulative CPU 246.31 sec
2023-01-05 08:00:53,314 Stage-18 map = 34%,  reduce = 0%, Cumulative CPU 155.27 sec
2023-01-05 08:00:54,325 Stage-18 map = 35%,  reduce = 0%, Cumulative CPU 157.23 sec
2023-01-05 08:00:54,775 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 08:00:59,395 Stage-18 map = 37%,  reduce = 0%, Cumulative CPU 164.56 sec
2023-01-05 08:01:01,423 Stage-18 map = 38%,  reduce = 0%, Cumulative CPU 169.65 sec
2023-01-05 08:01:01,728 Stage-23 map = 100%,  reduce = 25%, Cumulative CPU 250.98 sec
2023-01-05 08:01:03,754 Stage-23 map = 100%,  reduce = 100%, Cumulative CPU 253.57 sec
2023-01-05 08:01:05,474 Stage-18 map = 39%,  reduce = 0%, Cumulative CPU 173.48 sec
MapReduce Total cumulative CPU time: 4 minutes 13 seconds 570 msec
Ended Job = job_1672890466700_0081
2023-01-05 08:01:06,491 Stage-18 map = 40%,  reduce = 0%, Cumulative CPU 175.39 sec
Launching Job 13 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:01:07,503 Stage-18 map = 41%,  reduce = 0%, Cumulative CPU 178.5 sec
2023-01-05 08:01:11,592 Stage-18 map = 43%,  reduce = 0%, Cumulative CPU 182.56 sec
2023-01-05 08:01:12,605 Stage-18 map = 44%,  reduce = 0%, Cumulative CPU 184.03 sec
2023-01-05 08:01:17,680 Stage-18 map = 46%,  reduce = 0%, Cumulative CPU 189.16 sec
2023-01-05 08:01:19,732 Stage-18 map = 47%,  reduce = 0%, Cumulative CPU 192.52 sec
2023-01-05 08:01:23,903 Stage-18 map = 48%,  reduce = 0%, Cumulative CPU 195.79 sec
2023-01-05 08:01:24,919 Stage-18 map = 49%,  reduce = 0%, Cumulative CPU 197.34 sec
2023-01-05 08:01:30,193 Stage-18 map = 50%,  reduce = 0%, Cumulative CPU 202.85 sec
Starting Job = job_1672890466700_0089, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0089/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0089
2023-01-05 08:01:32,096 Stage-22 map = 1%,  reduce = 0%, Cumulative CPU 24.56 sec
2023-01-05 08:01:32,242 Stage-18 map = 51%,  reduce = 0%, Cumulative CPU 206.13 sec
2023-01-05 08:01:34,295 Stage-22 map = 2%,  reduce = 0%, Cumulative CPU 36.75 sec
2023-01-05 08:01:35,409 Stage-18 map = 52%,  reduce = 0%, Cumulative CPU 209.37 sec
2023-01-05 08:01:36,423 Stage-18 map = 53%,  reduce = 0%, Cumulative CPU 211.32 sec
2023-01-05 08:01:37,348 Stage-22 map = 3%,  reduce = 0%, Cumulative CPU 39.38 sec
2023-01-05 08:01:38,362 Stage-22 map = 4%,  reduce = 0%, Cumulative CPU 55.81 sec
2023-01-05 08:01:41,495 Stage-18 map = 55%,  reduce = 0%, Cumulative CPU 217.1 sec
2023-01-05 08:01:43,454 Stage-22 map = 5%,  reduce = 0%, Cumulative CPU 60.44 sec
2023-01-05 08:01:43,522 Stage-18 map = 67%,  reduce = 0%, Cumulative CPU 221.2 sec
2023-01-05 08:01:44,473 Stage-22 map = 7%,  reduce = 0%, Cumulative CPU 65.69 sec
2023-01-05 08:01:44,534 Stage-18 map = 75%,  reduce = 0%, Cumulative CPU 222.02 sec
2023-01-05 08:01:46,509 Stage-22 map = 8%,  reduce = 0%, Cumulative CPU 68.35 sec
2023-01-05 08:01:47,624 Stage-18 map = 76%,  reduce = 0%, Cumulative CPU 223.22 sec
2023-01-05 08:01:49,717 Stage-18 map = 77%,  reduce = 0%, Cumulative CPU 225.67 sec
2023-01-05 08:01:50,634 Stage-22 map = 10%,  reduce = 0%, Cumulative CPU 76.15 sec
2023-01-05 08:01:52,670 Stage-22 map = 11%,  reduce = 0%, Cumulative CPU 78.65 sec
2023-01-05 08:01:55,085 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 08:01:55,989 Stage-18 map = 78%,  reduce = 0%, Cumulative CPU 229.31 sec
2023-01-05 08:01:56,728 Stage-22 map = 13%,  reduce = 0%, Cumulative CPU 85.72 sec
2023-01-05 08:01:58,759 Stage-22 map = 14%,  reduce = 0%, Cumulative CPU 88.88 sec
2023-01-05 08:02:00,044 Stage-18 map = 79%,  reduce = 0%, Cumulative CPU 231.84 sec
2023-01-05 08:02:02,071 Stage-18 map = 80%,  reduce = 17%, Cumulative CPU 234.15 sec
2023-01-05 08:02:02,814 Stage-22 map = 17%,  reduce = 0%, Cumulative CPU 97.34 sec
2023-01-05 08:02:04,098 Stage-18 map = 89%,  reduce = 17%, Cumulative CPU 234.84 sec
2023-01-05 08:02:07,884 Stage-22 map = 18%,  reduce = 0%, Cumulative CPU 103.64 sec
2023-01-05 08:02:08,148 Stage-18 map = 89%,  reduce = 25%, Cumulative CPU 237.72 sec
2023-01-05 08:02:08,899 Stage-22 map = 20%,  reduce = 0%, Cumulative CPU 110.24 sec
2023-01-05 08:02:12,953 Stage-22 map = 21%,  reduce = 0%, Cumulative CPU 117.82 sec
2023-01-05 08:02:13,967 Stage-22 map = 22%,  reduce = 0%, Cumulative CPU 121.48 sec
2023-01-05 08:02:14,979 Stage-22 map = 23%,  reduce = 0%, Cumulative CPU 123.53 sec
2023-01-05 08:02:17,005 Stage-22 map = 24%,  reduce = 0%, Cumulative CPU 126.95 sec
2023-01-05 08:02:17,673 Stage-18 map = 90%,  reduce = 25%, Cumulative CPU 243.66 sec
2023-01-05 08:02:20,046 Stage-22 map = 25%,  reduce = 0%, Cumulative CPU 134.29 sec
2023-01-05 08:02:21,063 Stage-22 map = 26%,  reduce = 0%, Cumulative CPU 136.29 sec
2023-01-05 08:02:23,092 Stage-22 map = 27%,  reduce = 0%, Cumulative CPU 139.72 sec
2023-01-05 08:02:25,120 Stage-22 map = 28%,  reduce = 0%, Cumulative CPU 143.36 sec
2023-01-05 08:02:26,134 Stage-22 map = 29%,  reduce = 0%, Cumulative CPU 146.6 sec
2023-01-05 08:02:27,149 Stage-22 map = 30%,  reduce = 0%, Cumulative CPU 148.12 sec
2023-01-05 08:02:29,176 Stage-22 map = 31%,  reduce = 0%, Cumulative CPU 151.93 sec
2023-01-05 08:02:29,840 Stage-18 map = 91%,  reduce = 25%, Cumulative CPU 250.27 sec
2023-01-05 08:02:31,413 Stage-22 map = 32%,  reduce = 0%, Cumulative CPU 154.76 sec
2023-01-05 08:02:32,430 Stage-22 map = 33%,  reduce = 0%, Cumulative CPU 160.7 sec
2023-01-05 08:02:34,460 Stage-22 map = 34%,  reduce = 0%, Cumulative CPU 163.73 sec
2023-01-05 08:02:37,711 Stage-22 map = 36%,  reduce = 0%, Cumulative CPU 166.45 sec
2023-01-05 08:02:38,729 Stage-22 map = 37%,  reduce = 0%, Cumulative CPU 174.15 sec
2023-01-05 08:02:40,754 Stage-22 map = 38%,  reduce = 0%, Cumulative CPU 176.92 sec
2023-01-05 08:02:41,007 Stage-18 map = 100%,  reduce = 25%, Cumulative CPU 255.36 sec
2023-01-05 08:02:42,091 Stage-18 map = 100%,  reduce = 100%, Cumulative CPU 257.74 sec
2023-01-05 08:02:43,833 Stage-22 map = 39%,  reduce = 0%, Cumulative CPU 179.05 sec
MapReduce Total cumulative CPU time: 4 minutes 17 seconds 740 msec
Ended Job = job_1672890466700_0082
2023-01-05 08:02:44,845 Stage-22 map = 40%,  reduce = 0%, Cumulative CPU 185.93 sec
Launching Job 14 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:02:46,872 Stage-22 map = 41%,  reduce = 0%, Cumulative CPU 188.57 sec
2023-01-05 08:02:49,907 Stage-22 map = 43%,  reduce = 0%, Cumulative CPU 190.73 sec
2023-01-05 08:02:50,920 Stage-22 map = 45%,  reduce = 0%, Cumulative CPU 197.47 sec
2023-01-05 08:02:52,946 Stage-22 map = 46%,  reduce = 0%, Cumulative CPU 199.13 sec
2023-01-05 08:02:55,876 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 190.62 sec
2023-01-05 08:02:55,990 Stage-22 map = 47%,  reduce = 0%, Cumulative CPU 200.84 sec
2023-01-05 08:02:57,003 Stage-22 map = 49%,  reduce = 0%, Cumulative CPU 206.59 sec
Hadoop job information for Stage-24: number of mappers: 4; number of reducers: 1
2023-01-05 08:02:57,809 Stage-24 map = 0%,  reduce = 0%
2023-01-05 08:02:59,036 Stage-22 map = 50%,  reduce = 0%, Cumulative CPU 208.4 sec
2023-01-05 08:03:01,069 Stage-22 map = 51%,  reduce = 0%, Cumulative CPU 210.13 sec
2023-01-05 08:03:02,082 Stage-22 map = 53%,  reduce = 0%, Cumulative CPU 212.96 sec
2023-01-05 08:03:05,204 Stage-22 map = 54%,  reduce = 0%, Cumulative CPU 217.4 sec
2023-01-05 08:03:06,306 Stage-22 map = 65%,  reduce = 0%, Cumulative CPU 219.1 sec
2023-01-05 08:03:08,335 Stage-22 map = 66%,  reduce = 0%, Cumulative CPU 220.69 sec
2023-01-05 08:03:10,407 Stage-22 map = 67%,  reduce = 0%, Cumulative CPU 224.43 sec
Starting Job = job_1672890466700_0090, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0090/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0090
2023-01-05 08:03:14,691 Stage-22 map = 68%,  reduce = 0%, Cumulative CPU 227.99 sec
2023-01-05 08:03:15,441 Stage-24 map = 1%,  reduce = 0%, Cumulative CPU 9.87 sec
2023-01-05 08:03:17,590 Stage-17 map = 0%,  reduce = 0%
2023-01-05 08:03:20,904 Stage-22 map = 70%,  reduce = 0%, Cumulative CPU 234.36 sec
2023-01-05 08:03:21,839 Stage-24 map = 2%,  reduce = 0%, Cumulative CPU 50.54 sec
2023-01-05 08:03:22,856 Stage-24 map = 3%,  reduce = 0%, Cumulative CPU 54.1 sec
2023-01-05 08:03:27,203 Stage-22 map = 72%,  reduce = 0%, Cumulative CPU 241.24 sec
2023-01-05 08:03:27,982 Stage-24 map = 4%,  reduce = 0%, Cumulative CPU 60.32 sec
2023-01-05 08:03:28,292 Stage-22 map = 80%,  reduce = 8%, Cumulative CPU 242.31 sec
2023-01-05 08:03:29,309 Stage-22 map = 81%,  reduce = 8%, Cumulative CPU 245.32 sec
2023-01-05 08:03:33,061 Stage-24 map = 5%,  reduce = 0%, Cumulative CPU 63.99 sec
2023-01-05 08:03:33,793 Stage-22 map = 81%,  reduce = 17%, Cumulative CPU 246.87 sec
2023-01-05 08:03:35,218 Stage-24 map = 6%,  reduce = 0%, Cumulative CPU 70.63 sec
2023-01-05 08:03:39,015 Stage-22 map = 82%,  reduce = 17%, Cumulative CPU 250.59 sec
2023-01-05 08:03:39,285 Stage-24 map = 7%,  reduce = 0%, Cumulative CPU 74.88 sec
2023-01-05 08:03:41,315 Stage-24 map = 8%,  reduce = 0%, Cumulative CPU 79.58 sec
2023-01-05 08:03:44,003 Stage-17 map = 1%,  reduce = 0%, Cumulative CPU 27.71 sec
2023-01-05 08:03:45,385 Stage-24 map = 9%,  reduce = 0%, Cumulative CPU 84.49 sec
2023-01-05 08:03:46,407 Stage-22 map = 91%,  reduce = 25%, Cumulative CPU 254.76 sec
2023-01-05 08:03:47,420 Stage-24 map = 10%,  reduce = 0%, Cumulative CPU 88.36 sec
2023-01-05 08:03:48,513 Stage-22 map = 100%,  reduce = 25%, Cumulative CPU 259.56 sec
2023-01-05 08:03:50,294 Stage-17 map = 2%,  reduce = 0%, Cumulative CPU 33.57 sec
2023-01-05 08:03:50,616 Stage-22 map = 100%,  reduce = 100%, Cumulative CPU 261.07 sec
2023-01-05 08:03:52,497 Stage-24 map = 11%,  reduce = 0%, Cumulative CPU 95.81 sec
MapReduce Total cumulative CPU time: 4 minutes 21 seconds 70 msec
Ended Job = job_1672890466700_0083
2023-01-05 08:03:53,512 Stage-24 map = 12%,  reduce = 0%, Cumulative CPU 98.4 sec
Launching Job 15 out of 45
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:03:56,694 Stage-17 map = 3%,  reduce = 0%, Cumulative CPU 50.8 sec
2023-01-05 08:03:58,788 Stage-24 map = 14%,  reduce = 0%, Cumulative CPU 106.87 sec
2023-01-05 08:04:01,802 Stage-17 map = 4%,  reduce = 0%, Cumulative CPU 69.27 sec
2023-01-05 08:04:04,939 Stage-24 map = 16%,  reduce = 0%, Cumulative CPU 114.12 sec
Hadoop job information for Stage-25: number of mappers: 4; number of reducers: 1
2023-01-05 08:04:06,495 Stage-25 map = 0%,  reduce = 0%
2023-01-05 08:04:07,877 Stage-17 map = 6%,  reduce = 0%, Cumulative CPU 82.16 sec
2023-01-05 08:04:09,091 Stage-17 map = 7%,  reduce = 0%, Cumulative CPU 83.55 sec
2023-01-05 08:04:10,030 Stage-24 map = 17%,  reduce = 0%, Cumulative CPU 118.33 sec
2023-01-05 08:04:11,045 Stage-24 map = 18%,  reduce = 0%, Cumulative CPU 121.86 sec
2023-01-05 08:04:14,289 Stage-17 map = 8%,  reduce = 0%, Cumulative CPU 88.09 sec
2023-01-05 08:04:15,156 Stage-24 map = 19%,  reduce = 0%, Cumulative CPU 122.96 sec
2023-01-05 08:04:15,306 Stage-17 map = 9%,  reduce = 0%, Cumulative CPU 89.8 sec
2023-01-05 08:04:17,233 Stage-24 map = 20%,  reduce = 0%, Cumulative CPU 128.09 sec
2023-01-05 08:04:20,524 Stage-17 map = 10%,  reduce = 0%, Cumulative CPU 94.48 sec
2023-01-05 08:04:21,416 Stage-24 map = 21%,  reduce = 0%, Cumulative CPU 131.53 sec
2023-01-05 08:04:24,088 Stage-25 map = 1%,  reduce = 0%, Cumulative CPU 10.79 sec
2023-01-05 08:04:25,903 Stage-17 map = 11%,  reduce = 0%, Cumulative CPU 99.47 sec
2023-01-05 08:04:27,530 Stage-24 map = 22%,  reduce = 0%, Cumulative CPU 138.99 sec
2023-01-05 08:04:28,003 Stage-17 map = 12%,  reduce = 0%, Cumulative CPU 102.89 sec
2023-01-05 08:04:29,568 Stage-24 map = 23%,  reduce = 0%, Cumulative CPU 142.34 sec
2023-01-05 08:04:30,222 Stage-25 map = 2%,  reduce = 0%, Cumulative CPU 25.37 sec
Starting Job = job_1672890466700_0091, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0091/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0091
2023-01-05 08:04:33,263 Stage-25 map = 3%,  reduce = 0%, Cumulative CPU 28.82 sec
2023-01-05 08:04:33,302 Stage-17 map = 13%,  reduce = 0%, Cumulative CPU 109.57 sec
2023-01-05 08:04:33,658 Stage-24 map = 24%,  reduce = 0%, Cumulative CPU 146.95 sec
2023-01-05 08:04:39,364 Stage-25 map = 4%,  reduce = 0%, Cumulative CPU 43.67 sec
2023-01-05 08:04:39,763 Stage-24 map = 25%,  reduce = 0%, Cumulative CPU 153.0 sec
2023-01-05 08:04:40,820 Stage-24 map = 26%,  reduce = 0%, Cumulative CPU 157.39 sec
2023-01-05 08:04:45,467 Stage-25 map = 5%,  reduce = 0%, Cumulative CPU 39.43 sec
2023-01-05 08:04:46,967 Stage-24 map = 27%,  reduce = 0%, Cumulative CPU 162.09 sec
2023-01-05 08:04:48,621 Stage-25 map = 6%,  reduce = 0%, Cumulative CPU 42.4 sec
2023-01-05 08:04:53,352 Stage-24 map = 28%,  reduce = 0%, Cumulative CPU 169.34 sec
2023-01-05 08:04:54,768 Stage-25 map = 7%,  reduce = 0%, Cumulative CPU 48.97 sec
2023-01-05 08:04:56,851 Stage-25 map = 8%,  reduce = 0%, Cumulative CPU 51.79 sec
2023-01-05 08:04:59,529 Stage-24 map = 29%,  reduce = 0%, Cumulative CPU 175.73 sec
2023-01-05 08:05:03,051 Stage-25 map = 9%,  reduce = 0%, Cumulative CPU 58.81 sec
2023-01-05 08:05:03,644 Stage-24 map = 30%,  reduce = 0%, Cumulative CPU 178.96 sec
2023-01-05 08:05:09,197 Stage-25 map = 10%,  reduce = 0%, Cumulative CPU 65.68 sec
2023-01-05 08:05:09,750 Stage-24 map = 31%,  reduce = 0%, Cumulative CPU 184.94 sec
2023-01-05 08:05:15,338 Stage-25 map = 11%,  reduce = 0%, Cumulative CPU 72.59 sec
2023-01-05 08:05:16,052 Stage-24 map = 32%,  reduce = 0%, Cumulative CPU 190.31 sec
2023-01-05 08:05:18,437 Stage-25 map = 12%,  reduce = 0%, Cumulative CPU 76.15 sec
2023-01-05 08:05:21,488 Stage-25 map = 13%,  reduce = 0%, Cumulative CPU 78.35 sec
2023-01-05 08:05:22,143 Stage-24 map = 33%,  reduce = 0%, Cumulative CPU 195.86 sec
2023-01-05 08:05:27,657 Stage-25 map = 14%,  reduce = 0%, Cumulative CPU 84.99 sec
2023-01-05 08:05:28,246 Stage-24 map = 34%,  reduce = 0%, Cumulative CPU 201.13 sec
2023-01-05 08:05:30,703 Stage-25 map = 15%,  reduce = 0%, Cumulative CPU 88.52 sec
2023-01-05 08:05:33,744 Stage-25 map = 16%,  reduce = 0%, Cumulative CPU 91.12 sec
2023-01-05 08:05:34,552 Stage-24 map = 35%,  reduce = 0%, Cumulative CPU 206.19 sec
2023-01-05 08:05:35,572 Stage-24 map = 36%,  reduce = 0%, Cumulative CPU 209.76 sec
2023-01-05 08:05:36,816 Stage-25 map = 17%,  reduce = 0%, Cumulative CPU 94.97 sec
2023-01-05 08:05:40,887 Stage-24 map = 37%,  reduce = 0%, Cumulative CPU 213.81 sec
2023-01-05 08:05:42,928 Stage-25 map = 18%,  reduce = 0%, Cumulative CPU 102.15 sec
2023-01-05 08:05:45,983 Stage-24 map = 38%,  reduce = 0%, Cumulative CPU 215.48 sec
2023-01-05 08:05:45,987 Stage-25 map = 19%,  reduce = 0%, Cumulative CPU 106.44 sec
2023-01-05 08:05:47,199 Stage-24 map = 49%,  reduce = 0%, Cumulative CPU 218.5 sec
2023-01-05 08:05:49,035 Stage-25 map = 20%,  reduce = 0%, Cumulative CPU 109.71 sec
2023-01-05 08:05:53,432 Stage-24 map = 58%,  reduce = 0%, Cumulative CPU 220.95 sec
2023-01-05 08:05:54,365 Stage-25 map = 21%,  reduce = 0%, Cumulative CPU 115.37 sec
2023-01-05 08:05:57,444 Stage-25 map = 22%,  reduce = 0%, Cumulative CPU 118.33 sec
2023-01-05 08:06:00,488 Stage-25 map = 23%,  reduce = 0%, Cumulative CPU 121.26 sec
2023-01-05 08:06:05,720 Stage-24 map = 58%,  reduce = 17%, Cumulative CPU 221.47 sec
2023-01-05 08:06:06,633 Stage-25 map = 24%,  reduce = 0%, Cumulative CPU 126.7 sec
2023-01-05 08:06:09,729 Stage-25 map = 25%,  reduce = 0%, Cumulative CPU 129.52 sec
2023-01-05 08:06:12,820 Stage-25 map = 26%,  reduce = 0%, Cumulative CPU 131.52 sec
2023-01-05 08:06:18,909 Stage-25 map = 27%,  reduce = 0%, Cumulative CPU 136.36 sec
2023-01-05 08:06:21,955 Stage-25 map = 28%,  reduce = 0%, Cumulative CPU 139.93 sec
2023-01-05 08:06:23,982 Stage-25 map = 39%,  reduce = 0%, Cumulative CPU 142.08 sec
2023-01-05 08:06:28,042 Stage-25 map = 40%,  reduce = 0%, Cumulative CPU 145.38 sec
2023-01-05 08:06:33,110 Stage-25 map = 50%,  reduce = 0%, Cumulative CPU 148.24 sec
2023-01-05 08:06:41,215 Stage-25 map = 50%,  reduce = 17%, Cumulative CPU 148.74 sec
2023-01-05 08:06:45,049 Stage-24 map = 59%,  reduce = 17%, Cumulative CPU 211.54 sec
2023-01-05 08:06:51,127 Stage-24 map = 62%,  reduce = 17%, Cumulative CPU 205.21 sec
2023-01-05 08:06:57,210 Stage-24 map = 64%,  reduce = 17%, Cumulative CPU 212.05 sec
2023-01-05 08:07:03,285 Stage-24 map = 66%,  reduce = 17%, Cumulative CPU 219.18 sec
2023-01-05 08:07:06,751 Stage-25 map = 52%,  reduce = 17%, Cumulative CPU 158.16 sec
2023-01-05 08:07:08,353 Stage-24 map = 67%,  reduce = 17%, Cumulative CPU 224.24 sec
2023-01-05 08:07:09,366 Stage-24 map = 68%,  reduce = 17%, Cumulative CPU 226.83 sec
2023-01-05 08:07:13,028 Stage-25 map = 53%,  reduce = 17%, Cumulative CPU 159.93 sec
2023-01-05 08:07:14,429 Stage-24 map = 69%,  reduce = 17%, Cumulative CPU 231.72 sec
2023-01-05 08:07:15,443 Stage-24 map = 70%,  reduce = 17%, Cumulative CPU 233.12 sec
2023-01-05 08:07:19,311 Stage-25 map = 54%,  reduce = 17%, Cumulative CPU 161.69 sec
2023-01-05 08:07:20,508 Stage-24 map = 71%,  reduce = 17%, Cumulative CPU 237.3 sec
2023-01-05 08:07:21,521 Stage-24 map = 72%,  reduce = 17%, Cumulative CPU 238.69 sec
2023-01-05 08:07:24,577 Stage-25 map = 56%,  reduce = 17%, Cumulative CPU 163.41 sec
2023-01-05 08:07:26,590 Stage-24 map = 74%,  reduce = 17%, Cumulative CPU 241.82 sec
2023-01-05 08:07:27,602 Stage-24 map = 75%,  reduce = 17%, Cumulative CPU 243.45 sec
2023-01-05 08:07:30,856 Stage-25 map = 57%,  reduce = 17%, Cumulative CPU 166.34 sec
2023-01-05 08:07:32,663 Stage-24 map = 76%,  reduce = 17%, Cumulative CPU 246.09 sec
2023-01-05 08:07:33,675 Stage-24 map = 77%,  reduce = 17%, Cumulative CPU 247.61 sec
2023-01-05 08:07:36,934 Stage-25 map = 58%,  reduce = 17%, Cumulative CPU 169.9 sec
2023-01-05 08:07:38,942 Stage-24 map = 78%,  reduce = 17%, Cumulative CPU 249.86 sec
2023-01-05 08:07:39,955 Stage-24 map = 79%,  reduce = 17%, Cumulative CPU 252.47 sec
2023-01-05 08:07:45,025 Stage-24 map = 89%,  reduce = 17%, Cumulative CPU 254.05 sec
2023-01-05 08:07:46,036 Stage-24 map = 90%,  reduce = 17%, Cumulative CPU 257.53 sec
2023-01-05 08:07:48,060 Stage-24 map = 90%,  reduce = 25%, Cumulative CPU 257.56 sec
2023-01-05 08:07:49,081 Stage-25 map = 59%,  reduce = 17%, Cumulative CPU 176.86 sec
2023-01-05 08:07:55,173 Stage-25 map = 60%,  reduce = 17%, Cumulative CPU 180.2 sec
2023-01-05 08:07:57,197 Stage-24 map = 91%,  reduce = 25%, Cumulative CPU 265.3 sec
2023-01-05 08:08:01,246 Stage-25 map = 62%,  reduce = 17%, Cumulative CPU 183.26 sec
2023-01-05 08:08:02,259 Stage-24 map = 100%,  reduce = 25%, Cumulative CPU 266.98 sec
2023-01-05 08:08:03,272 Stage-24 map = 100%,  reduce = 100%, Cumulative CPU 269.44 sec
2023-01-05 08:08:07,327 Stage-25 map = 65%,  reduce = 17%, Cumulative CPU 187.82 sec
2023-01-05 08:08:12,590 Stage-25 map = 66%,  reduce = 17%, Cumulative CPU 190.08 sec
2023-01-05 08:08:14,816 Stage-25 map = 75%,  reduce = 17%, Cumulative CPU 190.62 sec
2023-01-05 08:08:17,860 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 190.64 sec
2023-01-05 08:09:03,464 Stage-24 map = 100%,  reduce = 100%, Cumulative CPU 269.44 sec
2023-01-05 08:09:18,662 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 190.85 sec
2023-01-05 08:10:04,290 Stage-24 map = 100%,  reduce = 100%, Cumulative CPU 269.44 sec
2023-01-05 08:10:19,503 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 191.05 sec
MapReduce Total cumulative CPU time: 4 minutes 29 seconds 440 msec
Ended Job = job_1672890466700_0085
Stage-75 is selected by condition resolver.
Stage-1 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
Hadoop job information for Stage-26: number of mappers: 4; number of reducers: 1
2023-01-05 08:11:12,327 Stage-26 map = 0%,  reduce = 0%
2023-01-05 08:11:12	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 08:11:13	End of local task; Time Taken: 0.404 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 17 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0092, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0092/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0092
2023-01-05 08:11:20,416 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 191.26 sec
2023-01-05 08:11:31,071 Stage-26 map = 4%,  reduce = 0%, Cumulative CPU 17.08 sec
2023-01-05 08:11:37,171 Stage-26 map = 7%,  reduce = 0%, Cumulative CPU 52.71 sec
2023-01-05 08:11:39,208 Stage-26 map = 8%,  reduce = 0%, Cumulative CPU 55.96 sec
2023-01-05 08:11:43,277 Stage-26 map = 10%,  reduce = 0%, Cumulative CPU 61.32 sec
2023-01-05 08:11:44,299 Stage-26 map = 11%,  reduce = 0%, Cumulative CPU 64.21 sec
2023-01-05 08:11:49,379 Stage-26 map = 13%,  reduce = 0%, Cumulative CPU 68.77 sec
2023-01-05 08:11:55,467 Stage-26 map = 16%,  reduce = 0%, Cumulative CPU 76.5 sec
2023-01-05 08:12:01,555 Stage-26 map = 18%,  reduce = 0%, Cumulative CPU 85.92 sec
2023-01-05 08:12:02,574 Stage-26 map = 19%,  reduce = 0%, Cumulative CPU 89.08 sec
2023-01-05 08:12:07,651 Stage-26 map = 21%,  reduce = 0%, Cumulative CPU 96.55 sec
2023-01-05 08:12:08,663 Stage-26 map = 22%,  reduce = 0%, Cumulative CPU 99.34 sec
2023-01-05 08:12:13,746 Stage-26 map = 24%,  reduce = 0%, Cumulative CPU 106.27 sec
2023-01-05 08:12:14,763 Stage-26 map = 25%,  reduce = 0%, Cumulative CPU 108.82 sec
2023-01-05 08:12:18,816 Stage-26 map = 26%,  reduce = 0%, Cumulative CPU 113.63 sec
2023-01-05 08:12:19,831 Stage-26 map = 28%,  reduce = 0%, Cumulative CPU 115.36 sec
2023-01-05 08:12:20,624 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 191.46 sec
2023-01-05 08:12:20,844 Stage-26 map = 29%,  reduce = 0%, Cumulative CPU 118.55 sec
2023-01-05 08:12:24,911 Stage-26 map = 30%,  reduce = 0%, Cumulative CPU 123.62 sec
2023-01-05 08:12:25,925 Stage-26 map = 31%,  reduce = 0%, Cumulative CPU 125.44 sec
2023-01-05 08:12:26,942 Stage-26 map = 33%,  reduce = 0%, Cumulative CPU 128.08 sec
2023-01-05 08:12:31,003 Stage-26 map = 34%,  reduce = 0%, Cumulative CPU 133.18 sec
2023-01-05 08:12:32,019 Stage-26 map = 35%,  reduce = 0%, Cumulative CPU 134.37 sec
2023-01-05 08:12:33,033 Stage-26 map = 36%,  reduce = 0%, Cumulative CPU 135.85 sec
2023-01-05 08:12:37,293 Stage-26 map = 38%,  reduce = 0%, Cumulative CPU 142.37 sec
2023-01-05 08:12:38,310 Stage-26 map = 39%,  reduce = 0%, Cumulative CPU 144.08 sec
2023-01-05 08:12:43,594 Stage-26 map = 41%,  reduce = 0%, Cumulative CPU 150.33 sec
2023-01-05 08:12:44,610 Stage-26 map = 42%,  reduce = 0%, Cumulative CPU 151.66 sec
2023-01-05 08:12:49,686 Stage-26 map = 44%,  reduce = 0%, Cumulative CPU 157.3 sec
2023-01-05 08:12:50,704 Stage-26 map = 45%,  reduce = 0%, Cumulative CPU 158.57 sec
2023-01-05 08:12:55,771 Stage-26 map = 48%,  reduce = 0%, Cumulative CPU 164.24 sec
2023-01-05 08:13:00,841 Stage-26 map = 50%,  reduce = 0%, Cumulative CPU 170.02 sec
2023-01-05 08:13:01,857 Stage-26 map = 51%,  reduce = 0%, Cumulative CPU 172.54 sec
2023-01-05 08:13:06,926 Stage-26 map = 53%,  reduce = 0%, Cumulative CPU 177.95 sec
2023-01-05 08:13:08,952 Stage-26 map = 54%,  reduce = 0%, Cumulative CPU 182.45 sec
2023-01-05 08:13:13,002 Stage-26 map = 56%,  reduce = 0%, Cumulative CPU 184.72 sec
2023-01-05 08:13:15,032 Stage-26 map = 57%,  reduce = 0%, Cumulative CPU 189.34 sec
2023-01-05 08:13:19,089 Stage-26 map = 67%,  reduce = 0%, Cumulative CPU 191.56 sec
2023-01-05 08:13:20,805 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 191.63 sec
2023-01-05 08:13:21,121 Stage-26 map = 68%,  reduce = 0%, Cumulative CPU 197.77 sec
2023-01-05 08:13:25,186 Stage-26 map = 78%,  reduce = 0%, Cumulative CPU 200.9 sec
2023-01-05 08:13:26,201 Stage-26 map = 79%,  reduce = 0%, Cumulative CPU 203.77 sec
2023-01-05 08:13:28,230 Stage-26 map = 90%,  reduce = 0%, Cumulative CPU 204.59 sec
2023-01-05 08:13:31,268 Stage-26 map = 100%,  reduce = 0%, Cumulative CPU 207.91 sec
2023-01-05 08:13:32,280 Stage-26 map = 100%,  reduce = 100%, Cumulative CPU 210.29 sec
MapReduce Total cumulative CPU time: 3 minutes 30 seconds 290 msec
Ended Job = job_1672890466700_0087
Hadoop job information for Stage-27: number of mappers: 4; number of reducers: 1
2023-01-05 08:13:44,849 Stage-27 map = 0%,  reduce = 0%
2023-01-05 08:13:59,292 Stage-27 map = 1%,  reduce = 0%, Cumulative CPU 7.39 sec
2023-01-05 08:14:03,370 Stage-27 map = 3%,  reduce = 0%, Cumulative CPU 16.8 sec
2023-01-05 08:14:05,400 Stage-27 map = 5%,  reduce = 0%, Cumulative CPU 33.94 sec
2023-01-05 08:14:09,464 Stage-27 map = 6%,  reduce = 0%, Cumulative CPU 50.17 sec
2023-01-05 08:14:11,492 Stage-27 map = 8%,  reduce = 0%, Cumulative CPU 56.85 sec
2023-01-05 08:14:15,558 Stage-27 map = 9%,  reduce = 0%, Cumulative CPU 61.65 sec
2023-01-05 08:14:17,590 Stage-27 map = 10%,  reduce = 0%, Cumulative CPU 66.26 sec
2023-01-05 08:14:18,606 Stage-27 map = 11%,  reduce = 0%, Cumulative CPU 70.13 sec
2023-01-05 08:14:21,089 Stage-25 map = 75%,  reduce = 25%, Cumulative CPU 191.86 sec
2023-01-05 08:14:21,863 Stage-27 map = 12%,  reduce = 0%, Cumulative CPU 71.44 sec
2023-01-05 08:14:23,896 Stage-27 map = 14%,  reduce = 0%, Cumulative CPU 79.21 sec
2023-01-05 08:14:26,366 Stage-25 map = 76%,  reduce = 25%, Cumulative CPU 203.87 sec
2023-01-05 08:14:26,959 Stage-27 map = 15%,  reduce = 0%, Cumulative CPU 80.83 sec
2023-01-05 08:14:30,059 Stage-27 map = 17%,  reduce = 0%, Cumulative CPU 88.5 sec
2023-01-05 08:14:33,109 Stage-27 map = 18%,  reduce = 0%, Cumulative CPU 91.03 sec
2023-01-05 08:14:36,160 Stage-27 map = 20%,  reduce = 0%, Cumulative CPU 98.7 sec
2023-01-05 08:14:37,784 Stage-25 map = 77%,  reduce = 25%, Cumulative CPU 209.92 sec
2023-01-05 08:14:39,206 Stage-27 map = 21%,  reduce = 0%, Cumulative CPU 101.75 sec
2023-01-05 08:14:42,265 Stage-27 map = 23%,  reduce = 0%, Cumulative CPU 107.75 sec
2023-01-05 08:14:48,365 Stage-27 map = 26%,  reduce = 0%, Cumulative CPU 117.86 sec
2023-01-05 08:14:49,952 Stage-25 map = 78%,  reduce = 25%, Cumulative CPU 216.15 sec
2023-01-05 08:14:53,470 Stage-27 map = 28%,  reduce = 0%, Cumulative CPU 124.56 sec
2023-01-05 08:14:56,036 Stage-25 map = 79%,  reduce = 25%, Cumulative CPU 219.05 sec
2023-01-05 08:14:57,534 Stage-27 map = 29%,  reduce = 0%, Cumulative CPU 129.12 sec
2023-01-05 08:14:59,564 Stage-27 map = 30%,  reduce = 0%, Cumulative CPU 132.26 sec
2023-01-05 08:15:00,581 Stage-27 map = 31%,  reduce = 0%, Cumulative CPU 133.85 sec
2023-01-05 08:15:03,647 Stage-27 map = 32%,  reduce = 0%, Cumulative CPU 136.67 sec
2023-01-05 08:15:05,673 Stage-27 map = 34%,  reduce = 0%, Cumulative CPU 140.23 sec
2023-01-05 08:15:08,212 Stage-25 map = 80%,  reduce = 25%, Cumulative CPU 224.39 sec
2023-01-05 08:15:09,736 Stage-27 map = 35%,  reduce = 0%, Cumulative CPU 144.7 sec
2023-01-05 08:15:11,769 Stage-27 map = 37%,  reduce = 0%, Cumulative CPU 148.16 sec
2023-01-05 08:15:12,786 Stage-27 map = 38%,  reduce = 0%, Cumulative CPU 149.05 sec
2023-01-05 08:15:14,300 Stage-25 map = 81%,  reduce = 25%, Cumulative CPU 226.44 sec
2023-01-05 08:15:14,817 Stage-27 map = 39%,  reduce = 0%, Cumulative CPU 150.87 sec
2023-01-05 08:15:17,859 Stage-27 map = 41%,  reduce = 0%, Cumulative CPU 154.93 sec
2023-01-05 08:15:20,377 Stage-25 map = 82%,  reduce = 25%, Cumulative CPU 229.55 sec
2023-01-05 08:15:23,946 Stage-27 map = 44%,  reduce = 0%, Cumulative CPU 159.68 sec
2023-01-05 08:15:26,467 Stage-25 map = 83%,  reduce = 25%, Cumulative CPU 231.72 sec
2023-01-05 08:15:30,033 Stage-27 map = 47%,  reduce = 0%, Cumulative CPU 165.42 sec
2023-01-05 08:15:31,739 Stage-25 map = 84%,  reduce = 25%, Cumulative CPU 234.5 sec
2023-01-05 08:15:33,080 Stage-27 map = 48%,  reduce = 0%, Cumulative CPU 167.03 sec
2023-01-05 08:15:36,133 Stage-27 map = 50%,  reduce = 0%, Cumulative CPU 171.73 sec
2023-01-05 08:15:37,827 Stage-25 map = 85%,  reduce = 25%, Cumulative CPU 236.08 sec
2023-01-05 08:15:41,400 Stage-27 map = 51%,  reduce = 0%, Cumulative CPU 172.85 sec
2023-01-05 08:15:42,414 Stage-27 map = 53%,  reduce = 0%, Cumulative CPU 179.96 sec
2023-01-05 08:15:43,908 Stage-25 map = 87%,  reduce = 25%, Cumulative CPU 238.19 sec
2023-01-05 08:15:47,481 Stage-27 map = 55%,  reduce = 0%, Cumulative CPU 185.66 sec
2023-01-05 08:15:49,992 Stage-25 map = 88%,  reduce = 25%, Cumulative CPU 240.1 sec
2023-01-05 08:15:51,736 Stage-27 map = 56%,  reduce = 0%, Cumulative CPU 189.43 sec
2023-01-05 08:15:53,764 Stage-27 map = 68%,  reduce = 0%, Cumulative CPU 192.53 sec
2023-01-05 08:15:55,792 Stage-27 map = 77%,  reduce = 0%, Cumulative CPU 195.74 sec
2023-01-05 08:15:56,117 Stage-25 map = 89%,  reduce = 25%, Cumulative CPU 243.56 sec
2023-01-05 08:15:59,871 Stage-27 map = 79%,  reduce = 0%, Cumulative CPU 202.54 sec
2023-01-05 08:16:02,230 Stage-25 map = 90%,  reduce = 25%, Cumulative CPU 246.64 sec
2023-01-05 08:16:05,962 Stage-27 map = 89%,  reduce = 0%, Cumulative CPU 208.54 sec
2023-01-05 08:16:10,023 Stage-27 map = 89%,  reduce = 25%, Cumulative CPU 209.1 sec
2023-01-05 08:16:14,401 Stage-25 map = 91%,  reduce = 25%, Cumulative CPU 253.51 sec
2023-01-05 08:16:17,442 Stage-25 map = 100%,  reduce = 25%, Cumulative CPU 255.56 sec
2023-01-05 08:16:18,153 Stage-27 map = 90%,  reduce = 25%, Cumulative CPU 212.94 sec
2023-01-05 08:16:18,454 Stage-25 map = 100%,  reduce = 100%, Cumulative CPU 256.61 sec
MapReduce Total cumulative CPU time: 4 minutes 16 seconds 610 msec
Ended Job = job_1672890466700_0086
Hadoop job information for Stage-28: number of mappers: 4; number of reducers: 1
2023-01-05 08:16:29,490 Stage-28 map = 0%,  reduce = 0%
2023-01-05 08:16:29,512 Stage-27 map = 91%,  reduce = 25%, Cumulative CPU 215.02 sec
2023-01-05 08:16:44,742 Stage-27 map = 100%,  reduce = 25%, Cumulative CPU 218.3 sec
2023-01-05 08:16:45,755 Stage-27 map = 100%,  reduce = 67%, Cumulative CPU 220.0 sec
2023-01-05 08:16:46,772 Stage-27 map = 100%,  reduce = 100%, Cumulative CPU 220.87 sec
2023-01-05 08:16:47,553 Stage-28 map = 2%,  reduce = 0%, Cumulative CPU 9.82 sec
MapReduce Total cumulative CPU time: 3 minutes 40 seconds 870 msec
Ended Job = job_1672890466700_0088
2023-01-05 08:16:48,570 Stage-28 map = 5%,  reduce = 0%, Cumulative CPU 20.05 sec
2023-01-05 08:16:51,618 Stage-28 map = 6%,  reduce = 0%, Cumulative CPU 48.55 sec
2023-01-05 08:16:53,653 Stage-28 map = 7%,  reduce = 0%, Cumulative CPU 51.87 sec
2023-01-05 08:16:54,678 Stage-28 map = 8%,  reduce = 0%, Cumulative CPU 54.02 sec
2023-01-05 08:16:57,724 Stage-28 map = 10%,  reduce = 0%, Cumulative CPU 59.98 sec
2023-01-05 08:16:59,765 Stage-28 map = 12%,  reduce = 0%, Cumulative CPU 65.26 sec
Hadoop job information for Stage-29: number of mappers: 4; number of reducers: 1
2023-01-05 08:17:01,363 Stage-29 map = 0%,  reduce = 0%
2023-01-05 08:17:03,824 Stage-28 map = 15%,  reduce = 0%, Cumulative CPU 71.83 sec
2023-01-05 08:17:05,990 Stage-28 map = 16%,  reduce = 0%, Cumulative CPU 76.53 sec
2023-01-05 08:17:09,193 Stage-28 map = 17%,  reduce = 0%, Cumulative CPU 78.77 sec
2023-01-05 08:17:11,390 Stage-28 map = 18%,  reduce = 0%, Cumulative CPU 84.57 sec
2023-01-05 08:17:12,413 Stage-28 map = 19%,  reduce = 0%, Cumulative CPU 87.91 sec
2023-01-05 08:17:15,616 Stage-28 map = 20%,  reduce = 0%, Cumulative CPU 92.35 sec
2023-01-05 08:17:17,672 Stage-29 map = 2%,  reduce = 0%, Cumulative CPU 10.01 sec
2023-01-05 08:17:18,686 Stage-29 map = 3%,  reduce = 0%, Cumulative CPU 20.0 sec
2023-01-05 08:17:18,789 Stage-28 map = 21%,  reduce = 0%, Cumulative CPU 99.1 sec
2023-01-05 08:17:22,102 Stage-28 map = 22%,  reduce = 0%, Cumulative CPU 103.34 sec
2023-01-05 08:17:23,758 Stage-29 map = 4%,  reduce = 0%, Cumulative CPU 21.78 sec
2023-01-05 08:17:24,296 Stage-28 map = 23%,  reduce = 0%, Cumulative CPU 109.83 sec
2023-01-05 08:17:24,773 Stage-29 map = 5%,  reduce = 0%, Cumulative CPU 23.56 sec
2023-01-05 08:17:27,346 Stage-28 map = 24%,  reduce = 0%, Cumulative CPU 111.51 sec
2023-01-05 08:17:29,847 Stage-29 map = 6%,  reduce = 0%, Cumulative CPU 50.99 sec
2023-01-05 08:17:30,408 Stage-28 map = 26%,  reduce = 0%, Cumulative CPU 119.24 sec
2023-01-05 08:17:31,885 Stage-29 map = 7%,  reduce = 0%, Cumulative CPU 58.89 sec
2023-01-05 08:17:33,472 Stage-28 map = 27%,  reduce = 0%, Cumulative CPU 121.69 sec
2023-01-05 08:17:34,499 Stage-28 map = 28%,  reduce = 0%, Cumulative CPU 124.49 sec
2023-01-05 08:17:35,513 Stage-28 map = 29%,  reduce = 0%, Cumulative CPU 127.07 sec
2023-01-05 08:17:35,959 Stage-29 map = 8%,  reduce = 0%, Cumulative CPU 60.5 sec
2023-01-05 08:17:36,974 Stage-29 map = 9%,  reduce = 0%, Cumulative CPU 62.64 sec
2023-01-05 08:17:37,986 Stage-29 map = 10%,  reduce = 0%, Cumulative CPU 67.37 sec
2023-01-05 08:17:39,576 Stage-28 map = 31%,  reduce = 0%, Cumulative CPU 131.77 sec
2023-01-05 08:17:41,619 Stage-28 map = 32%,  reduce = 0%, Cumulative CPU 135.84 sec
2023-01-05 08:17:44,070 Stage-29 map = 11%,  reduce = 0%, Cumulative CPU 73.75 sec
2023-01-05 08:17:45,678 Stage-28 map = 34%,  reduce = 0%, Cumulative CPU 142.28 sec
2023-01-05 08:17:47,704 Stage-28 map = 35%,  reduce = 0%, Cumulative CPU 144.23 sec
2023-01-05 08:17:48,119 Stage-29 map = 12%,  reduce = 0%, Cumulative CPU 75.49 sec
2023-01-05 08:17:48,316 Stage-17 map = 0%,  reduce = 0%
2023-01-05 08:17:48,722 Stage-28 map = 36%,  reduce = 0%, Cumulative CPU 146.08 sec
2023-01-05 08:17:49,131 Stage-29 map = 13%,  reduce = 0%, Cumulative CPU 77.33 sec
2023-01-05 08:17:50,146 Stage-29 map = 14%,  reduce = 0%, Cumulative CPU 81.12 sec
2023-01-05 08:17:51,779 Stage-28 map = 37%,  reduce = 0%, Cumulative CPU 149.93 sec
2023-01-05 08:17:53,808 Stage-28 map = 38%,  reduce = 0%, Cumulative CPU 152.16 sec
2023-01-05 08:17:54,828 Stage-28 map = 39%,  reduce = 0%, Cumulative CPU 153.97 sec
2023-01-05 08:17:55,316 Stage-29 map = 15%,  reduce = 0%, Cumulative CPU 84.56 sec
2023-01-05 08:17:56,331 Stage-29 map = 16%,  reduce = 0%, Cumulative CPU 88.54 sec
Hadoop job information for Stage-30: number of mappers: 4; number of reducers: 1
2023-01-05 08:17:56,463 Stage-30 map = 0%,  reduce = 0%
2023-01-05 08:17:57,877 Stage-28 map = 41%,  reduce = 0%, Cumulative CPU 159.11 sec
2023-01-05 08:17:59,930 Stage-28 map = 43%,  reduce = 0%, Cumulative CPU 162.9 sec
2023-01-05 08:18:02,454 Stage-29 map = 17%,  reduce = 0%, Cumulative CPU 95.88 sec
2023-01-05 08:18:04,390 Stage-28 map = 44%,  reduce = 0%, Cumulative CPU 167.18 sec
2023-01-05 08:18:06,521 Stage-28 map = 45%,  reduce = 0%, Cumulative CPU 170.38 sec
2023-01-05 08:18:07,579 Stage-29 map = 18%,  reduce = 0%, Cumulative CPU 101.65 sec
2023-01-05 08:18:09,722 Stage-28 map = 46%,  reduce = 0%, Cumulative CPU 173.84 sec
2023-01-05 08:18:10,805 Stage-28 map = 57%,  reduce = 0%, Cumulative CPU 175.56 sec
2023-01-05 08:18:11,638 Stage-29 map = 19%,  reduce = 0%, Cumulative CPU 105.72 sec
2023-01-05 08:18:12,789 Stage-30 map = 1%,  reduce = 0%, Cumulative CPU 10.04 sec
2023-01-05 08:18:12,914 Stage-28 map = 58%,  reduce = 0%, Cumulative CPU 177.39 sec
2023-01-05 08:18:14,677 Stage-29 map = 20%,  reduce = 0%, Cumulative CPU 112.01 sec
2023-01-05 08:18:15,843 Stage-30 map = 2%,  reduce = 0%, Cumulative CPU 21.1 sec
2023-01-05 08:18:18,316 Stage-28 map = 59%,  reduce = 0%, Cumulative CPU 182.75 sec
2023-01-05 08:18:18,882 Stage-30 map = 3%,  reduce = 0%, Cumulative CPU 22.91 sec
2023-01-05 08:18:20,751 Stage-29 map = 21%,  reduce = 0%, Cumulative CPU 118.9 sec
2023-01-05 08:18:22,604 Stage-28 map = 60%,  reduce = 0%, Cumulative CPU 186.71 sec
2023-01-05 08:18:24,802 Stage-29 map = 22%,  reduce = 0%, Cumulative CPU 123.65 sec
2023-01-05 08:18:24,979 Stage-30 map = 4%,  reduce = 0%, Cumulative CPU 38.38 sec
2023-01-05 08:18:27,114 Stage-28 map = 60%,  reduce = 8%, Cumulative CPU 188.94 sec
2023-01-05 08:18:28,016 Stage-30 map = 5%,  reduce = 0%, Cumulative CPU 40.29 sec
2023-01-05 08:18:28,128 Stage-28 map = 61%,  reduce = 8%, Cumulative CPU 191.72 sec
2023-01-05 08:18:30,189 Stage-28 map = 70%,  reduce = 8%, Cumulative CPU 193.81 sec
2023-01-05 08:18:30,874 Stage-29 map = 23%,  reduce = 0%, Cumulative CPU 130.52 sec
2023-01-05 08:18:33,256 Stage-28 map = 70%,  reduce = 17%, Cumulative CPU 193.85 sec
2023-01-05 08:18:34,141 Stage-30 map = 6%,  reduce = 0%, Cumulative CPU 57.72 sec
2023-01-05 08:18:34,273 Stage-28 map = 72%,  reduce = 17%, Cumulative CPU 197.94 sec
2023-01-05 08:18:35,995 Stage-29 map = 24%,  reduce = 0%, Cumulative CPU 135.59 sec
2023-01-05 08:18:37,199 Stage-30 map = 8%,  reduce = 0%, Cumulative CPU 65.92 sec
2023-01-05 08:18:38,020 Stage-29 map = 25%,  reduce = 0%, Cumulative CPU 140.23 sec
2023-01-05 08:18:40,634 Stage-28 map = 73%,  reduce = 17%, Cumulative CPU 201.95 sec
2023-01-05 08:18:42,270 Stage-30 map = 9%,  reduce = 0%, Cumulative CPU 69.53 sec
2023-01-05 08:18:43,295 Stage-30 map = 10%,  reduce = 0%, Cumulative CPU 72.42 sec
2023-01-05 08:18:44,097 Stage-29 map = 26%,  reduce = 0%, Cumulative CPU 145.93 sec
2023-01-05 08:18:45,934 Stage-28 map = 75%,  reduce = 17%, Cumulative CPU 206.05 sec
2023-01-05 08:18:46,347 Stage-30 map = 11%,  reduce = 0%, Cumulative CPU 73.59 sec
2023-01-05 08:18:48,156 Stage-29 map = 27%,  reduce = 0%, Cumulative CPU 147.89 sec
2023-01-05 08:18:48,372 Stage-30 map = 12%,  reduce = 0%, Cumulative CPU 77.57 sec
2023-01-05 08:18:48,930 Stage-17 map = 0%,  reduce = 0%
2023-01-05 08:18:50,216 Stage-29 map = 28%,  reduce = 0%, Cumulative CPU 152.98 sec
2023-01-05 08:18:52,315 Stage-28 map = 76%,  reduce = 17%, Cumulative CPU 211.32 sec
2023-01-05 08:18:52,438 Stage-30 map = 13%,  reduce = 0%, Cumulative CPU 80.24 sec
2023-01-05 08:18:54,266 Stage-29 map = 29%,  reduce = 0%, Cumulative CPU 154.69 sec
2023-01-05 08:18:54,473 Stage-30 map = 14%,  reduce = 0%, Cumulative CPU 86.06 sec
2023-01-05 08:18:56,292 Stage-29 map = 30%,  reduce = 0%, Cumulative CPU 159.8 sec
2023-01-05 08:18:58,533 Stage-30 map = 15%,  reduce = 0%, Cumulative CPU 87.02 sec
2023-01-05 08:18:58,633 Stage-28 map = 77%,  reduce = 17%, Cumulative CPU 215.6 sec
2023-01-05 08:18:59,341 Stage-29 map = 31%,  reduce = 0%, Cumulative CPU 161.89 sec
2023-01-05 08:19:00,581 Stage-30 map = 16%,  reduce = 0%, Cumulative CPU 93.87 sec
2023-01-05 08:19:02,378 Stage-29 map = 32%,  reduce = 0%, Cumulative CPU 164.44 sec
2023-01-05 08:19:03,389 Stage-29 map = 33%,  reduce = 0%, Cumulative CPU 165.93 sec
2023-01-05 08:19:04,631 Stage-30 map = 17%,  reduce = 0%, Cumulative CPU 94.97 sec
2023-01-05 08:19:04,748 Stage-28 map = 78%,  reduce = 17%, Cumulative CPU 220.52 sec
2023-01-05 08:19:06,662 Stage-30 map = 18%,  reduce = 0%, Cumulative CPU 101.94 sec
2023-01-05 08:19:07,456 Stage-29 map = 34%,  reduce = 0%, Cumulative CPU 168.52 sec
2023-01-05 08:19:08,468 Stage-29 map = 35%,  reduce = 0%, Cumulative CPU 170.33 sec
2023-01-05 08:19:09,858 Stage-28 map = 79%,  reduce = 17%, Cumulative CPU 224.51 sec
2023-01-05 08:19:11,513 Stage-29 map = 36%,  reduce = 0%, Cumulative CPU 173.05 sec
2023-01-05 08:19:11,753 Stage-30 map = 19%,  reduce = 0%, Cumulative CPU 105.55 sec
2023-01-05 08:19:14,555 Stage-29 map = 37%,  reduce = 0%, Cumulative CPU 176.49 sec
2023-01-05 08:19:15,566 Stage-29 map = 38%,  reduce = 0%, Cumulative CPU 177.64 sec
2023-01-05 08:19:15,821 Stage-30 map = 20%,  reduce = 0%, Cumulative CPU 111.39 sec
2023-01-05 08:19:16,181 Stage-28 map = 80%,  reduce = 17%, Cumulative CPU 228.08 sec
2023-01-05 08:19:17,849 Stage-30 map = 21%,  reduce = 0%, Cumulative CPU 113.42 sec
2023-01-05 08:19:19,634 Stage-29 map = 39%,  reduce = 0%, Cumulative CPU 180.12 sec
2023-01-05 08:19:20,647 Stage-29 map = 40%,  reduce = 0%, Cumulative CPU 181.28 sec
2023-01-05 08:19:21,931 Stage-30 map = 22%,  reduce = 0%, Cumulative CPU 118.72 sec
2023-01-05 08:19:22,291 Stage-28 map = 81%,  reduce = 17%, Cumulative CPU 231.8 sec
2023-01-05 08:19:23,700 Stage-29 map = 41%,  reduce = 0%, Cumulative CPU 184.59 sec
2023-01-05 08:19:24,966 Stage-30 map = 23%,  reduce = 0%, Cumulative CPU 126.08 sec
2023-01-05 08:19:26,743 Stage-29 map = 42%,  reduce = 0%, Cumulative CPU 188.99 sec
2023-01-05 08:19:27,998 Stage-30 map = 24%,  reduce = 0%, Cumulative CPU 127.69 sec
2023-01-05 08:19:28,407 Stage-28 map = 82%,  reduce = 17%, Cumulative CPU 235.62 sec
2023-01-05 08:19:29,787 Stage-29 map = 43%,  reduce = 0%, Cumulative CPU 190.08 sec
2023-01-05 08:19:30,021 Stage-30 map = 25%,  reduce = 0%, Cumulative CPU 130.24 sec
2023-01-05 08:19:31,039 Stage-30 map = 26%,  reduce = 0%, Cumulative CPU 134.77 sec
2023-01-05 08:19:32,606 Stage-28 map = 90%,  reduce = 17%, Cumulative CPU 236.86 sec
2023-01-05 08:19:32,839 Stage-29 map = 44%,  reduce = 0%, Cumulative CPU 195.23 sec
2023-01-05 08:19:33,632 Stage-28 map = 90%,  reduce = 25%, Cumulative CPU 236.89 sec
2023-01-05 08:19:34,645 Stage-28 map = 91%,  reduce = 25%, Cumulative CPU 241.47 sec
2023-01-05 08:19:35,882 Stage-29 map = 45%,  reduce = 0%, Cumulative CPU 196.62 sec
2023-01-05 08:19:36,108 Stage-30 map = 27%,  reduce = 0%, Cumulative CPU 137.82 sec
2023-01-05 08:19:36,892 Stage-29 map = 46%,  reduce = 0%, Cumulative CPU 198.0 sec
2023-01-05 08:19:37,121 Stage-30 map = 28%,  reduce = 0%, Cumulative CPU 142.85 sec
2023-01-05 08:19:38,912 Stage-29 map = 47%,  reduce = 0%, Cumulative CPU 202.63 sec
2023-01-05 08:19:42,179 Stage-30 map = 29%,  reduce = 0%, Cumulative CPU 146.44 sec
2023-01-05 08:19:42,978 Stage-29 map = 48%,  reduce = 0%, Cumulative CPU 205.11 sec
2023-01-05 08:19:43,195 Stage-30 map = 30%,  reduce = 0%, Cumulative CPU 150.51 sec
2023-01-05 08:19:43,992 Stage-29 map = 49%,  reduce = 0%, Cumulative CPU 206.38 sec
2023-01-05 08:19:46,855 Stage-28 map = 100%,  reduce = 25%, Cumulative CPU 246.37 sec
2023-01-05 08:19:47,872 Stage-28 map = 100%,  reduce = 100%, Cumulative CPU 248.29 sec
2023-01-05 08:19:48,056 Stage-29 map = 50%,  reduce = 0%, Cumulative CPU 209.6 sec
2023-01-05 08:19:48,266 Stage-30 map = 31%,  reduce = 0%, Cumulative CPU 154.67 sec
2023-01-05 08:19:49,078 Stage-29 map = 51%,  reduce = 0%, Cumulative CPU 210.88 sec
2023-01-05 08:19:49,123 Stage-17 map = 0%,  reduce = 0%
2023-01-05 08:19:49,278 Stage-30 map = 32%,  reduce = 0%, Cumulative CPU 159.52 sec
MapReduce Total cumulative CPU time: 4 minutes 8 seconds 290 msec
Ended Job = job_1672890466700_0089
2023-01-05 08:19:50,089 Stage-29 map = 52%,  reduce = 0%, Cumulative CPU 212.25 sec
2023-01-05 08:19:51,101 Stage-29 map = 63%,  reduce = 0%, Cumulative CPU 215.04 sec
2023-01-05 08:19:52,115 Stage-29 map = 72%,  reduce = 0%, Cumulative CPU 215.96 sec
2023-01-05 08:19:54,339 Stage-30 map = 33%,  reduce = 0%, Cumulative CPU 163.42 sec
2023-01-05 08:19:55,353 Stage-30 map = 34%,  reduce = 0%, Cumulative CPU 168.43 sec
2023-01-05 08:19:56,178 Stage-29 map = 73%,  reduce = 0%, Cumulative CPU 217.76 sec
2023-01-05 08:19:57,191 Stage-29 map = 74%,  reduce = 0%, Cumulative CPU 220.49 sec
2023-01-05 08:19:58,389 Stage-30 map = 35%,  reduce = 0%, Cumulative CPU 170.49 sec
2023-01-05 08:20:01,427 Stage-30 map = 37%,  reduce = 0%, Cumulative CPU 176.2 sec
2023-01-05 08:20:03,272 Stage-29 map = 75%,  reduce = 0%, Cumulative CPU 223.61 sec
2023-01-05 08:20:05,481 Stage-30 map = 38%,  reduce = 0%, Cumulative CPU 179.71 sec
2023-01-05 08:20:06,494 Stage-30 map = 39%,  reduce = 0%, Cumulative CPU 181.58 sec
2023-01-05 08:20:07,505 Stage-30 map = 40%,  reduce = 0%, Cumulative CPU 183.53 sec
2023-01-05 08:20:08,333 Stage-29 map = 76%,  reduce = 0%, Cumulative CPU 225.2 sec
2023-01-05 08:20:10,359 Stage-29 map = 76%,  reduce = 17%, Cumulative CPU 228.03 sec
2023-01-05 08:20:12,634 Stage-30 map = 42%,  reduce = 0%, Cumulative CPU 189.98 sec
2023-01-05 08:20:15,437 Stage-29 map = 77%,  reduce = 17%, Cumulative CPU 232.15 sec
2023-01-05 08:20:18,787 Stage-30 map = 44%,  reduce = 0%, Cumulative CPU 196.85 sec
2023-01-05 08:20:22,058 Stage-17 map = 1%,  reduce = 0%, Cumulative CPU 10.92 sec
2023-01-05 08:20:24,853 Stage-30 map = 45%,  reduce = 0%, Cumulative CPU 204.48 sec
2023-01-05 08:20:27,891 Stage-30 map = 46%,  reduce = 0%, Cumulative CPU 207.09 sec
2023-01-05 08:20:28,403 Stage-17 map = 2%,  reduce = 0%, Cumulative CPU 23.26 sec
2023-01-05 08:20:28,902 Stage-30 map = 55%,  reduce = 0%, Cumulative CPU 208.05 sec
2023-01-05 08:20:30,440 Stage-17 map = 3%,  reduce = 0%, Cumulative CPU 25.15 sec
2023-01-05 08:20:30,924 Stage-30 map = 56%,  reduce = 0%, Cumulative CPU 211.17 sec
2023-01-05 08:20:33,659 Stage-29 map = 78%,  reduce = 17%, Cumulative CPU 242.07 sec
2023-01-05 08:20:34,781 Stage-17 map = 4%,  reduce = 0%, Cumulative CPU 38.67 sec
2023-01-05 08:20:36,991 Stage-30 map = 57%,  reduce = 0%, Cumulative CPU 215.69 sec
2023-01-05 08:20:39,064 Stage-17 map = 5%,  reduce = 0%, Cumulative CPU 55.04 sec
2023-01-05 08:20:42,132 Stage-17 map = 6%,  reduce = 0%, Cumulative CPU 57.86 sec
2023-01-05 08:20:43,064 Stage-30 map = 58%,  reduce = 0%, Cumulative CPU 219.84 sec
2023-01-05 08:20:44,793 Stage-29 map = 79%,  reduce = 17%, Cumulative CPU 247.39 sec
2023-01-05 08:20:45,092 Stage-30 map = 58%,  reduce = 8%, Cumulative CPU 220.21 sec
2023-01-05 08:20:45,179 Stage-17 map = 7%,  reduce = 0%, Cumulative CPU 59.52 sec
2023-01-05 08:20:46,201 Stage-17 map = 8%,  reduce = 0%, Cumulative CPU 65.93 sec
2023-01-05 08:20:48,125 Stage-30 map = 59%,  reduce = 8%, Cumulative CPU 223.33 sec
2023-01-05 08:20:48,440 Stage-17 map = 9%,  reduce = 0%, Cumulative CPU 67.26 sec
2023-01-05 08:20:51,509 Stage-17 map = 10%,  reduce = 0%, Cumulative CPU 70.22 sec
2023-01-05 08:20:51,937 Stage-29 map = 80%,  reduce = 17%, Cumulative CPU 250.88 sec
2023-01-05 08:20:52,186 Stage-30 map = 60%,  reduce = 8%, Cumulative CPU 225.66 sec
2023-01-05 08:20:54,212 Stage-30 map = 61%,  reduce = 8%, Cumulative CPU 226.7 sec
2023-01-05 08:20:54,572 Stage-17 map = 11%,  reduce = 0%, Cumulative CPU 72.75 sec
2023-01-05 08:20:56,604 Stage-17 map = 12%,  reduce = 0%, Cumulative CPU 73.99 sec
2023-01-05 08:20:58,258 Stage-30 map = 62%,  reduce = 8%, Cumulative CPU 229.04 sec
2023-01-05 08:20:58,631 Stage-17 map = 13%,  reduce = 0%, Cumulative CPU 77.19 sec
2023-01-05 08:21:01,294 Stage-30 map = 63%,  reduce = 8%, Cumulative CPU 231.55 sec
2023-01-05 08:21:03,077 Stage-29 map = 81%,  reduce = 17%, Cumulative CPU 257.08 sec
2023-01-05 08:21:03,735 Stage-17 map = 14%,  reduce = 0%, Cumulative CPU 81.68 sec
2023-01-05 08:21:06,182 Stage-17 map = 15%,  reduce = 0%, Cumulative CPU 84.14 sec
2023-01-05 08:21:06,362 Stage-30 map = 64%,  reduce = 8%, Cumulative CPU 234.7 sec
2023-01-05 08:21:07,372 Stage-30 map = 65%,  reduce = 8%, Cumulative CPU 236.93 sec
2023-01-05 08:21:09,147 Stage-29 map = 82%,  reduce = 17%, Cumulative CPU 262.03 sec
2023-01-05 08:21:10,669 Stage-17 map = 16%,  reduce = 0%, Cumulative CPU 89.11 sec
2023-01-05 08:21:12,437 Stage-30 map = 66%,  reduce = 8%, Cumulative CPU 239.73 sec
2023-01-05 08:21:12,746 Stage-17 map = 17%,  reduce = 0%, Cumulative CPU 90.31 sec
2023-01-05 08:21:15,232 Stage-29 map = 83%,  reduce = 17%, Cumulative CPU 265.06 sec
2023-01-05 08:21:16,480 Stage-30 map = 67%,  reduce = 8%, Cumulative CPU 241.99 sec
2023-01-05 08:21:16,806 Stage-17 map = 18%,  reduce = 0%, Cumulative CPU 94.93 sec
2023-01-05 08:21:18,504 Stage-30 map = 76%,  reduce = 8%, Cumulative CPU 244.17 sec
2023-01-05 08:21:19,284 Stage-29 map = 91%,  reduce = 17%, Cumulative CPU 266.12 sec
2023-01-05 08:21:19,522 Stage-30 map = 77%,  reduce = 8%, Cumulative CPU 245.77 sec
2023-01-05 08:21:20,888 Stage-17 map = 19%,  reduce = 0%, Cumulative CPU 97.53 sec
2023-01-05 08:21:21,561 Stage-30 map = 77%,  reduce = 17%, Cumulative CPU 245.83 sec
2023-01-05 08:21:23,339 Stage-29 map = 91%,  reduce = 25%, Cumulative CPU 267.73 sec
2023-01-05 08:21:25,159 Stage-17 map = 20%,  reduce = 0%, Cumulative CPU 102.94 sec
2023-01-05 08:21:25,368 Stage-29 map = 100%,  reduce = 25%, Cumulative CPU 268.6 sec
2023-01-05 08:21:25,621 Stage-30 map = 78%,  reduce = 17%, Cumulative CPU 249.89 sec
2023-01-05 08:21:27,394 Stage-29 map = 100%,  reduce = 100%, Cumulative CPU 271.58 sec
2023-01-05 08:21:28,200 Stage-17 map = 21%,  reduce = 0%, Cumulative CPU 109.17 sec
2023-01-05 08:21:28,652 Stage-30 map = 89%,  reduce = 17%, Cumulative CPU 250.73 sec
MapReduce Total cumulative CPU time: 4 minutes 31 seconds 580 msec
Ended Job = job_1672890466700_0090
2023-01-05 08:21:29,663 Stage-30 map = 90%,  reduce = 17%, Cumulative CPU 253.28 sec
2023-01-05 08:21:30,256 Stage-17 map = 22%,  reduce = 0%, Cumulative CPU 110.72 sec
2023-01-05 08:21:33,300 Stage-17 map = 23%,  reduce = 0%, Cumulative CPU 113.53 sec
2023-01-05 08:21:33,707 Stage-30 map = 90%,  reduce = 25%, Cumulative CPU 253.32 sec
2023-01-05 08:21:34,314 Stage-17 map = 24%,  reduce = 0%, Cumulative CPU 118.57 sec
2023-01-05 08:21:36,368 Stage-17 map = 25%,  reduce = 0%, Cumulative CPU 120.19 sec
2023-01-05 08:21:39,408 Stage-17 map = 26%,  reduce = 0%, Cumulative CPU 122.89 sec
2023-01-05 08:21:42,454 Stage-17 map = 27%,  reduce = 0%, Cumulative CPU 130.13 sec
2023-01-05 08:21:45,502 Stage-17 map = 28%,  reduce = 0%, Cumulative CPU 135.31 sec
Hadoop job information for Stage-59: number of mappers: 1; number of reducers: 0
2023-01-05 08:21:47,843 Stage-59 map = 0%,  reduce = 0%
2023-01-05 08:21:47,892 Stage-30 map = 91%,  reduce = 25%, Cumulative CPU 260.53 sec
2023-01-05 08:21:48,542 Stage-17 map = 29%,  reduce = 0%, Cumulative CPU 141.59 sec
2023-01-05 08:21:51,585 Stage-17 map = 30%,  reduce = 0%, Cumulative CPU 145.03 sec
2023-01-05 08:21:56,000 Stage-59 map = 100%,  reduce = 0%, Cumulative CPU 3.65 sec
2023-01-05 08:21:56,690 Stage-17 map = 31%,  reduce = 0%, Cumulative CPU 151.08 sec
2023-01-05 08:21:58,725 Stage-17 map = 32%,  reduce = 0%, Cumulative CPU 155.86 sec
MapReduce Total cumulative CPU time: 3 seconds 650 msec
Ended Job = job_1672890466700_0092
2023-01-05 08:22:00,042 Stage-30 map = 100%,  reduce = 25%, Cumulative CPU 265.48 sec
2023-01-05 08:22:02,076 Stage-30 map = 100%,  reduce = 100%, Cumulative CPU 268.36 sec
2023-01-05 08:22:02,782 Stage-17 map = 33%,  reduce = 0%, Cumulative CPU 161.34 sec
MapReduce Total cumulative CPU time: 4 minutes 28 seconds 360 msec
Ended Job = job_1672890466700_0091
2023-01-05 08:22:03,795 Stage-17 map = 34%,  reduce = 0%, Cumulative CPU 163.33 sec
2023-01-05 08:22:06,835 Stage-17 map = 35%,  reduce = 0%, Cumulative CPU 168.58 sec
2023-01-05 08:22:08,873 Stage-17 map = 36%,  reduce = 0%, Cumulative CPU 170.86 sec
2023-01-05 08:22:09,886 Stage-17 map = 37%,  reduce = 0%, Cumulative CPU 173.18 sec
2023-01-05 08:22:12,930 Stage-17 map = 38%,  reduce = 0%, Cumulative CPU 177.21 sec
2023-01-05 08:22:14,960 Stage-17 map = 39%,  reduce = 0%, Cumulative CPU 179.41 sec
2023-01-05 08:22:15,972 Stage-17 map = 41%,  reduce = 0%, Cumulative CPU 183.69 sec
2023-01-05 08:22:19,010 Stage-17 map = 42%,  reduce = 0%, Cumulative CPU 185.92 sec
2023-01-05 08:22:21,041 Stage-17 map = 43%,  reduce = 0%, Cumulative CPU 188.04 sec
2023-01-05 08:22:22,056 Stage-17 map = 45%,  reduce = 0%, Cumulative CPU 191.68 sec
2023-01-05 08:22:27,124 Stage-17 map = 46%,  reduce = 0%, Cumulative CPU 196.2 sec
2023-01-05 08:22:28,142 Stage-17 map = 48%,  reduce = 0%, Cumulative CPU 200.09 sec
2023-01-05 08:22:30,374 Stage-17 map = 49%,  reduce = 0%, Cumulative CPU 201.93 sec
2023-01-05 08:22:33,632 Stage-17 map = 51%,  reduce = 0%, Cumulative CPU 204.45 sec
2023-01-05 08:22:34,649 Stage-17 map = 52%,  reduce = 0%, Cumulative CPU 206.45 sec
2023-01-05 08:22:36,680 Stage-17 map = 62%,  reduce = 0%, Cumulative CPU 208.84 sec
2023-01-05 08:22:38,704 Stage-17 map = 63%,  reduce = 0%, Cumulative CPU 210.04 sec
2023-01-05 08:22:40,735 Stage-17 map = 64%,  reduce = 0%, Cumulative CPU 211.97 sec
2023-01-05 08:22:44,792 Stage-17 map = 65%,  reduce = 0%, Cumulative CPU 212.91 sec
2023-01-05 08:22:45,804 Stage-17 map = 66%,  reduce = 0%, Cumulative CPU 214.23 sec
2023-01-05 08:22:47,829 Stage-17 map = 75%,  reduce = 0%, Cumulative CPU 215.36 sec
2023-01-05 08:22:50,866 Stage-17 map = 76%,  reduce = 0%, Cumulative CPU 216.17 sec
2023-01-05 08:22:54,920 Stage-17 map = 76%,  reduce = 17%, Cumulative CPU 219.08 sec
2023-01-05 08:22:56,945 Stage-17 map = 77%,  reduce = 17%, Cumulative CPU 220.24 sec
2023-01-05 08:22:57,957 Stage-17 map = 78%,  reduce = 17%, Cumulative CPU 223.64 sec
2023-01-05 08:23:03,027 Stage-17 map = 79%,  reduce = 17%, Cumulative CPU 224.76 sec
2023-01-05 08:23:04,041 Stage-17 map = 80%,  reduce = 17%, Cumulative CPU 227.9 sec
2023-01-05 08:23:05,056 Stage-17 map = 90%,  reduce = 17%, Cumulative CPU 228.33 sec
2023-01-05 08:23:07,081 Stage-17 map = 90%,  reduce = 25%, Cumulative CPU 228.35 sec
2023-01-05 08:23:10,116 Stage-17 map = 91%,  reduce = 25%, Cumulative CPU 231.27 sec
2023-01-05 08:23:13,152 Stage-17 map = 100%,  reduce = 25%, Cumulative CPU 233.42 sec
2023-01-05 08:23:14,170 Stage-17 map = 100%,  reduce = 100%, Cumulative CPU 235.37 sec
MapReduce Total cumulative CPU time: 3 minutes 55 seconds 370 msec
Ended Job = job_1672890466700_0084
Stage-74 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.


2023-01-05 08:23:24	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10059/HashTable-Stage-57/MapJoin-mapfile131--.hashtable2023-01-05 08:23:24	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10059/HashTable-Stage-57/MapJoin-mapfile131--.hashtable (289 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 19 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0093, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0093/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0093
Hadoop job information for Stage-57: number of mappers: 1; number of reducers: 0
2023-01-05 08:23:34,069 Stage-57 map = 0%,  reduce = 0%
2023-01-05 08:23:39,182 Stage-57 map = 100%,  reduce = 0%, Cumulative CPU 0.89 sec
MapReduce Total cumulative CPU time: 890 msec
Ended Job = job_1672890466700_0093
Stage-73 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 08:23:51	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 08:23:51	End of local task; Time Taken: 0.41 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 21 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0094, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0094/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0094
Hadoop job information for Stage-55: number of mappers: 1; number of reducers: 0
2023-01-05 08:24:02,358 Stage-55 map = 0%,  reduce = 0%
2023-01-05 08:24:07,429 Stage-55 map = 100%,  reduce = 0%, Cumulative CPU 0.64 sec
MapReduce Total cumulative CPU time: 640 msec
Ended Job = job_1672890466700_0094
Stage-72 is selected by condition resolver.
Stage-4 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 08:24:18	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 08:24:18	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10055/HashTable-Stage-53/MapJoin-mapfile111--.hashtable
2023-01-05 08:24:18	End of local task; Time Taken: 0.431 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 23 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0095, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0095/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0095
Hadoop job information for Stage-53: number of mappers: 1; number of reducers: 0
2023-01-05 08:24:28,332 Stage-53 map = 0%,  reduce = 0%
2023-01-05 08:24:33,419 Stage-53 map = 100%,  reduce = 0%, Cumulative CPU 0.83 sec
MapReduce Total cumulative CPU time: 830 msec
Ended Job = job_1672890466700_0095
Stage-71 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 08:24:44	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 08:24:45	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10053/HashTable-Stage-51/MapJoin-mapfile101--.hashtable (290 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 25 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0096, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0096/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0096
Hadoop job information for Stage-51: number of mappers: 1; number of reducers: 0
2023-01-05 08:24:53,521 Stage-51 map = 0%,  reduce = 0%
2023-01-05 08:24:58,626 Stage-51 map = 100%,  reduce = 0%, Cumulative CPU 0.91 sec
MapReduce Total cumulative CPU time: 910 msec
Ended Job = job_1672890466700_0096
Stage-70 is selected by condition resolver.
Stage-6 is filtered out by condition resolver.

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2023-01-05 08:25:08	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10051/HashTable-Stage-49/MapJoin-mapfile91--.hashtable
2023-01-05 08:25:08	End of local task; Time Taken: 0.413 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 27 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0097, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0097/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0097
Hadoop job information for Stage-49: number of mappers: 1; number of reducers: 0
2023-01-05 08:25:18,459 Stage-49 map = 0%,  reduce = 0%
2023-01-05 08:25:22,562 Stage-49 map = 100%,  reduce = 0%, Cumulative CPU 0.85 sec
MapReduce Total cumulative CPU time: 850 msec
Ended Job = job_1672890466700_0097
Stage-69 is selected by condition resolver.
Stage-7 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:25:32	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10049/HashTable-Stage-47/MapJoin-mapfile81--.hashtable2023-01-05 08:25:32	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10049/HashTable-Stage-47/MapJoin-mapfile81--.hashtable (281 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 29 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0098, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0098/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0098
Hadoop job information for Stage-47: number of mappers: 1; number of reducers: 0
2023-01-05 08:25:40,680 Stage-47 map = 0%,  reduce = 0%
2023-01-05 08:25:43,736 Stage-47 map = 100%,  reduce = 0%, Cumulative CPU 0.62 sec
MapReduce Total cumulative CPU time: 620 msec
Ended Job = job_1672890466700_0098
Stage-68 is selected by condition resolver.
Stage-8 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 08:25:54	Starting to launch local task to process map join;	maximum memory = 239075328

Execution completed successfully
MapredLocal task succeeded
Launching Job 31 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0099, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0099/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0099
Hadoop job information for Stage-45: number of mappers: 1; number of reducers: 0
2023-01-05 08:26:04,580 Stage-45 map = 0%,  reduce = 0%
2023-01-05 08:26:09,655 Stage-45 map = 100%,  reduce = 0%, Cumulative CPU 0.61 sec
MapReduce Total cumulative CPU time: 610 msec
Ended Job = job_1672890466700_0099
Stage-67 is selected by condition resolver.
Stage-9 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:26:21	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10045/HashTable-Stage-43/MapJoin-mapfile61--.hashtable2023-01-05 08:26:21	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10045/HashTable-Stage-43/MapJoin-mapfile61--.hashtable (290 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 33 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0100, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0100/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0100
Hadoop job information for Stage-43: number of mappers: 1; number of reducers: 0
2023-01-05 08:26:30,464 Stage-43 map = 0%,  reduce = 0%
2023-01-05 08:26:34,532 Stage-43 map = 100%,  reduce = 0%, Cumulative CPU 0.85 sec
MapReduce Total cumulative CPU time: 850 msec
Ended Job = job_1672890466700_0100
Stage-66 is selected by condition resolver.
Stage-10 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2023-01-05 08:26:44	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 08:26:44	End of local task; Time Taken: 0.411 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 35 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0101, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0101/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0101
Hadoop job information for Stage-41: number of mappers: 1; number of reducers: 0
2023-01-05 08:26:53,943 Stage-41 map = 0%,  reduce = 0%
2023-01-05 08:26:59,080 Stage-41 map = 100%,  reduce = 0%, Cumulative CPU 0.84 sec
MapReduce Total cumulative CPU time: 840 msec
Ended Job = job_1672890466700_0101
Stage-65 is selected by condition resolver.
Stage-11 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 08:27:08	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 08:27:08	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10041/HashTable-Stage-39/MapJoin-mapfile41--.hashtable (290 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 37 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0102, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0102/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0102
Hadoop job information for Stage-39: number of mappers: 1; number of reducers: 0
2023-01-05 08:27:18,374 Stage-39 map = 0%,  reduce = 0%
2023-01-05 08:27:22,443 Stage-39 map = 100%,  reduce = 0%, Cumulative CPU 0.88 sec
MapReduce Total cumulative CPU time: 880 msec
Ended Job = job_1672890466700_0102
Stage-64 is selected by condition resolver.
Stage-12 is filtered out by condition resolver.

SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:27:32	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10039/HashTable-Stage-37/MapJoin-mapfile31--.hashtable2023-01-05 08:27:32	End of local task; Time Taken: 0.404 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 39 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0103, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0103/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0103
Hadoop job information for Stage-37: number of mappers: 1; number of reducers: 0
2023-01-05 08:27:42,220 Stage-37 map = 0%,  reduce = 0%
2023-01-05 08:27:46,296 Stage-37 map = 100%,  reduce = 0%, Cumulative CPU 0.81 sec
MapReduce Total cumulative CPU time: 810 msec
Ended Job = job_1672890466700_0103
Stage-63 is selected by condition resolver.
Stage-13 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:27:56	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10037/HashTable-Stage-35/MapJoin-mapfile21--.hashtable
2023-01-05 08:27:56	End of local task; Time Taken: 0.398 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 41 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0104, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0104/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0104
Hadoop job information for Stage-35: number of mappers: 1; number of reducers: 0
2023-01-05 08:28:07,550 Stage-35 map = 0%,  reduce = 0%
2023-01-05 08:28:12,619 Stage-35 map = 100%,  reduce = 0%, Cumulative CPU 0.62 sec
MapReduce Total cumulative CPU time: 620 msec
Ended Job = job_1672890466700_0104
Stage-62 is selected by condition resolver.
Stage-14 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2023-01-05 08:28:22	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10035/HashTable-Stage-33/MapJoin-mapfile11--.hashtable2023-01-05 08:28:22	Uploaded 1 File to: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10035/HashTable-Stage-33/MapJoin-mapfile11--.hashtable (290 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 43 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0105, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0105/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0105
Hadoop job information for Stage-33: number of mappers: 1; number of reducers: 0
2023-01-05 08:28:32,311 Stage-33 map = 0%,  reduce = 0%
2023-01-05 08:28:36,381 Stage-33 map = 100%,  reduce = 0%, Cumulative CPU 0.88 sec
MapReduce Total cumulative CPU time: 880 msec
Ended Job = job_1672890466700_0105
Stage-61 is selected by condition resolver.
Stage-15 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:28:46	Dump the side-table for tag: 1 with group count: 1 into file: file:/tmp/hdfs/94f9a396-7091-49d0-9785-01f98008ae12/hive_2023-01-05_07-49-41_686_3724535917905133996-1/-local-10033/HashTable-Stage-31/MapJoin-mapfile01--.hashtable2023-01-05 08:28:46	End of local task; Time Taken: 0.456 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 45 out of 45
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0106, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0106/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0106
Hadoop job information for Stage-31: number of mappers: 1; number of reducers: 0
2023-01-05 08:28:56,751 Stage-31 map = 0%,  reduce = 0%
2023-01-05 08:29:02,857 Stage-31 map = 100%,  reduce = 0%, Cumulative CPU 0.88 sec
MapReduce Total cumulative CPU time: 880 msec
Ended Job = job_1672890466700_0106
MapReduce Jobs Launched:
Stage-Stage-16: Map: 4  Reduce: 1   Cumulative CPU: 240.24 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-21: Map: 4  Reduce: 1   Cumulative CPU: 235.97 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 4  Reduce: 1   Cumulative CPU: 252.71 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-20: Map: 4  Reduce: 1   Cumulative CPU: 271.24 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-23: Map: 4  Reduce: 1   Cumulative CPU: 253.57 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-18: Map: 4  Reduce: 1   Cumulative CPU: 257.74 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-22: Map: 4  Reduce: 1   Cumulative CPU: 261.07 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-24: Map: 4  Reduce: 1   Cumulative CPU: 269.44 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-26: Map: 4  Reduce: 1   Cumulative CPU: 210.29 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-25: Map: 4  Reduce: 1   Cumulative CPU: 256.61 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-27: Map: 4  Reduce: 1   Cumulative CPU: 220.87 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-28: Map: 4  Reduce: 1   Cumulative CPU: 248.29 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-29: Map: 4  Reduce: 1   Cumulative CPU: 271.58 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-59: Map: 1   Cumulative CPU: 3.65 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-30: Map: 4  Reduce: 1   Cumulative CPU: 268.36 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-17: Map: 4  Reduce: 1   Cumulative CPU: 235.37 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-57: Map: 1   Cumulative CPU: 0.89 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-55: Map: 1   Cumulative CPU: 0.64 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-53: Map: 1   Cumulative CPU: 0.83 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-51: Map: 1   Cumulative CPU: 0.91 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-49: Map: 1   Cumulative CPU: 0.85 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-47: Map: 1   Cumulative CPU: 0.62 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-45: Map: 1   Cumulative CPU: 0.61 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-43: Map: 1   Cumulative CPU: 0.85 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-41: Map: 1   Cumulative CPU: 0.84 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-39: Map: 1   Cumulative CPU: 0.88 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-37: Map: 1   Cumulative CPU: 0.81 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-35: Map: 1   Cumulative CPU: 0.62 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-33: Map: 1   Cumulative CPU: 0.88 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-31: Map: 1   Cumulative CPU: 0.88 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 days 1 hours 2 minutes 48 seconds 110 msec
OK
-172.3623168427947695877775	-501.3884857406149514127073	-828.0319617880655061734180	-1156.3508311282995753128234	-1485.5186738585807556315828
Time taken: 2363.514 seconds, Fetched: 1 row(s)
timediff:2369.235232263
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query10.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = df4adf65-b14b-4182-81bf-ba6bd599b9ac

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = c356d844-6736-4a76-af38-60e02e9ba4f3
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_current_addr_sk, c_customer_sk, c_current_cdemo_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_county, ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_demographics, Columns: cd_dep_employed_count, cd_demo_sk, cd_education_status, cd_credit_rating, cd_dep_count, cd_purchase_estimate, cd_marital_status, cd_dep_college_count, cd_gender
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_moy, d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_bill_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_ship_customer_sk
Query ID = hdfs_20230105082911_9cdfe3d6-9d2e-4302-9686-2368faecfdb9
Total jobs = 8


SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2023-01-05 08:29:51	Starting to launch local task to process map join;	maximum memory = 239075328






Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 8
Execution completed successfully
MapredLocal task succeeded
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 2 out of 8
Launching Job 3 out of 8
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:30:20	Processing rows:	300000	Hashtable size:	299999	Memory usage:	241745424	percentage:	1.011

Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-24

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
timediff:82.749310067
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query11.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = c9914d01-fec7-4142-a725-13aa70e2b2e6

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = a3732411-c893-42f2-8898-8a1ac950cdc0
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_preferred_cust_flag, c_customer_sk, c_login, c_last_name, c_customer_id, c_birth_country, c_first_name, c_email_address
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk, ss_ext_discount_amt, ss_ext_list_price
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_ext_discount_amt, ws_bill_customer_sk, ws_ext_list_price
Query ID = hdfs_20230105083033_23cf5e12-dcdb-4b43-b123-d39485d34fc6
Total jobs = 26

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.






SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Execution completed successfully
MapredLocal task succeeded
2023-01-05 08:31:17	End of local task; Time Taken: 6.007 sec.
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 26
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 2 out of 26
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 3 out of 26
Number of reduce tasks is set to 0 since there's no reduce operator
Launching Job 4 out of 26
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0113, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0113/
Starting Job = job_1672890466700_0112, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0112/
Starting Job = job_1672890466700_0110, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0110/
Starting Job = job_1672890466700_0111, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0111/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0110
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0112
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0113
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0111
Hadoop job information for Stage-39: number of mappers: 4; number of reducers: 0
Hadoop job information for Stage-35: number of mappers: 4; number of reducers: 0
2023-01-05 08:31:42,801 Stage-35 map = 0%,  reduce = 0%
2023-01-05 08:31:42,801 Stage-39 map = 0%,  reduce = 0%
Hadoop job information for Stage-43: number of mappers: 2; number of reducers: 0
2023-01-05 08:31:43,758 Stage-43 map = 0%,  reduce = 0%
2023-01-05 08:31:59,613 Stage-39 map = 3%,  reduce = 0%, Cumulative CPU 17.91 sec
2023-01-05 08:32:04,523 Stage-35 map = 2%,  reduce = 0%, Cumulative CPU 17.34 sec
2023-01-05 08:32:05,840 Stage-39 map = 5%,  reduce = 0%, Cumulative CPU 20.86 sec
2023-01-05 08:32:09,738 Stage-43 map = 2%,  reduce = 0%, Cumulative CPU 20.05 sec
2023-01-05 08:32:10,690 Stage-35 map = 4%,  reduce = 0%, Cumulative CPU 36.1 sec
2023-01-05 08:32:11,980 Stage-39 map = 7%,  reduce = 0%, Cumulative CPU 39.91 sec
2023-01-05 08:32:12,733 Stage-35 map = 5%,  reduce = 0%, Cumulative CPU 41.26 sec
2023-01-05 08:32:15,964 Stage-43 map = 3%,  reduce = 0%, Cumulative CPU 26.06 sec
2023-01-05 08:32:16,040 Stage-39 map = 8%,  reduce = 0%, Cumulative CPU 45.9 sec
2023-01-05 08:32:16,836 Stage-35 map = 7%,  reduce = 0%, Cumulative CPU 43.73 sec
2023-01-05 08:32:18,102 Stage-39 map = 10%,  reduce = 0%, Cumulative CPU 48.15 sec
2023-01-05 08:32:19,034 Stage-43 map = 4%,  reduce = 0%, Cumulative CPU 30.69 sec
2023-01-05 08:32:21,093 Stage-43 map = 5%,  reduce = 0%, Cumulative CPU 33.13 sec
2023-01-05 08:32:22,185 Stage-39 map = 11%,  reduce = 0%, Cumulative CPU 52.67 sec
2023-01-05 08:32:23,167 Stage-35 map = 9%,  reduce = 0%, Cumulative CPU 51.24 sec
2023-01-05 08:32:23,205 Stage-39 map = 12%,  reduce = 0%, Cumulative CPU 54.9 sec
2023-01-05 08:32:24,164 Stage-43 map = 6%,  reduce = 0%, Cumulative CPU 37.93 sec
2023-01-05 08:32:24,194 Stage-35 map = 10%,  reduce = 0%, Cumulative CPU 53.76 sec
2023-01-05 08:32:27,256 Stage-43 map = 7%,  reduce = 0%, Cumulative CPU 40.4 sec
2023-01-05 08:32:28,331 Stage-39 map = 13%,  reduce = 0%, Cumulative CPU 58.84 sec
2023-01-05 08:32:29,303 Stage-35 map = 12%,  reduce = 0%, Cumulative CPU 58.53 sec
2023-01-05 08:32:29,352 Stage-39 map = 15%,  reduce = 0%, Cumulative CPU 61.88 sec
2023-01-05 08:32:30,600 Stage-43 map = 8%,  reduce = 0%, Cumulative CPU 44.84 sec
2023-01-05 08:32:31,338 Stage-35 map = 13%,  reduce = 0%, Cumulative CPU 63.41 sec
2023-01-05 08:32:33,712 Stage-43 map = 9%,  reduce = 0%, Cumulative CPU 46.81 sec
2023-01-05 08:32:34,437 Stage-39 map = 16%,  reduce = 0%, Cumulative CPU 65.87 sec
2023-01-05 08:32:35,405 Stage-35 map = 14%,  reduce = 0%, Cumulative CPU 66.18 sec
2023-01-05 08:32:35,450 Stage-39 map = 17%,  reduce = 0%, Cumulative CPU 69.17 sec
2023-01-05 08:32:36,419 Stage-35 map = 15%,  reduce = 0%, Cumulative CPU 70.05 sec
2023-01-05 08:32:36,767 Stage-43 map = 10%,  reduce = 0%, Cumulative CPU 50.87 sec
2023-01-05 08:32:39,532 Stage-39 map = 18%,  reduce = 0%, Cumulative CPU 71.37 sec
2023-01-05 08:32:39,832 Stage-43 map = 11%,  reduce = 0%, Cumulative CPU 52.57 sec
2023-01-05 08:32:40,493 Stage-35 map = 16%,  reduce = 0%, Cumulative CPU 71.97 sec
2023-01-05 08:32:40,545 Stage-39 map = 19%,  reduce = 0%, Cumulative CPU 72.95 sec
2023-01-05 08:32:41,508 Stage-35 map = 17%,  reduce = 0%, Cumulative CPU 73.37 sec
2023-01-05 08:32:41,557 Stage-39 map = 20%,  reduce = 0%, Cumulative CPU 76.22 sec
2023-01-05 08:32:42,527 Stage-35 map = 18%,  reduce = 0%, Cumulative CPU 77.71 sec
2023-01-05 08:32:42,904 Stage-43 map = 12%,  reduce = 0%, Cumulative CPU 57.57 sec
2023-01-05 08:32:45,627 Stage-39 map = 21%,  reduce = 0%, Cumulative CPU 78.39 sec
2023-01-05 08:32:46,167 Stage-43 map = 13%,  reduce = 0%, Cumulative CPU 59.38 sec
2023-01-05 08:32:46,606 Stage-35 map = 19%,  reduce = 0%, Cumulative CPU 79.57 sec
2023-01-05 08:32:47,620 Stage-35 map = 20%,  reduce = 0%, Cumulative CPU 80.69 sec
2023-01-05 08:32:47,661 Stage-39 map = 22%,  reduce = 0%, Cumulative CPU 82.42 sec
2023-01-05 08:32:48,206 Stage-43 map = 14%,  reduce = 0%, Cumulative CPU 63.89 sec
2023-01-05 08:32:48,645 Stage-35 map = 21%,  reduce = 0%, Cumulative CPU 84.8 sec
2023-01-05 08:32:51,264 Stage-43 map = 15%,  reduce = 0%, Cumulative CPU 65.63 sec
2023-01-05 08:32:51,737 Stage-39 map = 23%,  reduce = 0%, Cumulative CPU 84.45 sec
2023-01-05 08:32:52,730 Stage-35 map = 23%,  reduce = 0%, Cumulative CPU 87.71 sec
2023-01-05 08:32:53,766 Stage-39 map = 24%,  reduce = 0%, Cumulative CPU 88.62 sec
2023-01-05 08:32:54,327 Stage-43 map = 16%,  reduce = 0%, Cumulative CPU 68.08 sec
2023-01-05 08:32:54,768 Stage-35 map = 24%,  reduce = 0%, Cumulative CPU 90.92 sec
2023-01-05 08:32:57,380 Stage-43 map = 17%,  reduce = 0%, Cumulative CPU 69.59 sec
2023-01-05 08:32:57,855 Stage-39 map = 25%,  reduce = 0%, Cumulative CPU 90.33 sec
2023-01-05 08:32:58,879 Stage-35 map = 25%,  reduce = 0%, Cumulative CPU 94.04 sec
2023-01-05 08:32:59,901 Stage-39 map = 27%,  reduce = 0%, Cumulative CPU 94.85 sec
2023-01-05 08:33:00,434 Stage-43 map = 18%,  reduce = 0%, Cumulative CPU 71.63 sec
2023-01-05 08:33:00,932 Stage-35 map = 26%,  reduce = 0%, Cumulative CPU 97.4 sec
2023-01-05 08:33:03,491 Stage-43 map = 19%,  reduce = 0%, Cumulative CPU 73.86 sec
2023-01-05 08:33:03,979 Stage-39 map = 28%,  reduce = 0%, Cumulative CPU 98.09 sec
2023-01-05 08:33:05,011 Stage-35 map = 27%,  reduce = 0%, Cumulative CPU 101.03 sec
2023-01-05 08:33:06,020 Stage-39 map = 29%,  reduce = 0%, Cumulative CPU 101.52 sec
2023-01-05 08:33:06,544 Stage-43 map = 20%,  reduce = 0%, Cumulative CPU 76.19 sec
2023-01-05 08:33:07,071 Stage-35 map = 29%,  reduce = 0%, Cumulative CPU 104.45 sec
2023-01-05 08:33:09,825 Stage-43 map = 22%,  reduce = 0%, Cumulative CPU 77.77 sec
2023-01-05 08:33:10,092 Stage-39 map = 30%,  reduce = 0%, Cumulative CPU 104.85 sec
2023-01-05 08:33:11,136 Stage-35 map = 30%,  reduce = 0%, Cumulative CPU 108.01 sec
2023-01-05 08:33:12,120 Stage-39 map = 31%,  reduce = 0%, Cumulative CPU 107.96 sec
2023-01-05 08:33:12,888 Stage-43 map = 23%,  reduce = 0%, Cumulative CPU 79.97 sec
2023-01-05 08:33:13,176 Stage-35 map = 31%,  reduce = 0%, Cumulative CPU 111.37 sec
2023-01-05 08:33:15,929 Stage-43 map = 24%,  reduce = 0%, Cumulative CPU 81.73 sec
2023-01-05 08:33:16,176 Stage-39 map = 32%,  reduce = 0%, Cumulative CPU 111.06 sec
2023-01-05 08:33:17,259 Stage-35 map = 32%,  reduce = 0%, Cumulative CPU 114.85 sec
2023-01-05 08:33:18,205 Stage-39 map = 33%,  reduce = 0%, Cumulative CPU 113.9 sec
2023-01-05 08:33:18,983 Stage-43 map = 25%,  reduce = 0%, Cumulative CPU 83.74 sec
2023-01-05 08:33:19,306 Stage-35 map = 33%,  reduce = 0%, Cumulative CPU 117.76 sec
2023-01-05 08:33:22,045 Stage-43 map = 27%,  reduce = 0%, Cumulative CPU 85.09 sec
2023-01-05 08:33:22,287 Stage-39 map = 34%,  reduce = 0%, Cumulative CPU 116.95 sec
2023-01-05 08:33:23,313 Stage-39 map = 35%,  reduce = 0%, Cumulative CPU 120.24 sec
2023-01-05 08:33:23,404 Stage-35 map = 34%,  reduce = 0%, Cumulative CPU 120.95 sec
2023-01-05 08:33:25,108 Stage-43 map = 28%,  reduce = 0%, Cumulative CPU 87.55 sec
2023-01-05 08:33:25,451 Stage-35 map = 35%,  reduce = 0%, Cumulative CPU 124.22 sec
2023-01-05 08:33:27,145 Stage-43 map = 29%,  reduce = 0%, Cumulative CPU 88.95 sec
2023-01-05 08:33:28,398 Stage-39 map = 37%,  reduce = 0%, Cumulative CPU 123.98 sec
2023-01-05 08:33:29,418 Stage-39 map = 38%,  reduce = 0%, Cumulative CPU 126.95 sec
2023-01-05 08:33:29,716 Stage-35 map = 36%,  reduce = 0%, Cumulative CPU 127.63 sec
2023-01-05 08:33:30,735 Stage-35 map = 37%,  reduce = 0%, Cumulative CPU 131.28 sec
2023-01-05 08:33:31,238 Stage-43 map = 30%,  reduce = 0%, Cumulative CPU 90.79 sec
2023-01-05 08:33:33,281 Stage-43 map = 32%,  reduce = 0%, Cumulative CPU 92.08 sec
2023-01-05 08:33:34,503 Stage-39 map = 39%,  reduce = 0%, Cumulative CPU 129.88 sec
2023-01-05 08:33:34,801 Stage-35 map = 38%,  reduce = 0%, Cumulative CPU 135.15 sec
2023-01-05 08:33:35,531 Stage-39 map = 40%,  reduce = 0%, Cumulative CPU 133.34 sec
2023-01-05 08:33:36,328 Stage-43 map = 33%,  reduce = 0%, Cumulative CPU 93.78 sec
2023-01-05 08:33:36,835 Stage-35 map = 40%,  reduce = 0%, Cumulative CPU 137.94 sec
2023-01-05 08:33:39,381 Stage-43 map = 35%,  reduce = 0%, Cumulative CPU 95.18 sec
2023-01-05 08:33:39,580 Stage-39 map = 41%,  reduce = 0%, Cumulative CPU 134.48 sec
2023-01-05 08:33:40,592 Stage-39 map = 42%,  reduce = 0%, Cumulative CPU 135.33 sec
2023-01-05 08:33:40,927 Stage-35 map = 41%,  reduce = 0%, Cumulative CPU 141.33 sec
2023-01-05 08:33:42,429 Stage-43 map = 36%,  reduce = 0%, Cumulative CPU 96.76 sec
2023-01-05 08:33:42,963 Stage-35 map = 42%,  reduce = 0%, Cumulative CPU 143.5 sec
2023-01-05 08:33:45,479 Stage-43 map = 37%,  reduce = 0%, Cumulative CPU 98.84 sec
2023-01-05 08:33:45,675 Stage-39 map = 43%,  reduce = 0%, Cumulative CPU 139.2 sec
2023-01-05 08:33:46,693 Stage-39 map = 44%,  reduce = 0%, Cumulative CPU 140.22 sec
2023-01-05 08:33:47,037 Stage-35 map = 43%,  reduce = 0%, Cumulative CPU 146.65 sec
2023-01-05 08:33:47,712 Stage-39 map = 45%,  reduce = 0%, Cumulative CPU 142.51 sec
2023-01-05 08:33:48,539 Stage-43 map = 38%,  reduce = 0%, Cumulative CPU 100.13 sec
2023-01-05 08:33:49,075 Stage-35 map = 45%,  reduce = 0%, Cumulative CPU 148.83 sec
2023-01-05 08:33:51,603 Stage-43 map = 39%,  reduce = 0%, Cumulative CPU 103.16 sec
2023-01-05 08:33:51,805 Stage-39 map = 46%,  reduce = 0%, Cumulative CPU 143.78 sec
2023-01-05 08:33:52,821 Stage-39 map = 47%,  reduce = 0%, Cumulative CPU 144.6 sec
2023-01-05 08:33:53,164 Stage-35 map = 46%,  reduce = 0%, Cumulative CPU 152.12 sec
2023-01-05 08:33:53,835 Stage-39 map = 48%,  reduce = 0%, Cumulative CPU 146.83 sec
2023-01-05 08:33:54,873 Stage-43 map = 40%,  reduce = 0%, Cumulative CPU 105.91 sec
2023-01-05 08:33:55,201 Stage-35 map = 47%,  reduce = 0%, Cumulative CPU 155.1 sec
2023-01-05 08:33:58,031 Stage-39 map = 49%,  reduce = 0%, Cumulative CPU 148.97 sec
2023-01-05 08:33:58,126 Stage-43 map = 41%,  reduce = 0%, Cumulative CPU 109.08 sec
2023-01-05 08:33:59,484 Stage-35 map = 49%,  reduce = 0%, Cumulative CPU 158.15 sec
2023-01-05 08:34:00,068 Stage-39 map = 50%,  reduce = 0%, Cumulative CPU 151.66 sec
2023-01-05 08:34:00,708 Stage-35 map = 50%,  reduce = 0%, Cumulative CPU 160.77 sec
2023-01-05 08:34:01,187 Stage-43 map = 42%,  reduce = 0%, Cumulative CPU 113.01 sec
2023-01-05 08:34:04,124 Stage-39 map = 52%,  reduce = 0%, Cumulative CPU 153.84 sec
2023-01-05 08:34:04,247 Stage-43 map = 43%,  reduce = 0%, Cumulative CPU 115.47 sec
2023-01-05 08:34:04,791 Stage-35 map = 51%,  reduce = 0%, Cumulative CPU 163.3 sec
2023-01-05 08:34:06,150 Stage-39 map = 53%,  reduce = 0%, Cumulative CPU 156.18 sec
2023-01-05 08:34:06,303 Stage-43 map = 44%,  reduce = 0%, Cumulative CPU 119.78 sec
2023-01-05 08:34:06,833 Stage-35 map = 53%,  reduce = 0%, Cumulative CPU 166.04 sec
2023-01-05 08:34:09,990 Stage-43 map = 45%,  reduce = 0%, Cumulative CPU 122.16 sec
2023-01-05 08:34:10,217 Stage-39 map = 54%,  reduce = 0%, Cumulative CPU 158.31 sec
2023-01-05 08:34:10,917 Stage-35 map = 54%,  reduce = 0%, Cumulative CPU 168.77 sec
2023-01-05 08:34:12,249 Stage-39 map = 55%,  reduce = 0%, Cumulative CPU 160.54 sec
2023-01-05 08:34:12,953 Stage-35 map = 56%,  reduce = 0%, Cumulative CPU 171.48 sec
2023-01-05 08:34:13,061 Stage-43 map = 46%,  reduce = 0%, Cumulative CPU 125.79 sec
2023-01-05 08:34:16,135 Stage-43 map = 47%,  reduce = 0%, Cumulative CPU 128.18 sec
2023-01-05 08:34:16,308 Stage-39 map = 57%,  reduce = 0%, Cumulative CPU 162.73 sec
2023-01-05 08:34:17,021 Stage-35 map = 57%,  reduce = 0%, Cumulative CPU 173.76 sec
2023-01-05 08:34:17,332 Stage-39 map = 58%,  reduce = 0%, Cumulative CPU 164.85 sec
2023-01-05 08:34:19,066 Stage-35 map = 58%,  reduce = 0%, Cumulative CPU 177.06 sec
2023-01-05 08:34:19,263 Stage-43 map = 48%,  reduce = 0%, Cumulative CPU 131.85 sec
2023-01-05 08:34:21,525 Stage-43 map = 49%,  reduce = 0%, Cumulative CPU 134.06 sec
2023-01-05 08:34:22,447 Stage-39 map = 60%,  reduce = 0%, Cumulative CPU 168.88 sec
2023-01-05 08:34:23,154 Stage-35 map = 59%,  reduce = 0%, Cumulative CPU 179.21 sec
2023-01-05 08:34:23,473 Stage-39 map = 61%,  reduce = 0%, Cumulative CPU 171.01 sec
2023-01-05 08:34:25,214 Stage-35 map = 60%,  reduce = 0%, Cumulative CPU 182.23 sec
2023-01-05 08:34:27,527 Stage-39 map = 62%,  reduce = 0%, Cumulative CPU 172.44 sec
2023-01-05 08:34:27,661 Stage-43 map = 50%,  reduce = 0%, Cumulative CPU 140.32 sec
2023-01-05 08:34:29,309 Stage-35 map = 61%,  reduce = 0%, Cumulative CPU 184.35 sec
2023-01-05 08:34:29,556 Stage-39 map = 64%,  reduce = 0%, Cumulative CPU 176.31 sec
2023-01-05 08:34:30,733 Stage-43 map = 51%,  reduce = 0%, Cumulative CPU 143.71 sec
2023-01-05 08:34:31,354 Stage-35 map = 62%,  reduce = 0%, Cumulative CPU 187.31 sec
2023-01-05 08:34:33,617 Stage-39 map = 65%,  reduce = 0%, Cumulative CPU 177.48 sec
2023-01-05 08:34:33,793 Stage-43 map = 52%,  reduce = 0%, Cumulative CPU 146.47 sec
2023-01-05 08:34:35,439 Stage-35 map = 64%,  reduce = 0%, Cumulative CPU 189.78 sec
2023-01-05 08:34:35,641 Stage-39 map = 66%,  reduce = 0%, Cumulative CPU 181.32 sec
2023-01-05 08:34:36,463 Stage-35 map = 65%,  reduce = 0%, Cumulative CPU 191.72 sec
2023-01-05 08:34:36,840 Stage-43 map = 53%,  reduce = 0%, Cumulative CPU 148.27 sec
2023-01-05 08:34:39,706 Stage-39 map = 67%,  reduce = 0%, Cumulative CPU 182.29 sec
2023-01-05 08:34:40,127 Stage-43 map = 54%,  reduce = 0%, Cumulative CPU 150.08 sec
2023-01-05 08:34:40,721 Stage-39 map = 68%,  reduce = 0%, Cumulative CPU 183.77 sec
2023-01-05 08:34:41,551 Stage-35 map = 67%,  reduce = 0%, Cumulative CPU 194.52 sec
2023-01-05 08:34:41,733 Stage-39 map = 69%,  reduce = 0%, Cumulative CPU 185.84 sec
2023-01-05 08:34:42,565 Stage-35 map = 68%,  reduce = 0%, Cumulative CPU 196.82 sec
2023-01-05 08:34:42,587 Stage-43 map = 55%,  reduce = 0%, Cumulative CPU 151.77 sec
2023-01-05 08:34:45,658 Stage-43 map = 56%,  reduce = 0%, Cumulative CPU 153.23 sec
2023-01-05 08:34:45,787 Stage-39 map = 70%,  reduce = 0%, Cumulative CPU 187.01 sec
2023-01-05 08:34:46,655 Stage-35 map = 69%,  reduce = 0%, Cumulative CPU 197.85 sec
2023-01-05 08:34:46,806 Stage-39 map = 71%,  reduce = 0%, Cumulative CPU 189.16 sec
2023-01-05 08:34:47,821 Stage-39 map = 73%,  reduce = 0%, Cumulative CPU 191.44 sec
2023-01-05 08:34:48,686 Stage-35 map = 71%,  reduce = 0%, Cumulative CPU 201.34 sec
2023-01-05 08:34:48,721 Stage-43 map = 57%,  reduce = 0%, Cumulative CPU 155.33 sec
2023-01-05 08:34:51,791 Stage-43 map = 58%,  reduce = 0%, Cumulative CPU 156.5 sec
2023-01-05 08:34:52,749 Stage-35 map = 72%,  reduce = 0%, Cumulative CPU 202.08 sec
2023-01-05 08:34:52,887 Stage-39 map = 74%,  reduce = 0%, Cumulative CPU 193.91 sec
2023-01-05 08:34:53,898 Stage-39 map = 75%,  reduce = 0%, Cumulative CPU 196.09 sec
2023-01-05 08:34:54,778 Stage-35 map = 73%,  reduce = 0%, Cumulative CPU 205.72 sec
2023-01-05 08:34:54,867 Stage-43 map = 59%,  reduce = 0%, Cumulative CPU 158.74 sec
2023-01-05 08:34:57,939 Stage-43 map = 60%,  reduce = 0%, Cumulative CPU 159.97 sec
2023-01-05 08:34:57,947 Stage-39 map = 76%,  reduce = 0%, Cumulative CPU 197.14 sec
2023-01-05 08:34:58,842 Stage-35 map = 75%,  reduce = 0%, Cumulative CPU 207.72 sec
2023-01-05 08:34:58,962 Stage-39 map = 77%,  reduce = 0%, Cumulative CPU 198.71 sec
2023-01-05 08:34:59,977 Stage-39 map = 78%,  reduce = 0%, Cumulative CPU 200.61 sec
2023-01-05 08:35:00,874 Stage-35 map = 76%,  reduce = 0%, Cumulative CPU 210.29 sec
2023-01-05 08:35:00,991 Stage-43 map = 61%,  reduce = 0%, Cumulative CPU 161.72 sec
2023-01-05 08:35:04,037 Stage-39 map = 79%,  reduce = 0%, Cumulative CPU 202.81 sec
2023-01-05 08:35:04,452 Stage-43 map = 62%,  reduce = 0%, Cumulative CPU 162.85 sec
2023-01-05 08:35:04,936 Stage-35 map = 78%,  reduce = 0%, Cumulative CPU 211.68 sec
2023-01-05 08:35:06,072 Stage-39 map = 81%,  reduce = 0%, Cumulative CPU 204.73 sec
2023-01-05 08:35:06,481 Stage-43 map = 63%,  reduce = 0%, Cumulative CPU 165.21 sec
2023-01-05 08:35:06,964 Stage-35 map = 79%,  reduce = 0%, Cumulative CPU 214.55 sec
2023-01-05 08:35:10,140 Stage-39 map = 82%,  reduce = 0%, Cumulative CPU 207.56 sec
2023-01-05 08:35:10,360 Stage-43 map = 64%,  reduce = 0%, Cumulative CPU 166.65 sec
2023-01-05 08:35:11,048 Stage-35 map = 80%,  reduce = 0%, Cumulative CPU 216.52 sec
2023-01-05 08:35:12,171 Stage-39 map = 84%,  reduce = 0%, Cumulative CPU 209.19 sec
2023-01-05 08:35:13,079 Stage-35 map = 82%,  reduce = 0%, Cumulative CPU 218.7 sec
2023-01-05 08:35:15,655 Stage-43 map = 66%,  reduce = 0%, Cumulative CPU 169.48 sec
2023-01-05 08:35:16,240 Stage-39 map = 85%,  reduce = 0%, Cumulative CPU 212.34 sec
2023-01-05 08:35:17,140 Stage-35 map = 83%,  reduce = 0%, Cumulative CPU 220.13 sec
2023-01-05 08:35:18,178 Stage-35 map = 84%,  reduce = 0%, Cumulative CPU 220.42 sec
2023-01-05 08:35:18,275 Stage-39 map = 87%,  reduce = 0%, Cumulative CPU 214.17 sec
2023-01-05 08:35:18,702 Stage-43 map = 67%,  reduce = 0%, Cumulative CPU 171.01 sec
2023-01-05 08:35:19,194 Stage-35 map = 85%,  reduce = 0%, Cumulative CPU 223.46 sec
2023-01-05 08:35:21,311 Stage-39 map = 88%,  reduce = 0%, Cumulative CPU 214.76 sec
2023-01-05 08:35:21,745 Stage-43 map = 68%,  reduce = 0%, Cumulative CPU 172.6 sec
2023-01-05 08:35:23,347 Stage-39 map = 89%,  reduce = 0%, Cumulative CPU 218.61 sec
2023-01-05 08:35:24,830 Stage-43 map = 69%,  reduce = 0%, Cumulative CPU 174.4 sec
2023-01-05 08:35:25,284 Stage-35 map = 87%,  reduce = 0%, Cumulative CPU 226.95 sec
2023-01-05 08:35:27,885 Stage-43 map = 71%,  reduce = 0%, Cumulative CPU 175.73 sec
2023-01-05 08:35:28,409 Stage-39 map = 90%,  reduce = 0%, Cumulative CPU 222.88 sec
2023-01-05 08:35:29,344 Stage-35 map = 88%,  reduce = 0%, Cumulative CPU 228.03 sec
2023-01-05 08:35:31,143 Stage-43 map = 72%,  reduce = 0%, Cumulative CPU 177.87 sec
2023-01-05 08:35:31,379 Stage-35 map = 89%,  reduce = 0%, Cumulative CPU 233.64 sec
2023-01-05 08:35:34,191 Stage-43 map = 74%,  reduce = 0%, Cumulative CPU 179.21 sec
2023-01-05 08:35:34,478 Stage-39 map = 91%,  reduce = 0%, Cumulative CPU 227.56 sec
2023-01-05 08:35:35,432 Stage-35 map = 90%,  reduce = 0%, Cumulative CPU 234.49 sec
2023-01-05 08:35:36,454 Stage-35 map = 91%,  reduce = 0%, Cumulative CPU 236.26 sec
2023-01-05 08:35:37,248 Stage-43 map = 75%,  reduce = 0%, Cumulative CPU 181.83 sec
2023-01-05 08:35:38,485 Stage-35 map = 92%,  reduce = 0%, Cumulative CPU 238.96 sec
2023-01-05 08:35:39,539 Stage-43 map = 77%,  reduce = 0%, Cumulative CPU 183.55 sec
2023-01-05 08:35:40,559 Stage-39 map = 93%,  reduce = 0%, Cumulative CPU 232.12 sec
2023-01-05 08:35:42,548 Stage-35 map = 93%,  reduce = 0%, Cumulative CPU 241.15 sec
2023-01-05 08:35:42,598 Stage-43 map = 78%,  reduce = 0%, Cumulative CPU 188.51 sec
2023-01-05 08:35:45,644 Stage-43 map = 81%,  reduce = 0%, Cumulative CPU 190.12 sec
2023-01-05 08:35:46,648 Stage-39 map = 94%,  reduce = 0%, Cumulative CPU 237.24 sec
2023-01-05 08:35:47,675 Stage-43 map = 82%,  reduce = 0%, Cumulative CPU 190.67 sec
2023-01-05 08:35:48,641 Stage-35 map = 94%,  reduce = 0%, Cumulative CPU 248.37 sec
2023-01-05 08:35:48,720 Stage-43 map = 83%,  reduce = 0%, Cumulative CPU 195.26 sec
2023-01-05 08:35:52,716 Stage-39 map = 95%,  reduce = 0%, Cumulative CPU 242.38 sec
2023-01-05 08:35:54,927 Stage-35 map = 96%,  reduce = 0%, Cumulative CPU 253.25 sec
2023-01-05 08:35:58,779 Stage-39 map = 96%,  reduce = 0%, Cumulative CPU 247.35 sec
2023-01-05 08:36:00,901 Stage-43 map = 84%,  reduce = 0%, Cumulative CPU 203.92 sec
2023-01-05 08:36:01,009 Stage-35 map = 97%,  reduce = 0%, Cumulative CPU 258.24 sec
2023-01-05 08:36:04,847 Stage-39 map = 97%,  reduce = 0%, Cumulative CPU 252.52 sec
2023-01-05 08:36:05,072 Stage-35 map = 98%,  reduce = 0%, Cumulative CPU 259.48 sec
2023-01-05 08:36:07,071 Stage-43 map = 85%,  reduce = 0%, Cumulative CPU 208.61 sec
2023-01-05 08:36:10,921 Stage-39 map = 99%,  reduce = 0%, Cumulative CPU 257.1 sec
2023-01-05 08:36:13,164 Stage-43 map = 86%,  reduce = 0%, Cumulative CPU 213.87 sec
2023-01-05 08:36:13,181 Stage-35 map = 99%,  reduce = 0%, Cumulative CPU 266.13 sec
2023-01-05 08:36:14,970 Stage-39 map = 100%,  reduce = 0%, Cumulative CPU 259.5 sec
MapReduce Total cumulative CPU time: 4 minutes 19 seconds 500 msec
Ended Job = job_1672890466700_0111
2023-01-05 08:36:18,250 Stage-35 map = 100%,  reduce = 0%, Cumulative CPU 268.96 sec
Stage-54 is selected by condition resolver.
Stage-55 is filtered out by condition resolver.
Stage-14 is filtered out by condition resolver.
2023-01-05 08:36:19,252 Stage-43 map = 87%,  reduce = 0%, Cumulative CPU 218.65 sec
MapReduce Total cumulative CPU time: 4 minutes 28 seconds 960 msec
Ended Job = job_1672890466700_0112
Stage-51 is selected by condition resolver.
Stage-52 is filtered out by condition resolver.
Stage-9 is filtered out by condition resolver.




2023-01-05 08:36:25,364 Stage-43 map = 88%,  reduce = 0%, Cumulative CPU 221.18 sec
Hadoop job information for Stage-31: number of mappers: 2; number of reducers: 0
2023-01-05 08:36:27,823 Stage-31 map = 0%,  reduce = 0%
2023-01-05 08:36:31,630 Stage-43 map = 90%,  reduce = 0%, Cumulative CPU 223.82 sec
2023-01-05 08:36:37,024 Stage-43 map = 91%,  reduce = 0%, Cumulative CPU 226.48 sec
2023-01-05 08:36:42,625 Stage-43 map = 92%,  reduce = 0%, Cumulative CPU 229.2 sec
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:36:42	Processing rows:	200000	Hashtable size:	199999	Memory usage:	233848448	percentage:	0.978
Hive Runtime Error: Map local work exhausted memory
2023-01-05 08:36:44,113 Stage-31 map = 4%,  reduce = 0%, Cumulative CPU 15.08 sec
2023-01-05 08:36:46,313 Stage-31 map = 6%,  reduce = 0%, Cumulative CPU 30.26 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-54

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 7 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:36:49,231 Stage-43 map = 93%,  reduce = 0%, Cumulative CPU 231.13 sec
2023-01-05 08:36:49,557 Stage-31 map = 9%,  reduce = 0%, Cumulative CPU 33.91 sec
2023-01-05 08:36:50	Processing rows:	200000	Hashtable size:	199999	Memory usage:	230573056	percentage:	0.964
2023-01-05 08:36:51,815 Stage-31 map = 10%,  reduce = 0%, Cumulative CPU 37.82 sec
Starting Job = job_1672890466700_0114, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0114/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0114
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-51

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 8 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:36:55,581 Stage-43 map = 94%,  reduce = 0%, Cumulative CPU 232.95 sec
2023-01-05 08:36:55,883 Stage-31 map = 12%,  reduce = 0%, Cumulative CPU 40.07 sec
Starting Job = job_1672890466700_0115, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0115/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0115
2023-01-05 08:36:57,926 Stage-31 map = 13%,  reduce = 0%, Cumulative CPU 44.1 sec
Hadoop job information for Stage-14: number of mappers: 2; number of reducers: 1
2023-01-05 08:37:00,457 Stage-14 map = 0%,  reduce = 0%
2023-01-05 08:37:00,659 Stage-43 map = 96%,  reduce = 0%, Cumulative CPU 235.48 sec
2023-01-05 08:37:02,002 Stage-31 map = 16%,  reduce = 0%, Cumulative CPU 47.56 sec
2023-01-05 08:37:04,047 Stage-31 map = 17%,  reduce = 0%, Cumulative CPU 51.7 sec
2023-01-05 08:37:06,764 Stage-43 map = 97%,  reduce = 0%, Cumulative CPU 238.98 sec
2023-01-05 08:37:08,117 Stage-31 map = 19%,  reduce = 0%, Cumulative CPU 54.39 sec
2023-01-05 08:37:10,152 Stage-31 map = 20%,  reduce = 0%, Cumulative CPU 56.92 sec
2023-01-05 08:37:10,711 Stage-14 map = 50%,  reduce = 0%, Cumulative CPU 8.14 sec
2023-01-05 08:37:13,054 Stage-43 map = 98%,  reduce = 0%, Cumulative CPU 242.18 sec
2023-01-05 08:37:13,197 Stage-31 map = 22%,  reduce = 0%, Cumulative CPU 58.36 sec
2023-01-05 08:37:16,251 Stage-31 map = 23%,  reduce = 0%, Cumulative CPU 60.53 sec
2023-01-05 08:37:16,798 Stage-14 map = 100%,  reduce = 0%, Cumulative CPU 21.44 sec
2023-01-05 08:37:19,182 Stage-43 map = 99%,  reduce = 0%, Cumulative CPU 245.64 sec
2023-01-05 08:37:19,352 Stage-31 map = 25%,  reduce = 0%, Cumulative CPU 62.6 sec
2023-01-05 08:37:22,418 Stage-31 map = 26%,  reduce = 0%, Cumulative CPU 64.11 sec
2023-01-05 08:37:25,481 Stage-31 map = 27%,  reduce = 0%, Cumulative CPU 65.45 sec
2023-01-05 08:37:28,528 Stage-31 map = 29%,  reduce = 0%, Cumulative CPU 68.38 sec
2023-01-05 08:37:29,344 Stage-43 map = 100%,  reduce = 0%, Cumulative CPU 250.97 sec
2023-01-05 08:37:29,985 Stage-14 map = 100%,  reduce = 71%, Cumulative CPU 36.89 sec
MapReduce Total cumulative CPU time: 4 minutes 10 seconds 970 msec
Ended Job = job_1672890466700_0113
Stage-57 is selected by condition resolver.
Stage-58 is filtered out by condition resolver.
Stage-19 is filtered out by condition resolver.
2023-01-05 08:37:31,576 Stage-31 map = 32%,  reduce = 0%, Cumulative CPU 70.89 sec


SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 08:37:33,620 Stage-31 map = 35%,  reduce = 0%, Cumulative CPU 73.89 sec
2023-01-05 08:37:36,090 Stage-14 map = 100%,  reduce = 77%, Cumulative CPU 43.43 sec
2023-01-05 08:37:37,729 Stage-31 map = 37%,  reduce = 0%, Cumulative CPU 77.4 sec
2023-01-05 08:37:39	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 08:37:39,763 Stage-31 map = 39%,  reduce = 0%, Cumulative CPU 82.04 sec
Hadoop job information for Stage-9: number of mappers: 2; number of reducers: 1
2023-01-05 08:37:40,633 Stage-9 map = 0%,  reduce = 0%
2023-01-05 08:37:42,322 Stage-14 map = 100%,  reduce = 83%, Cumulative CPU 50.22 sec
2023-01-05 08:37:43,864 Stage-31 map = 42%,  reduce = 0%, Cumulative CPU 87.08 sec
2023-01-05 08:37:46,022 Stage-31 map = 43%,  reduce = 0%, Cumulative CPU 91.58 sec
2023-01-05 08:37:47,815 Stage-14 map = 100%,  reduce = 90%, Cumulative CPU 50.22 sec
2023-01-05 08:37:49,325 Stage-31 map = 44%,  reduce = 0%, Cumulative CPU 95.23 sec

2023-01-05 08:37:52,532 Stage-31 map = 45%,  reduce = 0%, Cumulative CPU 97.98 sec
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-57

Logs:

2023-01-05 08:37:53,345 Stage-9 map = 50%,  reduce = 0%, Cumulative CPU 7.71 sec
2023-01-05 08:37:54,074 Stage-14 map = 100%,  reduce = 96%, Cumulative CPU 63.71 sec
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 10 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:37:55,404 Stage-9 map = 100%,  reduce = 0%, Cumulative CPU 21.87 sec
2023-01-05 08:37:55,575 Stage-31 map = 47%,  reduce = 0%, Cumulative CPU 101.75 sec
Starting Job = job_1672890466700_0116, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0116/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0116
2023-01-05 08:37:57,608 Stage-31 map = 48%,  reduce = 0%, Cumulative CPU 105.56 sec
2023-01-05 08:38:00,163 Stage-14 map = 100%,  reduce = 100%, Cumulative CPU 69.98 sec
2023-01-05 08:38:01,687 Stage-31 map = 49%,  reduce = 0%, Cumulative CPU 109.07 sec
MapReduce Total cumulative CPU time: 1 minutes 9 seconds 980 msec
Ended Job = job_1672890466700_0114
Launching Job 11 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:38:03,736 Stage-31 map = 50%,  reduce = 0%, Cumulative CPU 112.85 sec
Starting Job = job_1672890466700_0117, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0117/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0117
2023-01-05 08:38:07,842 Stage-31 map = 53%,  reduce = 0%, Cumulative CPU 115.91 sec
2023-01-05 08:38:09,867 Stage-31 map = 54%,  reduce = 0%, Cumulative CPU 120.64 sec
2023-01-05 08:38:12,868 Stage-9 map = 100%,  reduce = 71%, Cumulative CPU 34.74 sec
2023-01-05 08:38:14,018 Stage-31 map = 57%,  reduce = 0%, Cumulative CPU 123.56 sec
2023-01-05 08:38:16,050 Stage-31 map = 59%,  reduce = 0%, Cumulative CPU 126.94 sec
Hadoop job information for Stage-19: number of mappers: 2; number of reducers: 1
2023-01-05 08:38:16,718 Stage-19 map = 0%,  reduce = 0%
2023-01-05 08:38:18,979 Stage-9 map = 100%,  reduce = 75%, Cumulative CPU 39.51 sec
2023-01-05 08:38:20,105 Stage-31 map = 61%,  reduce = 0%, Cumulative CPU 129.7 sec
2023-01-05 08:38:22,152 Stage-31 map = 63%,  reduce = 0%, Cumulative CPU 132.79 sec
2023-01-05 08:38:25,076 Stage-9 map = 100%,  reduce = 82%, Cumulative CPU 45.94 sec
2023-01-05 08:38:25,891 Stage-19 map = 50%,  reduce = 0%, Cumulative CPU 6.99 sec
2023-01-05 08:38:26,249 Stage-31 map = 66%,  reduce = 0%, Cumulative CPU 135.53 sec
2023-01-05 08:38:28,279 Stage-31 map = 67%,  reduce = 0%, Cumulative CPU 138.66 sec
2023-01-05 08:38:28,952 Stage-19 map = 100%,  reduce = 0%, Cumulative CPU 15.46 sec
2023-01-05 08:38:31,199 Stage-9 map = 100%,  reduce = 87%, Cumulative CPU 51.7 sec
2023-01-05 08:38:31,349 Stage-31 map = 70%,  reduce = 0%, Cumulative CPU 140.78 sec
2023-01-05 08:38:34,428 Stage-31 map = 71%,  reduce = 0%, Cumulative CPU 142.77 sec
2023-01-05 08:38:37,306 Stage-9 map = 100%,  reduce = 92%, Cumulative CPU 56.91 sec
2023-01-05 08:38:37,481 Stage-31 map = 74%,  reduce = 0%, Cumulative CPU 144.76 sec
2023-01-05 08:38:40,536 Stage-31 map = 76%,  reduce = 0%, Cumulative CPU 147.17 sec
2023-01-05 08:38:42,189 Stage-19 map = 100%,  reduce = 87%, Cumulative CPU 29.94 sec
2023-01-05 08:38:43,407 Stage-9 map = 100%,  reduce = 97%, Cumulative CPU 63.27 sec
2023-01-05 08:38:43,591 Stage-31 map = 78%,  reduce = 0%, Cumulative CPU 149.57 sec
2023-01-05 08:38:45,619 Stage-31 map = 79%,  reduce = 0%, Cumulative CPU 149.97 sec
2023-01-05 08:38:46,465 Stage-9 map = 100%,  reduce = 100%, Cumulative CPU 67.38 sec
2023-01-05 08:38:46,639 Stage-31 map = 81%,  reduce = 0%, Cumulative CPU 152.14 sec
2023-01-05 08:38:47,363 Stage-19 map = 100%,  reduce = 100%, Cumulative CPU 35.33 sec
MapReduce Total cumulative CPU time: 35 seconds 330 msec
Ended Job = job_1672890466700_0116
MapReduce Total cumulative CPU time: 1 minutes 7 seconds 380 msec
Ended Job = job_1672890466700_0115
Launching Job 12 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0118, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0118/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0118
Launching Job 13 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:38:51,736 Stage-31 map = 82%,  reduce = 0%, Cumulative CPU 156.16 sec
Starting Job = job_1672890466700_0119, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0119/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0119
2023-01-05 08:38:57,839 Stage-31 map = 83%,  reduce = 0%, Cumulative CPU 159.43 sec
Hadoop job information for Stage-15: number of mappers: 1; number of reducers: 1
2023-01-05 08:38:58,071 Stage-15 map = 0%,  reduce = 0%
Hadoop job information for Stage-20: number of mappers: 1; number of reducers: 1
2023-01-05 08:39:00,331 Stage-20 map = 0%,  reduce = 0%
2023-01-05 08:39:04,129 Stage-31 map = 84%,  reduce = 0%, Cumulative CPU 162.76 sec
2023-01-05 08:39:06,229 Stage-15 map = 100%,  reduce = 0%, Cumulative CPU 6.39 sec
2023-01-05 08:39:06,456 Stage-20 map = 100%,  reduce = 0%, Cumulative CPU 4.27 sec
2023-01-05 08:39:10,241 Stage-31 map = 85%,  reduce = 0%, Cumulative CPU 166.24 sec
2023-01-05 08:39:14,631 Stage-20 map = 100%,  reduce = 100%, Cumulative CPU 7.09 sec
MapReduce Total cumulative CPU time: 7 seconds 90 msec
Ended Job = job_1672890466700_0118
2023-01-05 08:39:16,329 Stage-31 map = 86%,  reduce = 0%, Cumulative CPU 170.02 sec
2023-01-05 08:39:17,618 Stage-15 map = 100%,  reduce = 100%, Cumulative CPU 12.42 sec
MapReduce Total cumulative CPU time: 12 seconds 420 msec
Ended Job = job_1672890466700_0117
2023-01-05 08:39:22,426 Stage-31 map = 88%,  reduce = 0%, Cumulative CPU 173.44 sec
Hadoop job information for Stage-10: number of mappers: 1; number of reducers: 1
2023-01-05 08:39:25,426 Stage-10 map = 0%,  reduce = 0%
2023-01-05 08:39:28,518 Stage-31 map = 90%,  reduce = 0%, Cumulative CPU 177.04 sec
2023-01-05 08:39:33,796 Stage-31 map = 92%,  reduce = 0%, Cumulative CPU 180.5 sec
2023-01-05 08:39:35,665 Stage-10 map = 100%,  reduce = 0%, Cumulative CPU 6.36 sec
2023-01-05 08:39:39,880 Stage-31 map = 95%,  reduce = 0%, Cumulative CPU 184.24 sec
2023-01-05 08:39:45,881 Stage-10 map = 100%,  reduce = 100%, Cumulative CPU 13.15 sec
2023-01-05 08:39:45,982 Stage-31 map = 98%,  reduce = 0%, Cumulative CPU 189.14 sec
MapReduce Total cumulative CPU time: 13 seconds 150 msec
Ended Job = job_1672890466700_0119
2023-01-05 08:39:52,071 Stage-31 map = 99%,  reduce = 0%, Cumulative CPU 194.81 sec
2023-01-05 08:39:55,117 Stage-31 map = 100%,  reduce = 0%, Cumulative CPU 197.07 sec
MapReduce Total cumulative CPU time: 3 minutes 17 seconds 70 msec
Ended Job = job_1672890466700_0110
Stage-48 is selected by condition resolver.
Stage-49 is filtered out by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2023-01-05 08:40:04	Starting to launch local task to process map join;	maximum memory = 239075328
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-48

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 15 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0120, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0120/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0120
Hadoop job information for Stage-2: number of mappers: 2; number of reducers: 1
2023-01-05 08:40:25,921 Stage-2 map = 0%,  reduce = 0%
2023-01-05 08:40:32,029 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 6.22 sec
2023-01-05 08:40:36,116 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 13.81 sec
2023-01-05 08:40:50,499 Stage-2 map = 100%,  reduce = 96%, Cumulative CPU 28.96 sec
2023-01-05 08:40:51,515 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 30.89 sec
MapReduce Total cumulative CPU time: 30 seconds 890 msec
Ended Job = job_1672890466700_0120
Launching Job 16 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0121, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0121/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0121
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2023-01-05 08:41:00,655 Stage-3 map = 0%,  reduce = 0%
2023-01-05 08:41:07,804 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 4.41 sec
2023-01-05 08:41:16,988 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 9.64 sec
MapReduce Total cumulative CPU time: 9 seconds 640 msec
Ended Job = job_1672890466700_0121
Stage-44 is filtered out by condition resolver.
Stage-45 is filtered out by condition resolver.
Stage-46 is selected by condition resolver.
Stage-47 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
SLF4J: Class path contains multiple SLF4J bindings.

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:41:28	Processing rows:	200000	Hashtable size:	92487	Memory usage:	148909840	percentage:	0.623
2023-01-05 08:41:31	Processing rows:	400000	Hashtable size:	292487	Memory usage:	208800600	percentage:	0.873

Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-46

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 18 out of 26
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0122, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0122/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0122
Hadoop job information for Stage-4: number of mappers: 4; number of reducers: 1
2023-01-05 08:41:49,360 Stage-4 map = 0%,  reduce = 0%
2023-01-05 08:41:56,745 Stage-4 map = 75%,  reduce = 0%, Cumulative CPU 11.89 sec
2023-01-05 08:41:57,759 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 14.41 sec
2023-01-05 08:42:03,875 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 21.33 sec
MapReduce Total cumulative CPU time: 21 seconds 330 msec
Ended Job = job_1672890466700_0122
Launching Job 19 out of 26
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0123, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0123/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0123
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 08:42:12,921 Stage-5 map = 0%,  reduce = 0%
2023-01-05 08:42:19,028 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.27 sec
2023-01-05 08:42:23,094 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.47 sec
MapReduce Total cumulative CPU time: 2 seconds 470 msec
Ended Job = job_1672890466700_0123
MapReduce Jobs Launched:
Stage-Stage-39: Map: 4   Cumulative CPU: 259.5 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-35: Map: 4   Cumulative CPU: 268.96 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-43: Map: 2   Cumulative CPU: 250.97 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-14: Map: 2  Reduce: 1   Cumulative CPU: 69.98 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-19: Map: 2  Reduce: 1   Cumulative CPU: 35.33 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-9: Map: 2  Reduce: 1   Cumulative CPU: 67.38 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-20: Map: 1  Reduce: 1   Cumulative CPU: 7.09 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-15: Map: 1  Reduce: 1   Cumulative CPU: 12.42 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-10: Map: 1  Reduce: 1   Cumulative CPU: 13.15 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-31: Map: 2   Cumulative CPU: 197.07 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 30.89 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 9.64 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 4  Reduce: 1   Cumulative CPU: 21.33 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.47 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 20 minutes 46 seconds 180 msec
OK
AAAAAAAAAAAHFAAA	Estelle             	Massey                        	MACAO
AAAAAAAAAAAMCAAA	James               	Schmidt                       	CZECH REPUBLIC
AAAAAAAAAABEFAAA	James               	Long                          	SERBIA
AAAAAAAAAABJBAAA	Ann                 	South                         	CAYMAN ISLANDS
AAAAAAAAAACCHAAA	Richard             	Foster                        	KENYA
AAAAAAAAAACJAAAA	Janet               	Robbins                       	NORWAY
AAAAAAAAAACKGAAA	Betty               	Good                          	KAZAKHSTAN
AAAAAAAAAADHHAAA	Donald              	Devlin                        	HONG KONG
AAAAAAAAAAEAAAAA	NULL	Estrada                       	NULL
AAAAAAAAAAEKCAAA	Annetta             	Wright                        	TRINIDAD AND TOBAGO
AAAAAAAAAAFFEAAA	Mark                	Ferris                        	SLOVAKIA
AAAAAAAAAAHHDAAA	Steven              	Gardner                       	SAINT HELENA
AAAAAAAAAAIMEAAA	NULL	Robinson                      	NULL
AAAAAAAAAAIPAAAA	Sarah               	Luna                          	SOMALIA
AAAAAAAAAAJHDAAA	Anthony             	Jackson                       	ECUADOR
AAAAAAAAAAJOEAAA	George              	Clay                          	RWANDA
AAAAAAAAAAKFEAAA	Caryl               	Stone                         	AUSTRIA
AAAAAAAAAAKJAAAA	Tim                 	Lee                           	UZBEKISTAN
AAAAAAAAAALEBAAA	William             	Branson                       	BANGLADESH
AAAAAAAAAAMODAAA	Jason               	Perez                         	MALDIVES
AAAAAAAAAANPGAAA	Cyrus               	Blanchard                     	QATAR
AAAAAAAAAAOCFAAA	Richard             	Drake                         	TIMOR-LESTE
AAAAAAAAAAODEAAA	Danilo              	Mcgough                       	ETHIOPIA
AAAAAAAAAAOMFAAA	Anthony             	Lewis                         	KOREA, REPUBLIC OF
AAAAAAAAAAOPBAAA	NULL	Martinez                      	GREENLAND
AAAAAAAAAAOPCAAA	Kenneth             	Carr                          	MONTSERRAT
AAAAAAAAAAPNGAAA	Gustavo             	Lancaster                     	KUWAIT
AAAAAAAAABADFAAA	Andrea              	Cantrell                      	ANTIGUA AND BARBUDA
AAAAAAAAABAGFAAA	Louis               	Willis                        	FRENCH GUIANA
AAAAAAAAABALBAAA	John                	Robinson                      	SWEDEN
AAAAAAAAABBJFAAA	Jacob               	Palmer                        	MOZAMBIQUE
AAAAAAAAABDKDAAA	Lisa                	NULL	NULL
AAAAAAAAABIBFAAA	Jennifer            	Lutz                          	PALAU
AAAAAAAAABIOGAAA	Yolanda             	Cortez                        	MALI
AAAAAAAAABJDDAAA	Nellie              	Ramirez                       	MALAYSIA
AAAAAAAAABKIDAAA	Efrain              	Wong                          	GUINEA
AAAAAAAAABKLGAAA	Delilah             	Manning                       	BERMUDA
AAAAAAAAABNDBAAA	Laurie              	Bright                        	CROATIA
AAAAAAAAABOHAAAA	Audrey              	Guillory                      	ERITREA
AAAAAAAAABOIEAAA	Lynne               	Brady                         	MALI
AAAAAAAAABOJFAAA	William             	Bass                          	KAZAKHSTAN
AAAAAAAAABPFBAAA	Esther              	Kidd                          	BOLIVIA
AAAAAAAAABPIEAAA	Eric                	Whelan                        	ALBANIA
AAAAAAAAABPODAAA	Jeffrey             	Graham                        	DJIBOUTI
AAAAAAAAACBECAAA	Jim                 	Martinez                      	ALAND ISLANDS
AAAAAAAAACBPGAAA	Nadine              	Armstrong                     	BENIN
AAAAAAAAACCCFAAA	Donald              	Word                          	INDONESIA
AAAAAAAAACCEHAAA	NULL	Gonzales                      	NULL
AAAAAAAAACDFGAAA	NULL	Hinojosa                      	NULL
AAAAAAAAACDGDAAA	Bradley             	Garcia                        	BAHAMAS
AAAAAAAAACDOGAAA	Elizabeth           	Green                         	GUADELOUPE
AAAAAAAAACFKCAAA	Edward              	Connolly                      	FRANCE
AAAAAAAAACGAEAAA	Richard             	Labbe                         	TRINIDAD AND TOBAGO
AAAAAAAAACHFAAAA	NULL	Preston                       	NULL
AAAAAAAAACILEAAA	Dennis              	Adams                         	ETHIOPIA
AAAAAAAAACIOFAAA	Vanessa             	Reynolds                      	NETHERLANDS
AAAAAAAAACIPDAAA	Ethel               	Perry                         	GUINEA
AAAAAAAAACJDEAAA	Nicholas            	Bates                         	MARSHALL ISLANDS
AAAAAAAAACJIEAAA	Nakia               	Harkins                       	NEW CALEDONIA
AAAAAAAAACNEBAAA	Nicole              	Brown                         	GUAM
AAAAAAAAACOCBAAA	Marcy               	White                         	DENMARK
AAAAAAAAACOGDAAA	Edward              	Carter                        	MONTENEGRO
AAAAAAAAACPFHAAA	Benjamin            	Lister                        	INDIA
AAAAAAAAACPLDAAA	Paul                	Parra                         	MALI
AAAAAAAAADBKFAAA	Michele             	Waugh                         	OMAN
AAAAAAAAADCMAAAA	Lara                	Bush                          	MARTINIQUE
AAAAAAAAADCPFAAA	Daniel              	Mcghee                        	FRANCE
AAAAAAAAADDPAAAA	Brittni             	Mendez                        	JAPAN
AAAAAAAAADEICAAA	Robert              	Flowers                       	MALAYSIA
AAAAAAAAADEPCAAA	Sherman             	Dahl                          	WESTERN SAHARA
AAAAAAAAADFHBAAA	Adam                	Jones                         	ZAMBIA
AAAAAAAAADGGCAAA	Catherine           	Johnson                       	AFGHANISTAN
AAAAAAAAADHPFAAA	Alfredo             	Ramirez                       	PARAGUAY
AAAAAAAAADIIBAAA	Carla               	Williams                      	MALAYSIA
AAAAAAAAADJCGAAA	Robyn               	Vinson                        	LUXEMBOURG
AAAAAAAAADJFCAAA	Ralph               	Hargis                        	IRAQ
AAAAAAAAADJGCAAA	NULL	Armstrong                     	NAURU
AAAAAAAAADJJCAAA	Daniel              	Tan                           	KAZAKHSTAN
AAAAAAAAADJLFAAA	Teresa              	Bannister                     	SUDAN
AAAAAAAAADKOCAAA	NULL	NULL	NULL
AAAAAAAAADLGBAAA	James               	Smith                         	NULL
AAAAAAAAADMDGAAA	Edwin               	Westbrook                     	YEMEN
AAAAAAAAADMMGAAA	Corey               	Hemphill                      	LESOTHO
AAAAAAAAADPAGAAA	Alma                	Donovan                       	BRUNEI DARUSSALAM
AAAAAAAAAEALDAAA	Gary                	Tillman                       	AUSTRIA
AAAAAAAAAEANDAAA	Elsie               	Ritchie                       	ISLE OF MAN
AAAAAAAAAEBAHAAA	Chanelle            	Bull                          	COMOROS
AAAAAAAAAEBHFAAA	Charlotte           	Curtin                        	CAMBODIA
AAAAAAAAAEGJEAAA	Sara                	Peachey                       	HUNGARY
AAAAAAAAAEKABAAA	Ronald              	Martin                        	GUADELOUPE
AAAAAAAAAEKCGAAA	Ronald              	Matos                         	NIGER
AAAAAAAAAELOBAAA	Robert              	Fisk                          	CROATIA
AAAAAAAAAELPAAAA	Jennifer            	Alvarez                       	SINGAPORE
AAAAAAAAAENGHAAA	Matthew             	Harvey                        	ISRAEL
AAAAAAAAAEOMAAAA	Leonard             	Mcewen                        	MARTINIQUE
AAAAAAAAAEPHEAAA	Kathryn             	Green                         	GUINEA
AAAAAAAAAFAPFAAA	Beverly             	Ortega                        	URUGUAY
AAAAAAAAAFDKDAAA	Jaime               	Hall                          	COSTA RICA
AAAAAAAAAFEPFAAA	Karl                	Hargrave                      	JORDAN
AAAAAAAAAFFMAAAA	Ruth                	Anderson                      	BAHRAIN
Time taken: 710.902 seconds, Fetched: 100 row(s)
timediff:716.540410179
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query12.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 07df7410-4526-47ec-b9f0-c4de1f407b5b

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 7abc2454-1ed9-4ae7-b1d3-8924ae3c42f4
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_item_sk, ws_ext_sales_price
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_desc, i_item_id, i_class, i_category, i_current_price, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date, d_date_sk
Query ID = hdfs_20230105084230_2666a04e-eab4-4575-9b25-ab5f8fa33f57
Total jobs = 3
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2023-01-05 08:42:43	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 08:42:46	Dump the side-table for tag: 1 with group count: 30393 into file: file:/tmp/hdfs/07df7410-4526-47ec-b9f0-c4de1f407b5b/hive_2023-01-05_08-42-30_240_4430664075544526128-1/-local-10008/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2023-01-05 08:42:46	End of local task; Time Taken: 2.965 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0124, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0124/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0124
Hadoop job information for Stage-3: number of mappers: 2; number of reducers: 2
2023-01-05 08:43:02,856 Stage-3 map = 0%,  reduce = 0%
2023-01-05 08:43:20,306 Stage-3 map = 5%,  reduce = 0%, Cumulative CPU 36.45 sec
2023-01-05 08:43:25,425 Stage-3 map = 6%,  reduce = 0%, Cumulative CPU 42.45 sec
2023-01-05 08:43:26,450 Stage-3 map = 8%,  reduce = 0%, Cumulative CPU 47.38 sec
2023-01-05 08:43:31,568 Stage-3 map = 9%,  reduce = 0%, Cumulative CPU 54.92 sec
2023-01-05 08:43:32,592 Stage-3 map = 12%,  reduce = 0%, Cumulative CPU 60.38 sec
2023-01-05 08:43:37,711 Stage-3 map = 13%,  reduce = 0%, Cumulative CPU 64.7 sec
2023-01-05 08:43:38,729 Stage-3 map = 17%,  reduce = 0%, Cumulative CPU 70.7 sec
2023-01-05 08:43:43,838 Stage-3 map = 22%,  reduce = 0%, Cumulative CPU 80.07 sec
2023-01-05 08:43:49,973 Stage-3 map = 25%,  reduce = 0%, Cumulative CPU 91.8 sec
2023-01-05 08:43:56,100 Stage-3 map = 29%,  reduce = 0%, Cumulative CPU 102.36 sec
2023-01-05 08:44:02,248 Stage-3 map = 32%,  reduce = 0%, Cumulative CPU 115.21 sec
2023-01-05 08:44:08,366 Stage-3 map = 36%,  reduce = 0%, Cumulative CPU 127.33 sec
2023-01-05 08:44:13,477 Stage-3 map = 37%,  reduce = 0%, Cumulative CPU 132.97 sec
2023-01-05 08:44:14,501 Stage-3 map = 39%,  reduce = 0%, Cumulative CPU 137.37 sec
2023-01-05 08:44:19,598 Stage-3 map = 40%,  reduce = 0%, Cumulative CPU 143.1 sec
2023-01-05 08:44:20,624 Stage-3 map = 43%,  reduce = 0%, Cumulative CPU 147.13 sec
2023-01-05 08:44:25,717 Stage-3 map = 44%,  reduce = 0%, Cumulative CPU 151.33 sec
2023-01-05 08:44:26,737 Stage-3 map = 48%,  reduce = 0%, Cumulative CPU 155.56 sec
2023-01-05 08:44:27,762 Stage-3 map = 66%,  reduce = 0%, Cumulative CPU 156.53 sec
2023-01-05 08:44:31,829 Stage-3 map = 67%,  reduce = 0%, Cumulative CPU 162.12 sec
2023-01-05 08:44:37,946 Stage-3 map = 69%,  reduce = 0%, Cumulative CPU 166.64 sec
2023-01-05 08:44:44,078 Stage-3 map = 71%,  reduce = 8%, Cumulative CPU 173.01 sec
2023-01-05 08:44:45,098 Stage-3 map = 71%,  reduce = 17%, Cumulative CPU 173.48 sec
2023-01-05 08:44:50,206 Stage-3 map = 72%,  reduce = 17%, Cumulative CPU 179.75 sec
2023-01-05 08:44:56,318 Stage-3 map = 73%,  reduce = 17%, Cumulative CPU 186.6 sec
2023-01-05 08:45:02,444 Stage-3 map = 74%,  reduce = 17%, Cumulative CPU 192.51 sec
2023-01-05 08:45:07,544 Stage-3 map = 75%,  reduce = 17%, Cumulative CPU 197.09 sec
2023-01-05 08:45:13,676 Stage-3 map = 77%,  reduce = 17%, Cumulative CPU 201.78 sec
2023-01-05 08:45:19,793 Stage-3 map = 78%,  reduce = 17%, Cumulative CPU 205.64 sec
2023-01-05 08:45:25,891 Stage-3 map = 81%,  reduce = 17%, Cumulative CPU 210.65 sec
2023-01-05 08:45:31,996 Stage-3 map = 82%,  reduce = 17%, Cumulative CPU 217.24 sec
2023-01-05 08:45:38,108 Stage-3 map = 83%,  reduce = 17%, Cumulative CPU 223.34 sec
2023-01-05 08:45:40,150 Stage-3 map = 100%,  reduce = 17%, Cumulative CPU 223.39 sec
2023-01-05 08:45:41,167 Stage-3 map = 100%,  reduce = 58%, Cumulative CPU 228.67 sec
2023-01-05 08:45:42,184 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 230.84 sec
MapReduce Total cumulative CPU time: 3 minutes 50 seconds 840 msec
Ended Job = job_1672890466700_0124
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0125, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0125/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0125
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 08:45:52,020 Stage-4 map = 0%,  reduce = 0%
2023-01-05 08:45:57,103 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 2.11 sec
2023-01-05 08:46:04,215 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 6.09 sec
MapReduce Total cumulative CPU time: 6 seconds 90 msec
Ended Job = job_1672890466700_0125
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0126, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0126/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0126
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 08:46:12,294 Stage-5 map = 0%,  reduce = 0%
2023-01-05 08:46:15,389 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.26 sec
2023-01-05 08:46:19,460 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.05 sec
MapReduce Total cumulative CPU time: 2 seconds 50 msec
Ended Job = job_1672890466700_0126
MapReduce Jobs Launched:
Stage-Stage-3: Map: 2  Reduce: 2   Cumulative CPU: 230.84 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 6.09 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.05 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 minutes 58 seconds 980 msec
OK
AAAAAAAAFKOHBAAA	Now other policies will send vertical policies.	Books                                             	NULL	NULL	29730.92	29.68573830629781473
AAAAAAAAHLGEAAAA	Hands complete very by a schools. Growing, public animals would support british exhibitions. Armed areas select brilliant streets. Broad, basic hours	Books                                             	NULL	NULL	475.54	0.47481732802674330
AAAAAAAANFJNAAAA	Of course common authorities would not preserve just to a ris	Books                                             	NULL	4.17	781.98	0.78079163513133012
AAAAAAAAOJGAAAAA	NULL	Books                                             	NULL	NULL	21048.88	21.01689228993471936
AAAAAAAAPHPCBAAA	NULL	Books                                             	NULL	NULL	61.28	0.06118687357841365
AAAAAAAAAADNAAAA	Then important men think most by a russians. New, radical hundreds stand officially short-term birds; so active years share still charts. Menta	Books                                             	arts                                              	3.38	7936.92	0.80975552523178555
AAAAAAAAAAKAAAAA	Small, political activities help great, bad policies. Therefore square features provide on a machines. Rules make over me	Books                                             	arts                                              	2.42	1349.72	0.13770369709104358
AAAAAAAAABOBBAAA	Still possible dollars will not choose bold issues. Old, obvious thanks shall stimulate only by the schools. Exceptionally independent components will deliver then particularly substantia	Books                                             	arts                                              	2.75	10308.60	1.05172356624539298
AAAAAAAAACIEAAAA	Im	Books                                             	arts                                              	4.02	16872.06	1.72135334702153980
AAAAAAAAACKBAAAA	Clinical, inc initiatives make specially according to a activities.	Books                                             	arts                                              	6.92	1183.26	0.12072079884712994
AAAAAAAAAFKNAAAA	Loans ought to give legal ministers; clothes must not establish. Necessary, slight patients see traditionally advanced demands; differences may challen	Books                                             	arts                                              	2.75	536.06	0.05469093135066889
AAAAAAAAAFLCBAAA	Games can reflect frequently in a patterns. Head lines can make then dramatically bitter meeting	Books                                             	arts                                              	7.64	801.00	0.08172114317778940
AAAAAAAAAGCPAAAA	Here underlying barriers would trouble however open likely flowers; obviously natural managers allow on the others. Special, senior flowers can advance later in a companies. Both outer votes	Books                                             	arts                                              	2.95	4289.82	0.43766416283014296
AAAAAAAAAHAGAAAA	Emotional, good options exploit about christian eyes. Forth small branches s	Books                                             	arts                                              	5.40	1701.00	0.17354265236631681
AAAAAAAAAHLABAAA	Texts reach sometimes. Homes will rescue etc somewhere total households; final insects purchase before then economic members. Only	Books                                             	arts                                              	4.52	11932.09	1.21735834619259562
AAAAAAAAAIJCAAAA	Simply small grounds use exactly effects. Services could kill especially aware, large observers. Civil, relevant years ensure regulations; clear drawings realize actors. Products employ a	Books                                             	arts                                              	1.76	2207.63	0.22523102036652086
AAAAAAAAAINHAAAA	Inches would force once crops. Courts will keep in a lands. Groups lead most long, fresh pupils. Marine patients used to give breasts. Little existing exercises shall look now legal institutions. Ma	Books                                             	arts                                              	0.95	6797.95	0.69355336487572214
AAAAAAAAALMEBAAA	Pressures include other issues. Old, old results shall help 	Books                                             	arts                                              	9.71	237.44	0.02422455460191550
AAAAAAAAAMFCBAAA	Now medium categories may give completely recent little jeans. Mildly regional elements put more logical forms. Sophisticated,	Books                                             	arts                                              	5.00	4874.14	0.49727876755129889
AAAAAAAAAMOIAAAA	Necessarily great children shall not master more. Explicitly apparent writings may not grind g	Books                                             	arts                                              	2.44	667.56	0.06810707408210373
AAAAAAAAANCIAAAA	Previous change	Books                                             	arts                                              	4.95	3438.33	0.35079183298687718
AAAAAAAAANEOAAAA	Then coming qualities show mental, forthcoming passengers. Yet empirical courses permit better heavy countries. Actually new areas might supply about acts. Only urban losses pay. TradiBooks                                             	arts                                              	2.42	1288.10	0.13141698442860240
AAAAAAAAANOEAAAA	Chief cattle develop less within the nations; situations show in the pairs. Public, relevant risks try. Liberal, direct races could pay professional services. Methods could not 	Books                                             	arts                                              	5.35	251.60	0.02566921301314833
AAAAAAAAAOFFBAAA	Once 	Books                                             	arts                                              	6.44	1065.60	0.10871666687921646
AAAAAAAAAONABAAA	Far traditional years might dream of course clever vo	Books                                             	arts                                              	6.38	112.00	0.01142667669901674
AAAAAAAABEEABAAA	Words would advance fo	Books                                             	arts                                              	2.40	6116.46	0.62402509787917820
AAAAAAAABELFBAAA	Weekly shows cannot suppose	Books                                             	arts                                              	8.71	14545.57	1.48399576600818742
AAAAAAAABFDGBAAA	Whole, different improvements used to distinguish as possible scales. Once flat c	Books                                             	arts                                              	0.65	73.78	0.00752732327547728
AAAAAAAABFNMAAAA	As model thousands respond rather. Pounds ought to dedu	Books                                             	arts                                              	1.63	2665.71	0.27196612806549934
AAAAAAAABIDKAAAA	Video-taped, easy buildings replace actually free, formal stars. Large others could not help. Things deal gently goods. Additional, mediterranean minutes describe no	Books                                             	arts                                              	9.36	2814.27	0.28712279851555227
AAAAAAAABIHMAAAA	Very red months used to help until a drugs. As well remaining transactions choose plea	Books                                             	arts                                              	1.99	88.20	0.00899850790047569
AAAAAAAABKDIAAAA	French books undergo so technical figures; beings test then friends. Top, constant proceedings will not face too. Weakly public cuts change organic, resp	Books                                             	arts                                              	2.01	1166.92	0.11905372833586268
AAAAAAAABOFFAAAA	Local, religious hours turn always other prices. Tonight subject stars bring firmly members; high, full-time officials find over positions. Benefits may not relax far so various bonds. Direct feat	Books                                             	arts                                              	9.66	90.50	0.00923316286840192
AAAAAAAABPNFAAAA	Too certain firms could watch just relative examples. Again likely services beat on a lessons. Sure, small laws could spend as quite only countries. Clear hills may not interpret open netw	Books                                             	arts                                              	5.69	42.25	0.00431050973690587
AAAAAAAACABDBAAA	Consumers hear totally organisations. Events must not help lang	Books                                             	arts                                              	9.38	1548.64	0.15799829109969010
AAAAAAAACBACAAAA	Remaining, main passengers go far sure men. 	Books                                             	arts                                              	4.78	3036.34	0.30977924578832592
AAAAAAAACBANAAAA	Upper, emotional sections used to stop as much particular efforts. Legal ties bring rather primarily possible 	Books                                             	arts                                              	6.76	49.68	0.00506854730720671
AAAAAAAACBCEBAAA	Eastern students might not achieve. Recent countries could not live effectively again other stations. Houses leave clearly industrial levels. Proper elect	Books                                             	arts                                              	9.51	2684.71	0.27390458214836826
AAAAAAAACBPDBAAA	Fears can persuade roman margins. English courses give plans. Daily, high representatives would not want between a sections. National studies cannot accept big, unemployed or	Books                                             	arts                                              	1.26	2522.79	0.25738487240636119
AAAAAAAACCKGAAAA	Trades enter ahead other horses. Rich, obvious clients avoid on a agencies. Only obvious provisions may take long, grateful differences. Tenderly extensiv	Books                                             	arts                                              	77.82	357.28	0.03645109866986342
AAAAAAAACCLCAAAA	Multiple, personal attitudes change so. Major, international companies can give scales. Strong women may take there expensive scores	Books                                             	arts                                              	45.80	17578.22	1.79339854360884037
AAAAAAAACCNCBAAA	Grand, black miles shall think definitely pictures. European, personal councils 	Books                                             	arts                                              	0.42	10155.26	1.03607922155764988
AAAAAAAACDAFAAAA	Local, foreign lips take highly more wonderful schools. Artistic, dry goods used to lose only in a designs. Now differen	Books                                             	arts                                              	3.87	2039.60	0.20808794460102279
AAAAAAAACDFGBAAA	In addition select allegations want however readily vital classes. Minor children overcome today crazy parts. Easily final clergy turn yet low hundreds. Reduced photographs sha	Books                                             	arts                                              	5.58	49.36	0.00503589965949524
AAAAAAAACFGJAAAA	External employees find young weeks. Fond, suitable files see no doubt sheer plans; books win low. Corners will not check less active times. Targets could not find wrong, growing 	Books                                             	arts                                              	4.84	92.16	0.00940252254090521
AAAAAAAACFOGBAAA	E	Books                                             	arts                                              	1.04	2932.94	0.29922997462155510
AAAAAAAACIBIBAAA	Important things shall eat for example top times. Terms ought to consider ever little efforts. Expensive services ought to pull rather white applicati	Books                                             	arts                                              	0.79	4374.52	0.44630558708377437
AAAAAAAACIMEAAAA	Pages may take less about a causes. Feelings used to win reason	Books                                             	arts                                              	3.84	1385.20	0.14132350503105352
AAAAAAAACJHHAAAA	Pretty inc companies would take actually sorts. Only medic	Books                                             	arts                                              	5.03	34.56	0.00352594595283945
AAAAAAAACJHHBAAA	Big, various eyes go early here parliamentary years. Practical, special purposes agree in a uses. Previous miles perform raw, past developer	Books                                             	arts                                              	7.86	2771.30	0.28273883157129558
AAAAAAAACKEAAAAA	Legs appear eventually soci	Books                                             	arts                                              	35.27	111.60	0.01138586713937740
AAAAAAAACKGHAAAA	Burning children suffer backwards equations. As basic districts hope well separately relevant days. Ready organisms like 	Books                                             	arts                                              	2.39	7357.00	0.75058982566666242
AAAAAAAACKPGBAAA	Very fine sites understand manufacturing affairs. Young, high rights shall not ensur	Books                                             	arts                                              	53.34	1005.06	0.10254014002780151
AAAAAAAACLIABAAA	Conditions help quite necessary, small patients. Old, interesting problems testify even at a offices. Male, good worlds get so local calls. Already capable objectives may ease 	Books                                             	arts                                              	6.51	4963.52	0.50639766365271065
AAAAAAAACLOABAAA	Also decent methods match particularly. Complaints play regularly over hard states. Centuries find very	Books                                             	arts                                              	1.88	5061.86	0.51643069389004374
AAAAAAAACLPHAAAA	Social grounds must tell thus expo	Books                                             	arts                                              	3.60	17279.02	1.76287299299860992
AAAAAAAACMHIAAAA	Wild, certain rounds want here parliamentary courts. British, white reasons sit all but churches. Stro	Books                                             	arts                                              	6.35	7672.17	0.78274469794549373
AAAAAAAACNGNAAAA	Young, industrial students shall give very difficult friends. Lips suit then dangers. New, big records show for a features; british, dead classes	Books                                             	arts                                              	78.79	8834.23	0.90130259013173836
AAAAAAAADAAKAAAA	Parliamentary relationships require almost slim papers. Important, particular scores would give anywhere across the details. Other railways can imagine much wit	Books                                             	arts                                              	0.56	4523.76	0.46153163378521419
AAAAAAAADCCDAAAA	Glad users understand very almost original jobs. Towns can understand. Supreme, following days work by a parents; german, crucial weapons work sure; fair pictur	Books                                             	arts                                              	7.18	1100.32	0.11225893665591165
AAAAAAAADFJLAAAA	Patterns may try english, criminal difficulties. Cups must progress then prisoners. Again new goals will collect heavily local, other offices. Hours avoid most with a films. Premises can get. Eyes	Books                                             	arts                                              	1.69	2037.43	0.20786655273997934
AAAAAAAADFLJAAAA	Green proposals let successfully legal, central horses; bands form classes. Chief, difficult children used to find of course open lines. Present men can see. Inc, separate losses ask too often	Books                                             	arts                                              	2.41	2865.86	0.29238621147003686
AAAAAAAADHIABAAA	Hundreds see all right to a windows. Other, marginal questions may not kill below dear, grea	Books                                             	arts                                              	3.74	5317.13	0.54247433461288307
AAAAAAAADJFCAAAA	Significant, preliminary boys can remain lightly more pale discussion	Books                                             	arts                                              	2.74	1370.21	0.13979416678356905
AAAAAAAADNHOAAAA	Personnel end to a years. Customers expect originally bizarre, wonderful things. Supporters continue more for a judges. 	Books                                             	arts                                              	8.57	5134.17	0.52380804203384643
AAAAAAAADNPMAAAA	Again personal victims might resist thus so popular cells. Doctors ought to curtail throughout a owners; informal responsibilities s	Books                                             	arts                                              	1.92	5271.95	0.53786489485161899
AAAAAAAADPJABAAA	Even other h	Books                                             	arts                                              	1.86	4169.26	0.42536416155484422
AAAAAAAADPLBBAAA	Furious services operate initially private ingredients. Local, sufficient years shall move ot	Books                                             	arts                                              	3.05	4559.98	0.46522693941055692
AAAAAAAAEBLIBAAA	Claims might not develop la	Books                                             	arts                                              	3.77	40.08	0.00408911787586242
AAAAAAAAEBPGAAAA	Very real boards may take in order. Pages used to escape a bit to a efforts. So fat members operate reasonably officers. Secondly italian areas may not understand too 	Books                                             	arts                                              	3.10	2110.50	0.21532143904709678
AAAAAAAAEDKDAAAA	Times live now to a sales. British years bring all financ	Books                                             	arts                                              	4.24	2120.64	0.21635596138395419
AAAAAAAAEECGAAAA	National, recent flats see seldom. At last vast considerations can think most. Romantic products develop fully long moves. Labour styles offer nece	Books                                             	arts                                              	3.41	2002.60	0.20431306033438333
AAAAAAAAEEOABAAA	About great banks would not like sadly; physical sections ensure; just british theories can put rather on a leaders. So sexual functions need safely particular, redundant men. Agricul	Books                                             	arts                                              	2.51	4386.88	0.44756660247663015
AAAAAAAAEFAEBAAA	Rather legislative rights must n	Books                                             	arts                                              	4.20	13517.21	1.37907846913139403
AAAAAAAAEFMNAAAA	Men teach hands. Strange dev	Books                                             	arts                                              	4.54	2156.28	0.21999209314781988
AAAAAAAAEGAEAAAA	Far injuries pay so various arms. Courses could go anywhere universal possibilities; talks stand since mean, colonial scho	Books                                             	arts                                              	9.57	34736.40	3.54394296864040401
AAAAAAAAEHLEAAAA	Big, good girls look yesterday. Powerful, extensive schools help instead in order 	Books                                             	arts                                              	3.27	388.68	0.03965464910155204
AAAAAAAAEKHEBAAA	Old researchers save just necessary guests; both different workers boost both japanese things. Now high affairs arise curiously in a steps. Easily additional illustrations remain	Books                                             	arts                                              	40.68	361.08	0.03683878948643720
AAAAAAAAELKNAAAA	Interactions cannot try others. Students cannot cut then. Neither 	Books                                             	arts                                              	1.76	3319.38	0.33865609018912680
AAAAAAAAELNCBAAA	Aware, glad employers would meet about worth a courts. Glasses li	Books                                             	arts                                              	0.34	7603.87	0.77577646563707548
AAAAAAAAEMGDBAAA	Visual intentions ensure advanced weapons. Existing historians will lack principally at least limited industries. Famous, par	Books                                             	arts                                              	0.81	4073.58	0.41560251488911277
AAAAAAAAEMLHBAAA	Thus other sales could not reflect rather significant average prices. Royal, large days may leave other belie	Books                                             	arts                                              	9.10	4160.16	0.42443574407304911
AAAAAAAAENDLAAAA	Though running books get issues. Others recognise per a reports. Social characters must not attack more blue, similar re	Books                                             	arts                                              	0.67	179.91	0.01835511968678663
AAAAAAAAENGBBAAA	Serious schools used to tell vividly long unions. New difficulties may prevent almost asleep cases. Complete classes become well diseases. P	Books                                             	arts                                              	2.20	13061.99	1.33263514978383686
AAAAAAAAEONDBAAA	Days must intervene small, economic delegates; likely, british solutions uncover still examples. Common, daily figures last on a services. Existing influences can prevent under the motives. Methods	Books                                             	arts                                              	1.10	2372.86	0.24208842921454351
AAAAAAAAEPCHBAAA	Real plans would stay religious, due months. Other 	Books                                             	arts                                              	2.87	16805.321.71454427199571500
AAAAAAAAEPDDAAAA	Services used to work most new provi	Books                                             	arts                                              	2.84	1748.41	0.17837960542257024
AAAAAAAAEPKAAAAA	Here political studies give once at the qu	Books                                             	arts                                              	1.78	446.29	0.04553224592860878
AAAAAAAAFAAOAAAA	Central things should concentrate educational, bad trends. Groups might not go accused, heavy ages. Only necessary offers sleep even virtually able bodies; popular, new 	Books                                             	arts                                              	1.10	1894.03	0.19323632560927397
AAAAAAAAFACIBAAA	Only	Books                                             	arts                                              	7.40	712.45	0.07268692691262928
AAAAAAAAFBMBAAAA	Years light glasses. Contemporary members might detect even drawings. Private instructions ought to expect well main streets. Children will say well; usually young members ought to ensure enough. 	Books                                             	arts                                              	4.78	58.76	0.00599492431101986
AAAAAAAAFBNPAAAA	Always individual aspects could counteract. Primary, victorian meetings used to bring women. Things might make of course. Mistakes shall increase 	Books                                             	arts                                              	0.79	7268.90	0.74160152015609657
AAAAAAAAFCFBAAAA	Golden estates meet as yet hands. About solid proteins used to tell. Once causal boots imagine frequently new elections; flexible, other ways find re	Books                                             	arts                                              	9.76	6155.64	0.62802239424585209
AAAAAAAAFCKCAAAA	Brilliant, acceptable resources might not pick as. Positive, married parties support only strongly impossible needs. Photogra	Books                                             	arts                                              	2.44	1573.59	0.16054378738219428
AAAAAAAAFDAOAAAA	Procedures shall break right unemployed reasons. Better english days move topics. Ways provide other, other signals. Windows used to influence however ha	Books                                             	arts                                              	7.41	1522.10	0.15529057681761953
AAAAAAAAFJAOAAAA	Writers drive surely; sheets will give. Teeth might include scottish, superb colours; debts would affect just at the advantages; natural programmes shall build other, only eyes. Of course volun	Books                                             	arts                                              	4.60	231.84	0.02365322076696466
AAAAAAAAFMIGAAAA	Intimately alive police would not relieve ther	Books                                             	arts                                              	26.86	2128.71	0.21717929424967799
AAAAAAAAFNKGAAAA	Fundamental, red drivers show in public in a houses. Political houses mean no longer long cities. 	Books                                             	arts                                              	3.12	9373.68	0.95633938250035071
AAAAAAAAFOHLAAAA	Regions like for example then new men. Shops mi	Books                                             	arts                                              	4.62	4536.60	0.46284162064963718
AAAAAAAAFONOAAAA	Years shall clean only minutes. Positions meet high emotional, concerned languages. Local, average relations could act.	Books                                             	arts                                              	1.52	1584.00	0.16160585617180825
Time taken: 231.652 seconds, Fetched: 100 row(s)
timediff:237.142647397
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query13.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 33057a77-39c3-43f9-8294-ff8cb8353b02

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 6199841a-dba4-4eb9-9e63-9ef20e4e53c1
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_cdemo_sk, ss_ext_sales_price, ss_net_profit, ss_ext_wholesale_cost, ss_addr_sk, ss_store_sk, ss_hdemo_sk, ss_quantity, ss_sales_price
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_store_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_demographics, Columns: cd_demo_sk, cd_education_status, cd_marital_status
No Stats for tpcds_bin_partitioned_orc_10@household_demographics, Columns: hd_demo_sk, hd_dep_count
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_state, ca_address_sk, ca_country
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
Query ID = hdfs_20230105084627_37092838-e159-4780-a0ab-d8cf394a75ad
Total jobs = 1
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 08:46:41	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 08:46:45	Processing rows:	200000	Hashtable size:	199999	Memory usage:	13547340percentage:	0.567
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-17

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
timediff:25.656845028
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query14.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 729bdf2b-13d4-4206-992e-5441e520f152

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = fa7e929b-ff4f-47f2-a5f0-1711ba9ba4f9
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_item_sk, ss_quantity, ss_list_price
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_class_id, i_category_id, i_brand_id, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_moy, d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_class_id, i_category_id, i_brand_id, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_item_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_class_id, i_category_id, i_brand_id, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_item_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_class_id, i_category_id, i_brand_id, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_quantity, cs_list_price
No Stats for tpcds_bin_partitioned_orc_10@web_sales, Columns: ws_quantity, ws_list_price
Warning: Map Join MAPJOIN[831][bigTable=?] in task 'Stage-119:MAPRED' is a cross product
Warning: Map Join MAPJOIN[809][bigTable=?] in task 'Stage-117:MAPRED' is a cross product
Warning: Map Join MAPJOIN[820][bigTable=?] in task 'Stage-118:MAPRED' is a cross product
Warning: Shuffle Join JOIN[186][tables = [$hdt$_1, $hdt$_2, $hdt$_0]] in Stage 'Stage-3:MAPRED' is a cross product
Warning: Map Join MAPJOIN[906][bigTable=?] in task 'Stage-140:MAPRED' is a cross product
Warning: Map Join MAPJOIN[917][bigTable=?] in task 'Stage-141:MAPRED' is a cross product
Warning: Map Join MAPJOIN[895][bigTable=?] in task 'Stage-139:MAPRED' is a cross product
Warning: Shuffle Join JOIN[375][tables = [$hdt$_1, $hdt$_2, $hdt$_0]] in Stage 'Stage-43:MAPRED' is a cross product
Warning: Map Join MAPJOIN[981][bigTable=?] in task 'Stage-161:MAPRED' is a cross product
Warning: Map Join MAPJOIN[992][bigTable=?] in task 'Stage-162:MAPRED' is a cross product
Warning: Map Join MAPJOIN[1003][bigTable=?] in task 'Stage-163:MAPRED' is a cross product
Warning: Shuffle Join JOIN[565][tables = [$hdt$_2, $hdt$_3, $hdt$_1]] in Stage 'Stage-81:MAPRED' is a cross product
Query ID = hdfs_20230105084653_37c10fdb-eb50-4e03-b212-fd22aa56dcba
Total jobs = 53
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

2023-01-05 08:48:12	Starting to launch local task to process map join;	maximum memory = 239075328



2023-01-05 08:48:17	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10109/HashTable-Stage-42/MapJoin-mapfile241--.hashtable
2023-01-05 08:48:17	End of local task; Time Taken: 14.985 sec.
SLF4J: Class path contains multiple SLF4J bindings.2023-01-05 08:48:19	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10093/HashTable-Stage-131/MapJoin-mapfile121--.hashtable (893 bytes)
2023-01-05 08:48:19	End of local task; Time Taken: 13.021 sec.







2023-01-05 08:48:28	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10099/HashTable-Stage-33/MapJoin-mapfile171--.hashtable (23389 bytes)
Execution completed successfully
MapredLocal task succeeded
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]

Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded



Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded


Execution completed successfully
MapredLocal task succeeded


Execution completed successfully
MapredLocal task succeeded


SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

2023-01-05 08:49:08	Starting to launch local task to process map join;	maximum memory = 239075328
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 08:49:21	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10123/HashTable-Stage-65/MapJoin-mapfile351--.hashtable
2023-01-05 08:49:21	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10123/HashTable-Stage-65/MapJoin-mapfile351--.hashtable (23389 bytes)



Execution completed successfully
MapredLocal task succeeded



Execution completed successfully
MapredLocal task succeeded





2023-01-05 08:49:44	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10137/HashTable-Stage-88/MapJoin-mapfile461--.hashtable2023-01-05 08:49:44	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10137/HashTable-Stage-88/MapJoin-mapfile461--.hashtable (23389 bytes)
2023-01-05 08:49:44	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10137/HashTable-Stage-88/MapJoin-mapfile481--.hashtable
2023-01-05 08:49:44	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10137/HashTable-Stage-88/MapJoin-mapfile481--.hashtable (23389 bytes)2023-01-05 08:49:44	End of local task; Time Taken: 9.393 sec.

2023-01-05 08:49:48	Dump the side-table for tag: 1 with group count: 101562 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10149/HashTable-Stage-103/MapJoin-mapfile541--.hashtable
2023-01-05 08:49:49	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10149/HashTable-Stage-103/MapJoin-mapfile551--.hashtable
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 53
Number of reduce tasks is set to 0 since there's no reduce operator
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 53
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
Launching Job 3 out of 53
  set mapreduce.job.reduces=<number>
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Execution completed successfully
MapredLocal task succeeded
Execution completed successfully
MapredLocal task succeeded
Launching Job 4 out of 53
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Launching Job 5 out of 53
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Execution completed successfully
MapredLocal task succeeded
Launching Job 6 out of 53
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Execution completed successfully
2023-01-05 08:50:23	Dump the side-table for tag: 1 with group count: 101562 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10153/HashTable-Stage-114/MapJoin-mapfile581--.hashtable2023-01-05 08:50:23	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10153/HashTable-Stage-114/MapJoin-mapfile581--.hashtable (2791926 bytes)
MapredLocal task succeeded
2023-01-05 08:50:23	Dump the side-table for tag: 1 with group count: 1096 into file: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10153/HashTable-Stage-114/MapJoin-mapfile591--.hashtable
2023-01-05 08:50:23	Uploaded 1 File to: file:/tmp/hdfs/729bdf2b-13d4-4206-992e-5441e520f152/hive_2023-01-05_08-46-53_074_5107524574391363326-1/-local-10153/HashTable-Stage-114/MapJoin-mapfile591--.hashtable (23389 bytes)
2023-01-05 08:50:23	End of local task; Time Taken: 7.895 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 53
Launching Job 8 out of 53
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0127, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0127/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0127
Hadoop job information for Stage-131: number of mappers: 4; number of reducers: 0
2023-01-05 08:51:22,090 Stage-131 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0132, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0132/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0132
2023-01-05 08:51:34,031 Stage-131 map = 1%,  reduce = 0%, Cumulative CPU 12.39 sec
2023-01-05 08:51:36,553 Stage-131 map = 5%,  reduce = 0%, Cumulative CPU 39.56 sec
Hadoop job information for Stage-27: number of mappers: 4; number of reducers: 4
2023-01-05 08:51:37,518 Stage-27 map = 0%,  reduce = 0%
2023-01-05 08:51:40,181 Stage-131 map = 6%,  reduce = 0%, Cumulative CPU 40.93 sec
Starting Job = job_1672890466700_0133, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0133/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0133
2023-01-05 08:51:42,253 Stage-131 map = 9%,  reduce = 0%, Cumulative CPU 45.73 sec
Starting Job = job_1672890466700_0134, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0134/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0134
2023-01-05 08:51:45,538 Stage-131 map = 10%,  reduce = 0%, Cumulative CPU 46.93 sec
2023-01-05 08:51:48,892 Stage-131 map = 12%,  reduce = 0%, Cumulative CPU 50.68 sec
2023-01-05 08:51:54,193 Stage-131 map = 14%,  reduce = 0%, Cumulative CPU 55.25 sec
2023-01-05 08:51:55,157 Stage-27 map = 1%,  reduce = 0%, Cumulative CPU 10.98 sec
2023-01-05 08:51:55,210 Stage-131 map = 15%,  reduce = 0%, Cumulative CPU 56.27 sec
2023-01-05 08:51:56,245 Stage-27 map = 2%,  reduce = 0%, Cumulative CPU 24.03 sec
Hadoop job information for Stage-33: number of mappers: 3; number of reducers: 3
2023-01-05 08:51:56,266 Stage-33 map = 0%,  reduce = 0%
2023-01-05 08:51:57,433 Stage-131 map = 16%,  reduce = 0%, Cumulative CPU 57.24 sec
2023-01-05 08:51:59,519 Stage-27 map = 4%,  reduce = 0%, Cumulative CPU 50.42 sec
2023-01-05 08:52:01,150 Stage-131 map = 17%,  reduce = 0%, Cumulative CPU 61.15 sec
2023-01-05 08:52:02,729 Stage-27 map = 5%,  reduce = 0%, Cumulative CPU 54.83 sec
2023-01-05 08:52:04,306 Stage-131 map = 18%,  reduce = 0%, Cumulative CPU 62.21 sec
2023-01-05 08:52:05,913 Stage-27 map = 6%,  reduce = 0%, Cumulative CPU 58.06 sec
2023-01-05 08:52:06,895 Stage-131 map = 19%,  reduce = 0%, Cumulative CPU 65.72 sec
2023-01-05 08:52:09,083 Stage-131 map = 20%,  reduce = 0%, Cumulative CPU 66.83 sec
2023-01-05 08:52:09,123 Stage-27 map = 7%,  reduce = 0%, Cumulative CPU 61.56 sec
2023-01-05 08:52:12,530 Stage-131 map = 21%,  reduce = 0%, Cumulative CPU 70.8 sec
2023-01-05 08:52:12,573 Stage-27 map = 8%,  reduce = 0%, Cumulative CPU 64.42 sec
2023-01-05 08:52:13,767 Stage-33 map = 1%,  reduce = 0%, Cumulative CPU 22.6 sec
2023-01-05 08:52:15,040 Stage-27 map = 9%,  reduce = 0%, Cumulative CPU 68.33 sec
2023-01-05 08:52:15,041 Stage-33 map = 2%,  reduce = 0%, Cumulative CPU 34.05 sec
2023-01-05 08:52:15,100 Stage-131 map = 22%,  reduce = 0%, Cumulative CPU 71.82 sec
2023-01-05 08:52:17,831 Stage-27 map = 10%,  reduce = 0%, Cumulative CPU 72.67 sec
2023-01-05 08:52:19,327 Stage-131 map = 24%,  reduce = 0%, Cumulative CPU 75.84 sec
2023-01-05 08:52:20,771 Stage-27 map = 11%,  reduce = 0%, Cumulative CPU 76.52 sec
2023-01-05 08:52:20,787 Stage-33 map = 4%,  reduce = 0%, Cumulative CPU 42.26 sec
2023-01-05 08:52:23,553 Stage-27 map = 12%,  reduce = 0%, Cumulative CPU 78.75 sec
2023-01-05 08:52:25,054 Stage-131 map = 26%,  reduce = 0%, Cumulative CPU 80.68 sec
2023-01-05 08:52:26,232 Stage-33 map = 6%,  reduce = 0%, Cumulative CPU 50.51 sec
2023-01-05 08:52:26,235 Stage-27 map = 13%,  reduce = 0%, Cumulative CPU 83.59 sec
2023-01-05 08:52:27,629 Stage-131 map = 27%,  reduce = 0%, Cumulative CPU 81.99 sec
2023-01-05 08:52:28,769 Stage-27 map = 14%,  reduce = 0%, Cumulative CPU 85.43 sec
2023-01-05 08:52:30,048 Stage-131 map = 28%,  reduce = 0%, Cumulative CPU 84.5 sec
2023-01-05 08:52:30,066 Stage-27 map = 15%,  reduce = 0%, Cumulative CPU 90.24 sec
2023-01-05 08:52:33,137 Stage-131 map = 29%,  reduce = 0%, Cumulative CPU 86.72 sec
2023-01-05 08:52:33,137 Stage-27 map = 16%,  reduce = 0%, Cumulative CPU 93.81 sec
2023-01-05 08:52:33,155 Stage-33 map = 8%,  reduce = 0%, Cumulative CPU 60.41 sec
2023-01-05 08:52:36,049 Stage-131 map = 30%,  reduce = 0%, Cumulative CPU 89.41 sec
2023-01-05 08:52:36,056 Stage-27 map = 17%,  reduce = 0%, Cumulative CPU 99.58 sec
2023-01-05 08:52:38,548 Stage-33 map = 10%,  reduce = 0%, Cumulative CPU 70.06 sec
2023-01-05 08:52:39,731 Stage-27 map = 18%,  reduce = 0%, Cumulative CPU 103.76 sec
2023-01-05 08:52:39,733 Stage-131 map = 31%,  reduce = 0%, Cumulative CPU 92.47 sec
2023-01-05 08:52:40,824 Stage-27 map = 19%,  reduce = 0%, Cumulative CPU 105.9 sec
2023-01-05 08:52:42,221 Stage-131 map = 32%,  reduce = 0%, Cumulative CPU 95.55 sec
2023-01-05 08:52:42,221 Stage-27 map = 20%,  reduce = 0%, Cumulative CPU 108.94 sec
2023-01-05 08:52:44,453 Stage-33 map = 11%,  reduce = 0%, Cumulative CPU 78.01 sec
2023-01-05 08:52:45,542 Stage-131 map = 33%,  reduce = 0%, Cumulative CPU 98.3 sec
2023-01-05 08:52:45,546 Stage-27 map = 21%,  reduce = 0%, Cumulative CPU 112.75 sec
2023-01-05 08:52:47,641 Stage-27 map = 22%,  reduce = 0%, Cumulative CPU 118.45 sec
2023-01-05 08:52:48,865 Stage-131 map = 34%,  reduce = 0%, Cumulative CPU 102.28 sec
2023-01-05 08:52:50,419 Stage-27 map = 23%,  reduce = 0%, Cumulative CPU 120.44 sec
2023-01-05 08:52:50,439 Stage-33 map = 13%,  reduce = 0%, Cumulative CPU 87.58 sec
2023-01-05 08:52:51,403 Stage-131 map = 35%,  reduce = 0%, Cumulative CPU 103.99 sec
2023-01-05 08:52:51,435 Stage-27 map = 24%,  reduce = 0%, Cumulative CPU 122.54 sec
2023-01-05 08:52:53,980 Stage-27 map = 25%,  reduce = 0%, Cumulative CPU 127.53 sec
2023-01-05 08:52:55,437 Stage-131 map = 37%,  reduce = 0%, Cumulative CPU 107.59 sec
2023-01-05 08:52:56,754 Stage-33 map = 15%,  reduce = 0%, Cumulative CPU 98.91 sec
2023-01-05 08:52:56,756 Stage-27 map = 27%,  reduce = 0%, Cumulative CPU 131.93 sec
2023-01-05 08:53:00,821 Stage-131 map = 39%,  reduce = 0%, Cumulative CPU 113.61 sec
2023-01-05 08:53:02,347 Stage-33 map = 16%,  reduce = 0%, Cumulative CPU 110.37 sec
2023-01-05 08:53:02,364 Stage-27 map = 28%,  reduce = 0%, Cumulative CPU 139.56 sec
2023-01-05 08:53:03,922 Stage-131 map = 40%,  reduce = 0%, Cumulative CPU 115.45 sec
2023-01-05 08:53:03,931 Stage-27 map = 29%,  reduce = 0%, Cumulative CPU 142.72 sec
2023-01-05 08:53:06,359 Stage-131 map = 41%,  reduce = 0%, Cumulative CPU 118.37 sec
2023-01-05 08:53:09,026 Stage-131 map = 42%,  reduce = 0%, Cumulative CPU 121.25 sec
2023-01-05 08:53:09,028 Stage-27 map = 31%,  reduce = 0%, Cumulative CPU 154.11 sec
2023-01-05 08:53:09,035 Stage-33 map = 18%,  reduce = 0%, Cumulative CPU 119.28 sec
2023-01-05 08:53:12,223 Stage-131 map = 43%,  reduce = 0%, Cumulative CPU 124.26 sec
2023-01-05 08:53:13,358 Stage-131 map = 44%,  reduce = 0%, Cumulative CPU 125.42 sec
2023-01-05 08:53:15,838 Stage-27 map = 32%,  reduce = 0%, Cumulative CPU 163.69 sec
Starting Job = job_1672890466700_0128, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0128/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0128
2023-01-05 08:53:15,913 Stage-33 map = 19%,  reduce = 0%, Cumulative CPU 128.4 sec
2023-01-05 08:53:17,279 Stage-27 map = 33%,  reduce = 0%, Cumulative CPU 166.39 sec
2023-01-05 08:53:18,289 Stage-131 map = 45%,  reduce = 0%, Cumulative CPU 129.55 sec
2023-01-05 08:53:19,338 Stage-131 map = 46%,  reduce = 0%, Cumulative CPU 130.78 sec
2023-01-05 08:53:20,525 Stage-27 map = 34%,  reduce = 0%, Cumulative CPU 171.15 sec
2023-01-05 08:53:20,546 Stage-33 map = 20%,  reduce = 0%, Cumulative CPU 139.31 sec
Starting Job = job_1672890466700_0129, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0129/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0129
2023-01-05 08:53:23,031 Stage-131 map = 47%,  reduce = 0%, Cumulative CPU 132.35 sec
2023-01-05 08:53:23,040 Stage-27 map = 35%,  reduce = 0%, Cumulative CPU 176.03 sec
2023-01-05 08:53:24,065 Stage-131 map = 48%,  reduce = 0%, Cumulative CPU 134.97 sec
Starting Job = job_1672890466700_0131, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0131/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0131
Starting Job = job_1672890466700_0130, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0130/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0130
2023-01-05 08:53:25,135 Stage-131 map = 49%,  reduce = 0%, Cumulative CPU 136.21 sec
2023-01-05 08:53:26,150 Stage-27 map = 36%,  reduce = 0%, Cumulative CPU 180.61 sec
2023-01-05 08:53:26,676 Stage-33 map = 22%,  reduce = 0%, Cumulative CPU 149.1 sec
2023-01-05 08:53:29,206 Stage-27 map = 37%,  reduce = 0%, Cumulative CPU 185.3 sec
2023-01-05 08:53:30,227 Stage-131 map = 50%,  reduce = 0%, Cumulative CPU 139.83 sec
2023-01-05 08:53:31,250 Stage-131 map = 51%,  reduce = 0%, Cumulative CPU 141.01 sec
2023-01-05 08:53:32,266 Stage-27 map = 38%,  reduce = 0%, Cumulative CPU 189.96 sec
2023-01-05 08:53:32,827 Stage-33 map = 24%,  reduce = 0%, Cumulative CPU 156.57 sec
2023-01-05 08:53:33,283 Stage-27 map = 39%,  reduce = 0%, Cumulative CPU 192.97 sec
2023-01-05 08:53:33,296 Stage-131 map = 52%,  reduce = 0%, Cumulative CPU 142.79 sec
2023-01-05 08:53:36,323 Stage-27 map = 40%,  reduce = 0%, Cumulative CPU 197.61 sec
2023-01-05 08:53:36,356 Stage-131 map = 53%,  reduce = 0%, Cumulative CPU 145.36 sec
2023-01-05 08:53:37,376 Stage-131 map = 54%,  reduce = 0%, Cumulative CPU 147.12 sec
2023-01-05 08:53:38,357 Stage-27 map = 41%,  reduce = 0%, Cumulative CPU 199.42 sec
2023-01-05 08:53:38,962 Stage-33 map = 25%,  reduce = 0%, Cumulative CPU 163.87 sec
2023-01-05 08:53:39,407 Stage-131 map = 55%,  reduce = 0%, Cumulative CPU 148.92 sec
2023-01-05 08:53:41,522 Stage-27 map = 42%,  reduce = 0%, Cumulative CPU 204.71 sec
2023-01-05 08:53:42,499 Stage-131 map = 56%,  reduce = 0%, Cumulative CPU 150.91 sec
2023-01-05 08:53:44,139 Stage-33 map = 27%,  reduce = 0%, Cumulative CPU 171.75 sec
2023-01-05 08:53:44,667 Stage-27 map = 43%,  reduce = 0%, Cumulative CPU 209.35 sec
2023-01-05 08:53:45,578 Stage-131 map = 57%,  reduce = 0%, Cumulative CPU 154.4 sec
2023-01-05 08:53:45,694 Stage-27 map = 44%,  reduce = 0%, Cumulative CPU 211.86 sec
2023-01-05 08:53:48,631 Stage-131 map = 59%,  reduce = 0%, Cumulative CPU 158.66 sec
2023-01-05 08:53:48,778 Stage-27 map = 45%,  reduce = 0%, Cumulative CPU 217.14 sec
2023-01-05 08:53:50,345 Stage-33 map = 29%,  reduce = 0%, Cumulative CPU 179.85 sec
2023-01-05 08:53:50,837 Stage-27 map = 47%,  reduce = 0%, Cumulative CPU 221.87 sec
2023-01-05 08:53:51,691 Stage-131 map = 60%,  reduce = 0%, Cumulative CPU 160.25 sec
2023-01-05 08:53:53,950 Stage-27 map = 48%,  reduce = 0%, Cumulative CPU 226.45 sec
2023-01-05 08:53:54,768 Stage-131 map = 62%,  reduce = 0%, Cumulative CPU 164.5 sec
2023-01-05 08:53:55,976 Stage-27 map = 49%,  reduce = 0%, Cumulative CPU 228.94 sec
2023-01-05 08:53:56,474 Stage-33 map = 31%,  reduce = 0%, Cumulative CPU 187.84 sec
2023-01-05 08:53:57,004 Stage-27 map = 50%,  reduce = 0%, Cumulative CPU 231.0 sec
2023-01-05 08:53:59,868 Stage-131 map = 63%,  reduce = 0%, Cumulative CPU 166.56 sec
2023-01-05 08:54:00,112 Stage-27 map = 51%,  reduce = 0%, Cumulative CPU 235.55 sec
2023-01-05 08:54:00,894 Stage-131 map = 64%,  reduce = 0%, Cumulative CPU 169.22 sec
2023-01-05 08:54:02,615 Stage-33 map = 33%,  reduce = 0%, Cumulative CPU 195.8 sec
2023-01-05 08:54:03,186 Stage-27 map = 52%,  reduce = 0%, Cumulative CPU 239.13 sec
2023-01-05 08:54:03,983 Stage-131 map = 65%,  reduce = 0%, Cumulative CPU 170.3 sec
2023-01-05 08:54:06,037 Stage-131 map = 66%,  reduce = 0%, Cumulative CPU 171.26 sec
2023-01-05 08:54:06,239 Stage-27 map = 61%,  reduce = 0%, Cumulative CPU 244.87 sec
2023-01-05 08:54:07,062 Stage-131 map = 67%,  reduce = 0%, Cumulative CPU 173.64 sec
2023-01-05 08:54:08,271 Stage-27 map = 62%,  reduce = 0%, Cumulative CPU 246.83 sec
2023-01-05 08:54:08,726 Stage-33 map = 36%,  reduce = 0%, Cumulative CPU 203.42 sec
2023-01-05 08:54:12,307 Stage-131 map = 69%,  reduce = 0%, Cumulative CPU 176.46 sec
2023-01-05 08:54:12,354 Stage-27 map = 63%,  reduce = 0%, Cumulative CPU 251.35 sec
2023-01-05 08:54:15,018 Stage-33 map = 38%,  reduce = 0%, Cumulative CPU 209.94 sec
2023-01-05 08:54:17,568 Stage-27 map = 64%,  reduce = 0%, Cumulative CPU 255.16 sec
2023-01-05 08:54:18,716 Stage-131 map = 71%,  reduce = 0%, Cumulative CPU 181.76 sec
2023-01-05 08:54:20,344 Stage-33 map = 39%,  reduce = 0%, Cumulative CPU 215.78 sec
2023-01-05 08:54:20,721 Stage-27 map = 65%,  reduce = 0%, Cumulative CPU 259.9 sec
2023-01-05 08:54:21,778 Stage-131 map = 72%,  reduce = 0%, Cumulative CPU 185.21 sec
2023-01-05 08:54:22,912 Stage-27 map = 65%,  reduce = 4%, Cumulative CPU 260.79 sec
2023-01-05 08:54:23,936 Stage-27 map = 66%,  reduce = 4%, Cumulative CPU 264.83 sec
2023-01-05 08:54:24,843 Stage-131 map = 73%,  reduce = 0%, Cumulative CPU 189.81 sec
2023-01-05 08:54:26,662 Stage-33 map = 41%,  reduce = 0%, Cumulative CPU 222.58 sec
2023-01-05 08:54:27,935 Stage-131 map = 74%,  reduce = 0%, Cumulative CPU 191.38 sec
2023-01-05 08:54:29,157 Stage-27 map = 66%,  reduce = 6%, Cumulative CPU 268.59 sec
2023-01-05 08:54:29,984 Stage-131 map = 75%,  reduce = 0%, Cumulative CPU 192.82 sec
2023-01-05 08:54:30,184 Stage-27 map = 68%,  reduce = 6%, Cumulative CPU 273.73 sec
2023-01-05 08:54:31,006 Stage-131 map = 77%,  reduce = 0%, Cumulative CPU 195.66 sec
2023-01-05 08:54:31,196 Stage-27 map = 68%,  reduce = 8%, Cumulative CPU 274.35 sec
2023-01-05 08:54:32,803 Stage-33 map = 42%,  reduce = 0%, Cumulative CPU 229.9 sec
2023-01-05 08:54:35,330 Stage-27 map = 69%,  reduce = 8%, Cumulative CPU 279.9 sec
2023-01-05 08:54:36,116 Stage-131 map = 78%,  reduce = 0%, Cumulative CPU 198.37 sec
2023-01-05 08:54:36,342 Stage-27 map = 70%,  reduce = 8%, Cumulative CPU 282.36 sec
2023-01-05 08:54:37,133 Stage-131 map = 79%,  reduce = 0%, Cumulative CPU 201.32 sec
2023-01-05 08:54:39,017 Stage-33 map = 44%,  reduce = 0%, Cumulative CPU 237.78 sec
2023-01-05 08:54:39,182 Stage-131 map = 80%,  reduce = 0%, Cumulative CPU 202.49 sec
2023-01-05 08:54:41,546 Stage-27 map = 71%,  reduce = 8%, Cumulative CPU 287.51 sec
2023-01-05 08:54:42,241 Stage-131 map = 81%,  reduce = 0%, Cumulative CPU 204.93 sec
2023-01-05 08:54:42,629 Stage-27 map = 80%,  reduce = 8%, Cumulative CPU 290.6 sec
2023-01-05 08:54:43,660 Stage-27 map = 89%,  reduce = 8%, Cumulative CPU 292.37 sec
2023-01-05 08:54:44,192 Stage-33 map = 45%,  reduce = 0%, Cumulative CPU 240.44 sec
2023-01-05 08:54:45,210 Stage-33 map = 46%,  reduce = 0%, Cumulative CPU 246.44 sec
2023-01-05 08:54:45,501 Stage-131 map = 82%,  reduce = 0%, Cumulative CPU 207.67 sec
2023-01-05 08:54:46,773 Stage-27 map = 89%,  reduce = 17%, Cumulative CPU 292.52 sec
2023-01-05 08:54:47,852 Stage-27 map = 89%,  reduce = 21%, Cumulative CPU 295.03 sec
2023-01-05 08:54:48,594 Stage-131 map = 84%,  reduce = 0%, Cumulative CPU 210.53 sec
2023-01-05 08:54:48,876 Stage-27 map = 89%,  reduce = 25%, Cumulative CPU 295.06 sec
2023-01-05 08:54:50,370 Stage-33 map = 48%,  reduce = 0%, Cumulative CPU 256.45 sec
2023-01-05 08:54:51,677 Stage-131 map = 85%,  reduce = 0%, Cumulative CPU 213.82 sec
2023-01-05 08:54:54,016 Stage-27 map = 90%,  reduce = 25%, Cumulative CPU 297.81 sec
2023-01-05 08:54:54,760 Stage-131 map = 87%,  reduce = 0%, Cumulative CPU 216.78 sec
2023-01-05 08:54:55,785 Stage-131 map = 88%,  reduce = 0%, Cumulative CPU 218.44 sec
2023-01-05 08:54:56,452 Stage-33 map = 50%,  reduce = 0%, Cumulative CPU 265.8 sec
2023-01-05 08:54:58,020 Stage-131 map = 89%,  reduce = 0%, Cumulative CPU 219.32 sec
2023-01-05 08:54:59,035 Stage-131 map = 90%,  reduce = 0%, Cumulative CPU 220.34 sec
2023-01-05 08:55:00,148 Stage-27 map = 91%,  reduce = 25%, Cumulative CPU 300.1 sec
2023-01-05 08:55:01,070 Stage-131 map = 91%,  reduce = 0%, Cumulative CPU 224.46 sec
2023-01-05 08:55:02,591 Stage-33 map = 52%,  reduce = 0%, Cumulative CPU 274.86 sec
2023-01-05 08:55:03,118 Stage-131 map = 92%,  reduce = 0%, Cumulative CPU 225.51 sec
2023-01-05 08:55:04,275 Stage-27 map = 100%,  reduce = 25%, Cumulative CPU 302.59 sec
2023-01-05 08:55:05,301 Stage-27 map = 100%,  reduce = 81%, Cumulative CPU 307.27 sec
2023-01-05 08:55:06,243 Stage-131 map = 93%,  reduce = 0%, Cumulative CPU 227.82 sec
2023-01-05 08:55:06,318 Stage-27 map = 100%,  reduce = 100%, Cumulative CPU 309.12 sec
2023-01-05 08:55:07,261 Stage-131 map = 94%,  reduce = 0%, Cumulative CPU 230.09 sec
MapReduce Total cumulative CPU time: 5 minutes 9 seconds 120 msec
Ended Job = job_1672890466700_0132
2023-01-05 08:55:08,715 Stage-33 map = 55%,  reduce = 0%, Cumulative CPU 285.14 sec
Launching Job 9 out of 53
Number of reduce tasks is set to 0 since there's no reduce operator
2023-01-05 08:55:12,351 Stage-131 map = 95%,  reduce = 0%, Cumulative CPU 231.84 sec
2023-01-05 08:55:14,846 Stage-33 map = 58%,  reduce = 0%, Cumulative CPU 294.16 sec
2023-01-05 08:55:18,456 Stage-131 map = 96%,  reduce = 0%, Cumulative CPU 235.98 sec
2023-01-05 08:55:19,507 Stage-131 map = 97%,  reduce = 0%, Cumulative CPU 237.93 sec
Hadoop job information for Stage-42: number of mappers: 9; number of reducers: 1
2023-01-05 08:55:20,647 Stage-42 map = 0%,  reduce = 0%
2023-01-05 08:55:21,017 Stage-33 map = 71%,  reduce = 0%, Cumulative CPU 303.48 sec
2023-01-05 08:55:22,031 Stage-33 map = 82%,  reduce = 0%, Cumulative CPU 304.67 sec
2023-01-05 08:55:22,567 Stage-131 map = 98%,  reduce = 0%, Cumulative CPU 239.05 sec
2023-01-05 08:55:25,826 Stage-131 map = 99%,  reduce = 0%, Cumulative CPU 240.76 sec
2023-01-05 08:55:27,143 Stage-33 map = 83%,  reduce = 0%, Cumulative CPU 308.31 sec
Starting Job = job_1672890466700_0135, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0135/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0135
2023-01-05 08:55:35,043 Stage-131 map = 100%,  reduce = 0%, Cumulative CPU 242.84 sec
MapReduce Total cumulative CPU time: 4 minutes 2 seconds 840 msec
Ended Job = job_1672890466700_0127
2023-01-05 08:55:36,615 Stage-33 map = 83%,  reduce = 7%, Cumulative CPU 311.0 sec
Launching Job 10 out of 53
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 08:55:38,717 Stage-33 map = 83%,  reduce = 15%, Cumulative CPU 312.86 sec
2023-01-05 08:55:45,017 Stage-33 map = 84%,  reduce = 15%, Cumulative CPU 314.3 sec
2023-01-05 08:55:48,212 Stage-33 map = 84%,  reduce = 22%, Cumulative CPU 314.93 sec
Starting Job = job_1672890466700_0136, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0136/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0136
2023-01-05 08:55:48,336 Stage-42 map = 1%,  reduce = 0%, Cumulative CPU 22.51 sec
2023-01-05 08:56:00,584 Stage-42 map = 2%,  reduce = 0%, Cumulative CPU 92.87 sec
2023-01-05 08:56:03,119 Stage-33 map = 85%,  reduce = 22%, Cumulative CPU 320.71 sec
2023-01-05 08:56:06,704 Stage-42 map = 3%,  reduce = 0%, Cumulative CPU 146.22 sec
Hadoop job information for Stage-2: number of mappers: 9; number of reducers: 1
2023-01-05 08:56:10,322 Stage-2 map = 0%,  reduce = 0%
2023-01-05 08:56:15,071 Stage-42 map = 4%,  reduce = 0%, Cumulative CPU 211.24 sec
2023-01-05 08:56:20,956 Stage-33 map = 86%,  reduce = 22%, Cumulative CPU 328.51 sec
2023-01-05 08:56:27,229 Stage-33 map = 87%,  reduce = 22%, Cumulative CPU 332.15 sec
2023-01-05 08:56:32,007 Stage-42 map = 5%,  reduce = 0%, Cumulative CPU 251.01 sec
2023-01-05 08:56:39,259 Stage-33 map = 88%,  reduce = 22%, Cumulative CPU 339.98 sec
2023-01-05 08:56:44,550 Stage-33 map = 100%,  reduce = 22%, Cumulative CPU 343.21 sec
2023-01-05 08:56:48,753 Stage-33 map = 100%,  reduce = 63%, Cumulative CPU 348.68 sec
2023-01-05 08:56:54,047 Stage-33 map = 100%,  reduce = 74%, Cumulative CPU 350.29 sec
2023-01-05 08:56:59,305 Stage-42 map = 6%,  reduce = 0%, Cumulative CPU 311.48 sec
2023-01-05 08:57:10,414 Stage-2 map = 0%,  reduce = 0%, Cumulative CPU 70.66 sec
Hadoop job information for Stage-12: number of mappers: 9; number of reducers: 1
2023-01-05 09:07:04,708 Stage-12 map = 0%,  reduce = 0%
2023-01-05 09:07:47,788 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:07:47,788 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:07:47,889 Stage-12 map = 1%,  reduce = 0%, Cumulative CPU 124.85 sec
2023-01-05 09:07:48,984 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:08:11,105 Stage-12 map = 2%,  reduce = 0%, Cumulative CPU 217.66 sec
Hadoop job information for Stage-38: number of mappers: 2; number of reducers: 2
2023-01-05 09:08:14,135 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:08:30,489 Stage-12 map = 3%,  reduce = 0%, Cumulative CPU 339.6 sec
Hadoop job information for Stage-153: number of mappers: 3; number of reducers: 0
2023-01-05 09:08:47,309 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:08:47,874 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:08:47,947 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:08:49,267 Stage-42 map = 0%,  reduce = 0%
Hadoop job information for Stage-50: number of mappers: 9; number of reducers: 1
2023-01-05 09:08:50,811 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:08:52,302 Stage-12 map = 4%,  reduce = 0%, Cumulative CPU 399.68 sec
2023-01-05 09:09:00,623 Stage-38 map = 1%,  reduce = 0%, Cumulative CPU 40.3 sec
2023-01-05 09:09:07,240 Stage-12 map = 5%,  reduce = 0%, Cumulative CPU 469.57 sec
2023-01-05 09:09:18,026 Stage-38 map = 2%,  reduce = 0%, Cumulative CPU 52.74 sec
2023-01-05 09:09:20,150 Stage-38 map = 1%,  reduce = 0%, Cumulative CPU 27.26 sec
2023-01-05 09:09:22,208 Stage-12 map = 6%,  reduce = 0%, Cumulative CPU 533.15 sec
2023-01-05 09:09:47,495 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:09:48,409 Stage-12 map = 7%,  reduce = 0%, Cumulative CPU 604.55 sec
2023-01-05 09:09:48,880 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:09:48,935 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:09:49,370 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:09:51,809 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:10:19,506 Stage-12 map = 8%,  reduce = 0%, Cumulative CPU 696.63 sec
2023-01-05 09:10:48,122 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:10:49,679 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:10:49,680 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:10:50,175 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:10:52,510 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:10:52,797 Stage-12 map = 9%,  reduce = 0%, Cumulative CPU 791.65 sec
2023-01-05 09:11:23,036 Stage-12 map = 10%,  reduce = 0%, Cumulative CPU 877.98 sec
2023-01-05 09:11:48,449 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:11:50,506 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:11:50,508 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:11:51,146 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:11:52,102 Stage-12 map = 11%,  reduce = 0%, Cumulative CPU 957.39 sec
2023-01-05 09:11:52,935 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:12:33,706 Stage-12 map = 12%,  reduce = 0%, Cumulative CPU 1077.1 sec
2023-01-05 09:12:48,831 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:12:51,406 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:12:51,407 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:12:51,919 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:12:53,321 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:13:16,090 Stage-12 map = 13%,  reduce = 0%, Cumulative CPU 1196.82 sec
2023-01-05 09:13:48,919 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:13:52,264 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:13:52,265 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:13:52,720 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:13:53,515 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:13:54,412 Stage-12 map = 14%,  reduce = 0%, Cumulative CPU 1309.2 sec
2023-01-05 09:14:28,921 Stage-12 map = 15%,  reduce = 0%, Cumulative CPU 1404.93 sec
2023-01-05 09:14:49,292 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:14:53,067 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:14:53,066 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:14:53,553 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:14:54,004 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:15:04,513 Stage-12 map = 16%,  reduce = 0%, Cumulative CPU 1501.86 sec
2023-01-05 09:15:36,012 Stage-12 map = 17%,  reduce = 0%, Cumulative CPU 1590.9 sec
2023-01-05 09:15:49,492 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:15:53,885 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:15:53,885 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:15:54,343 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:15:54,703 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:16:13,519 Stage-12 map = 18%,  reduce = 0%, Cumulative CPU 1701.99 sec
2023-01-05 09:16:42,814 Stage-12 map = 19%,  reduce = 0%, Cumulative CPU 1781.66 sec
2023-01-05 09:16:50,190 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:16:54,756 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:16:54,756 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:16:55,152 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:16:55,299 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:17:12,113 Stage-12 map = 20%,  reduce = 0%, Cumulative CPU 1865.67 sec
2023-01-05 09:17:39,001 Stage-12 map = 21%,  reduce = 0%, Cumulative CPU 1945.99 sec
2023-01-05 09:17:50,508 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:17:55,315 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:17:55,572 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:17:55,572 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:17:56,156 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:18:05,939 Stage-12 map = 22%,  reduce = 0%, Cumulative CPU 2021.53 sec
2023-01-05 09:18:30,803 Stage-12 map = 23%,  reduce = 0%, Cumulative CPU 2093.41 sec
2023-01-05 09:18:51,446 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:18:54,601 Stage-12 map = 24%,  reduce = 0%, Cumulative CPU 2162.76 sec
2023-01-05 09:18:55,616 Stage-50 map = 0%,  reduce = 0%
2023-01-05 09:18:56,401 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:18:56,403 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:18:56,969 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:19:19,505 Stage-12 map = 25%,  reduce = 0%, Cumulative CPU 2231.7 sec
2023-01-05 09:19:34,237 Stage-12 map = 23%,  reduce = 0%, Cumulative CPU 2024.71 sec
2023-01-05 09:19:52,103 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:19:56,224 Stage-12 map = 24%,  reduce = 0%, Cumulative CPU 2084.98 sec
2023-01-05 09:19:56,328 Stage-50 map = 0%,  reduce = 0%, Cumulative CPU 57.59 sec
2023-01-05 09:19:57,309 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:19:57,310 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:19:57,875 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:20:15,818 Stage-12 map = 25%,  reduce = 0%, Cumulative CPU 2145.15 sec
2023-01-05 09:20:43,724 Stage-12 map = 26%,  reduce = 0%, Cumulative CPU 2220.97 sec
2023-01-05 09:20:52,315 Stage-153 map = 0%,  reduce = 0%
2023-01-05 09:20:56,592 Stage-50 map = 0%,  reduce = 0%, Cumulative CPU 57.59 sec
2023-01-05 09:20:58,127 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:20:58,127 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:20:58,666 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:21:09,012 Stage-12 map = 27%,  reduce = 0%, Cumulative CPU 2291.45 sec
2023-01-05 09:21:09,304 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:21:29,264 Stage-12 map = 31%,  reduce = 0%, Cumulative CPU 2349.56 sec
2023-01-05 09:21:39,500 Stage-12 map = 32%,  reduce = 0%, Cumulative CPU 2376.0 sec
2023-01-05 09:21:43,798 Stage-153 map = 1%,  reduce = 0%, Cumulative CPU 23.31 sec
2023-01-05 09:21:44,923 Stage-12 map = 36%,  reduce = 0%, Cumulative CPU 2395.06 sec
2023-01-05 09:21:50,053 Stage-153 map = 2%,  reduce = 0%, Cumulative CPU 30.62 sec
2023-01-05 09:21:51,094 Stage-153 map = 3%,  reduce = 0%, Cumulative CPU 34.39 sec
2023-01-05 09:21:57,092 Stage-50 map = 0%,  reduce = 0%, Cumulative CPU 97.98 sec
2023-01-05 09:21:57,393 Stage-153 map = 4%,  reduce = 0%, Cumulative CPU 39.84 sec
2023-01-05 09:21:58,471 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:21:59,097 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:21:59,586 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:22:02,606 Stage-153 map = 5%,  reduce = 0%, Cumulative CPU 43.78 sec
2023-01-05 09:22:04,891 Stage-12 map = 36%,  reduce = 7%, Cumulative CPU 2431.2 sec
2023-01-05 09:22:05,592 Stage-50 map = 1%,  reduce = 0%, Cumulative CPU 108.97 sec
2023-01-05 09:22:07,988 Stage-153 map = 6%,  reduce = 0%, Cumulative CPU 47.44 sec
2023-01-05 09:22:10,018 Stage-153 map = 7%,  reduce = 0%, Cumulative CPU 51.37 sec
2023-01-05 09:22:10,293 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:22:15,292 Stage-153 map = 8%,  reduce = 0%, Cumulative CPU 54.96 sec
2023-01-05 09:22:20,491 Stage-153 map = 9%,  reduce = 0%, Cumulative CPU 62.03 sec
2023-01-05 09:22:21,505 Stage-153 map = 10%,  reduce = 0%, Cumulative CPU 64.23 sec
2023-01-05 09:22:26,691 Stage-153 map = 11%,  reduce = 0%, Cumulative CPU 69.24 sec
2023-01-05 09:22:32,900 Stage-153 map = 12%,  reduce = 0%, Cumulative CPU 77.2 sec
2023-01-05 09:22:33,914 Stage-153 map = 13%,  reduce = 0%, Cumulative CPU 79.99 sec
2023-01-05 09:22:34,002 Stage-12 map = 37%,  reduce = 7%, Cumulative CPU 2485.23 sec
2023-01-05 09:22:39,114 Stage-153 map = 14%,  reduce = 0%, Cumulative CPU 84.82 sec
2023-01-05 09:22:44,290 Stage-153 map = 15%,  reduce = 0%, Cumulative CPU 89.29 sec
2023-01-05 09:22:50,503 Stage-153 map = 16%,  reduce = 0%, Cumulative CPU 97.33 sec
2023-01-05 09:22:56,689 Stage-153 map = 17%,  reduce = 0%, Cumulative CPU 105.0 sec
2023-01-05 09:22:57,703 Stage-153 map = 18%,  reduce = 0%, Cumulative CPU 107.57 sec
2023-01-05 09:22:58,562 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:22:59,206 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:22:59,875 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:23:02,999 Stage-153 map = 19%,  reduce = 0%, Cumulative CPU 116.0 sec
2023-01-05 09:23:06,104 Stage-50 map = 1%,  reduce = 0%, Cumulative CPU 171.02 sec
2023-01-05 09:23:09,308 Stage-153 map = 20%,  reduce = 0%, Cumulative CPU 123.74 sec
2023-01-05 09:23:10,443 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:23:15,542 Stage-153 map = 21%,  reduce = 0%, Cumulative CPU 131.93 sec
2023-01-05 09:23:21,806 Stage-12 map = 38%,  reduce = 7%, Cumulative CPU 2590.6 sec
2023-01-05 09:23:22,834 Stage-153 map = 22%,  reduce = 0%, Cumulative CPU 142.16 sec
2023-01-05 09:23:33,201 Stage-153 map = 23%,  reduce = 0%, Cumulative CPU 155.04 sec
2023-01-05 09:23:40,507 Stage-153 map = 24%,  reduce = 0%, Cumulative CPU 165.88 sec
2023-01-05 09:23:46,742 Stage-153 map = 25%,  reduce = 0%, Cumulative CPU 174.18 sec
2023-01-05 09:23:49,911 Stage-50 map = 2%,  reduce = 0%, Cumulative CPU 219.6 sec
2023-01-05 09:23:57,012 Stage-12 map = 39%,  reduce = 7%, Cumulative CPU 2665.44 sec
2023-01-05 09:23:58,147 Stage-153 map = 26%,  reduce = 0%, Cumulative CPU 187.44 sec
2023-01-05 09:23:58,668 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:23:59,389 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:23:59,988 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:24:04,358 Stage-153 map = 27%,  reduce = 0%, Cumulative CPU 199.87 sec
2023-01-05 09:24:10,596 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:24:15,705 Stage-153 map = 28%,  reduce = 0%, Cumulative CPU 213.7 sec
2023-01-05 09:24:21,914 Stage-153 map = 29%,  reduce = 0%, Cumulative CPU 221.36 sec
2023-01-05 09:24:27,010 Stage-12 map = 40%,  reduce = 7%, Cumulative CPU 2730.34 sec
2023-01-05 09:24:28,038 Stage-12 map = 43%,  reduce = 7%, Cumulative CPU 2734.24 sec
2023-01-05 09:24:29,159 Stage-153 map = 30%,  reduce = 0%, Cumulative CPU 232.07 sec
2023-01-05 09:24:30,082 Stage-12 map = 43%,  reduce = 11%, Cumulative CPU 2738.9 sec
2023-01-05 09:24:33,120 Stage-12 map = 44%,  reduce = 11%, Cumulative CPU 2745.42 sec
2023-01-05 09:24:34,230 Stage-153 map = 31%,  reduce = 0%, Cumulative CPU 237.18 sec
2023-01-05 09:24:41,388 Stage-153 map = 32%,  reduce = 0%, Cumulative CPU 247.68 sec
2023-01-05 09:24:46,478 Stage-153 map = 33%,  reduce = 0%, Cumulative CPU 255.49 sec
2023-01-05 09:24:50,740 Stage-50 map = 2%,  reduce = 0%, Cumulative CPU 283.76 sec
2023-01-05 09:24:51,487 Stage-12 map = 45%,  reduce = 11%, Cumulative CPU 2782.15 sec
2023-01-05 09:24:52,574 Stage-153 map = 34%,  reduce = 0%, Cumulative CPU 263.13 sec
2023-01-05 09:24:56,585 Stage-12 map = 49%,  reduce = 11%, Cumulative CPU 2791.86 sec
2023-01-05 09:24:58,682 Stage-153 map = 35%,  reduce = 0%, Cumulative CPU 271.03 sec
2023-01-05 09:24:58,682 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:24:59,627 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:24:59,660 Stage-12 map = 49%,  reduce = 15%, Cumulative CPU 2798.14 sec
2023-01-05 09:25:00,007 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:25:04,833 Stage-153 map = 36%,  reduce = 0%, Cumulative CPU 278.17 sec
2023-01-05 09:25:09,908 Stage-153 map = 37%,  reduce = 0%, Cumulative CPU 283.21 sec
2023-01-05 09:25:10,880 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:25:16,010 Stage-153 map = 38%,  reduce = 0%, Cumulative CPU 290.97 sec
2023-01-05 09:25:17,050 Stage-153 map = 39%,  reduce = 0%, Cumulative CPU 293.47 sec
2023-01-05 09:25:23,134 Stage-153 map = 40%,  reduce = 0%, Cumulative CPU 301.01 sec
2023-01-05 09:25:28,228 Stage-153 map = 41%,  reduce = 0%, Cumulative CPU 306.09 sec
2023-01-05 09:25:33,200 Stage-12 map = 50%,  reduce = 15%, Cumulative CPU 2844.75 sec
2023-01-05 09:25:34,368 Stage-153 map = 42%,  reduce = 0%, Cumulative CPU 313.49 sec
2023-01-05 09:25:35,386 Stage-153 map = 43%,  reduce = 0%, Cumulative CPU 315.74 sec
2023-01-05 09:25:41,493 Stage-153 map = 44%,  reduce = 0%, Cumulative CPU 322.81 sec
2023-01-05 09:25:46,588 Stage-153 map = 45%,  reduce = 0%, Cumulative CPU 328.22 sec
2023-01-05 09:25:50,980 Stage-50 map = 2%,  reduce = 0%, Cumulative CPU 351.81 sec
2023-01-05 09:25:51,689 Stage-153 map = 46%,  reduce = 0%, Cumulative CPU 332.84 sec
2023-01-05 09:25:52,702 Stage-153 map = 47%,  reduce = 0%, Cumulative CPU 335.45 sec
2023-01-05 09:25:57,802 Stage-153 map = 48%,  reduce = 0%, Cumulative CPU 340.09 sec
2023-01-05 09:25:58,810 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:25:58,817 Stage-153 map = 49%,  reduce = 0%, Cumulative CPU 342.36 sec
2023-01-05 09:25:59,848 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:26:00,039 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:26:03,932 Stage-153 map = 50%,  reduce = 0%, Cumulative CPU 346.96 sec
2023-01-05 09:26:04,963 Stage-153 map = 51%,  reduce = 0%, Cumulative CPU 351.76 sec
2023-01-05 09:26:10,061 Stage-153 map = 52%,  reduce = 0%, Cumulative CPU 354.12 sec
2023-01-05 09:26:11,070 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:26:11,074 Stage-153 map = 53%,  reduce = 0%, Cumulative CPU 358.87 sec
2023-01-05 09:26:14,969 Stage-12 map = 51%,  reduce = 15%, Cumulative CPU 2893.82 sec
2023-01-05 09:26:16,182 Stage-153 map = 54%,  reduce = 0%, Cumulative CPU 363.97 sec
2023-01-05 09:26:17,199 Stage-153 map = 55%,  reduce = 0%, Cumulative CPU 366.47 sec
2023-01-05 09:26:22,302 Stage-153 map = 56%,  reduce = 0%, Cumulative CPU 371.34 sec
2023-01-05 09:26:23,331 Stage-153 map = 57%,  reduce = 0%, Cumulative CPU 373.58 sec
2023-01-05 09:26:28,430 Stage-153 map = 58%,  reduce = 0%, Cumulative CPU 378.14 sec
2023-01-05 09:26:29,458 Stage-153 map = 59%,  reduce = 0%, Cumulative CPU 380.37 sec
2023-01-05 09:26:34,542 Stage-153 map = 60%,  reduce = 0%, Cumulative CPU 385.46 sec
2023-01-05 09:26:35,581 Stage-153 map = 61%,  reduce = 0%, Cumulative CPU 387.48 sec
2023-01-05 09:26:40,862 Stage-153 map = 62%,  reduce = 0%, Cumulative CPU 392.74 sec
2023-01-05 09:26:45,493 Stage-12 map = 52%,  reduce = 15%, Cumulative CPU 2926.14 sec
2023-01-05 09:26:45,970 Stage-153 map = 63%,  reduce = 0%, Cumulative CPU 397.23 sec
2023-01-05 09:26:48,006 Stage-153 map = 64%,  reduce = 0%, Cumulative CPU 401.47 sec
2023-01-05 09:26:51,076 Stage-50 map = 3%,  reduce = 0%, Cumulative CPU 420.82 sec
2023-01-05 09:26:54,115 Stage-153 map = 65%,  reduce = 0%, Cumulative CPU 407.98 sec
2023-01-05 09:26:58,198 Stage-153 map = 66%,  reduce = 0%, Cumulative CPU 411.01 sec
2023-01-05 09:26:58,972 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:26:59,956 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:27:00,256 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:27:04,286 Stage-153 map = 67%,  reduce = 0%, Cumulative CPU 418.49 sec
2023-01-05 09:27:05,298 Stage-153 map = 68%,  reduce = 0%, Cumulative CPU 423.43 sec
2023-01-05 09:27:05,871 Stage-12 map = 56%,  reduce = 15%, Cumulative CPU 2953.28 sec
2023-01-05 09:27:10,403 Stage-153 map = 69%,  reduce = 0%, Cumulative CPU 425.86 sec
2023-01-05 09:27:11,190 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:27:11,440 Stage-153 map = 70%,  reduce = 0%, Cumulative CPU 431.19 sec
2023-01-05 09:27:11,988 Stage-12 map = 56%,  reduce = 19%, Cumulative CPU 2953.34 sec
2023-01-05 09:27:16,551 Stage-153 map = 72%,  reduce = 0%, Cumulative CPU 436.82 sec
2023-01-05 09:27:22,665 Stage-153 map = 74%,  reduce = 0%, Cumulative CPU 445.81 sec
2023-01-05 09:27:28,785 Stage-153 map = 76%,  reduce = 0%, Cumulative CPU 454.93 sec
2023-01-05 09:27:34,902 Stage-153 map = 78%,  reduce = 0%, Cumulative CPU 464.61 sec
2023-01-05 09:27:41,008 Stage-153 map = 80%,  reduce = 0%, Cumulative CPU 474.13 sec
2023-01-05 09:27:47,148 Stage-153 map = 81%,  reduce = 0%, Cumulative CPU 482.77 sec
2023-01-05 09:27:51,226 Stage-50 map = 3%,  reduce = 0%, Cumulative CPU 501.8 sec
2023-01-05 09:27:53,284 Stage-153 map = 82%,  reduce = 0%, Cumulative CPU 490.55 sec
2023-01-05 09:27:59,224 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:27:59,399 Stage-153 map = 83%,  reduce = 0%, Cumulative CPU 501.7 sec
2023-01-05 09:28:00,239 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:28:00,415 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:28:05,510 Stage-153 map = 84%,  reduce = 0%, Cumulative CPU 509.65 sec
2023-01-05 09:28:11,505 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:28:11,633 Stage-153 map = 85%,  reduce = 0%, Cumulative CPU 517.34 sec
2023-01-05 09:28:12,193 Stage-12 map = 56%,  reduce = 19%, Cumulative CPU 2953.78 sec
2023-01-05 09:28:16,789 Stage-153 map = 86%,  reduce = 0%, Cumulative CPU 521.21 sec
2023-01-05 09:28:20,890 Stage-50 map = 4%,  reduce = 0%, Cumulative CPU 545.3 sec
2023-01-05 09:28:23,000 Stage-153 map = 87%,  reduce = 0%, Cumulative CPU 527.47 sec
2023-01-05 09:28:29,304 Stage-153 map = 88%,  reduce = 0%, Cumulative CPU 532.09 sec
2023-01-05 09:28:35,693 Stage-153 map = 89%,  reduce = 0%, Cumulative CPU 537.82 sec
2023-01-05 09:28:42,027 Stage-153 map = 90%,  reduce = 0%, Cumulative CPU 542.77 sec
2023-01-05 09:28:48,406 Stage-153 map = 91%,  reduce = 0%, Cumulative CPU 547.86 sec
2023-01-05 09:28:59,479 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:28:59,924 Stage-153 map = 92%,  reduce = 0%, Cumulative CPU 556.79 sec
2023-01-05 09:29:00,477 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:29:00,573 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:29:05,289 Stage-153 map = 93%,  reduce = 0%, Cumulative CPU 558.77 sec
2023-01-05 09:29:11,609 Stage-153 map = 94%,  reduce = 0%, Cumulative CPU 562.13 sec
2023-01-05 09:29:11,765 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:29:12,695 Stage-12 map = 56%,  reduce = 19%, Cumulative CPU 3088.99 sec
2023-01-05 09:29:18,016 Stage-153 map = 95%,  reduce = 0%, Cumulative CPU 566.16 sec
2023-01-05 09:29:21,318 Stage-50 map = 4%,  reduce = 0%, Cumulative CPU 602.24 sec
2023-01-05 09:29:24,399 Stage-153 map = 96%,  reduce = 0%, Cumulative CPU 571.65 sec
2023-01-05 09:29:30,694 Stage-153 map = 97%,  reduce = 0%, Cumulative CPU 576.44 sec
2023-01-05 09:29:35,910 Stage-153 map = 98%,  reduce = 0%, Cumulative CPU 578.67 sec
2023-01-05 09:29:38,019 Stage-12 map = 57%,  reduce = 19%, Cumulative CPU 3135.86 sec
2023-01-05 09:29:40,295 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 652.76 sec
2023-01-05 09:29:41,129 Stage-153 map = 99%,  reduce = 0%, Cumulative CPU 583.49 sec
2023-01-05 09:29:47,407 Stage-153 map = 100%,  reduce = 0%, Cumulative CPU 587.11 sec
MapReduce Total cumulative CPU time: 9 minutes 47 seconds 110 msec
Ended Job = job_1672890466700_0135
Launching Job 11 out of 53
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 09:30:00,283 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:30:00,664 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:30:01,295 Stage-42 map = 0%,  reduce = 0%
Starting Job = job_1672890466700_0137, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0137/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0137
2023-01-05 09:30:12,475 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:30:31,303 Stage-12 map = 58%,  reduce = 19%, Cumulative CPU 3224.15 sec
2023-01-05 09:30:40,637 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:31:01,168 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:31:01,621 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:31:02,088 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:31:12,573 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:31:17,401 Stage-12 map = 59%,  reduce = 19%, Cumulative CPU 3298.81 sec
2023-01-05 09:31:41,418 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:32:00,190 Stage-12 map = 60%,  reduce = 19%, Cumulative CPU 3369.2 sec
2023-01-05 09:32:01,272 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:32:02,518 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:32:02,892 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:32:13,438 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:32:41,890 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:32:52,290 Stage-12 map = 61%,  reduce = 19%, Cumulative CPU 3458.86 sec
2023-01-05 09:33:02,062 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:33:03,294 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:33:03,801 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:33:14,220 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:33:42,019 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:33:52,425 Stage-12 map = 61%,  reduce = 19%, Cumulative CPU 3561.8 sec
2023-01-05 09:34:02,876 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:34:04,094 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:34:04,314 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:34:09,202 Stage-12 map = 62%,  reduce = 19%, Cumulative CPU 3588.78 sec
2023-01-05 09:34:15,065 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:34:42,395 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:35:03,810 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:35:04,800 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:35:04,901 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:35:09,604 Stage-12 map = 62%,  reduce = 19%, Cumulative CPU 3690.21 sec
2023-01-05 09:35:15,996 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:35:19,030 Stage-12 map = 63%,  reduce = 19%, Cumulative CPU 3711.16 sec
2023-01-05 09:35:42,488 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:36:04,628 Stage-12 map = 64%,  reduce = 19%, Cumulative CPU 3788.3 sec
2023-01-05 09:36:04,655 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:36:05,403 Stage-42 map = 0%,  reduce = 0%
2023-01-05 09:36:05,678 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:36:16,821 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:36:42,101 Stage-12 map = 65%,  reduce = 19%, Cumulative CPU 3853.84 sec
2023-01-05 09:36:42,609 Stage-50 map = 5%,  reduce = 0%, Cumulative CPU 639.21 sec
2023-01-05 09:36:59,917 Stage-12 map = 0%,  reduce = 0%
2023-01-05 09:37:05,470 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:37:06,514 Stage-2 map = 0%,  reduce = 0%
2023-01-05 09:37:17,626 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:37:44,741 Stage-12 map = 1%,  reduce = 0%, Cumulative CPU 24.16 sec
2023-01-05 09:37:56,528 Stage-12 map = 2%,  reduce = 0%, Cumulative CPU 31.46 sec
2023-01-05 09:37:58,817 Stage-2 map = 1%,  reduce = 0%, Cumulative CPU 84.48 sec
2023-01-05 09:38:06,403 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:38:18,599 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:39:07,178 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:39:19,354 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:40:07,927 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:40:20,081 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:41:08,730 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:41:20,887 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:42:09,559 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:42:21,729 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:43:10,358 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:43:22,514 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:44:11,114 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:44:23,269 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:45:11,911 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:45:24,079 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:46:12,699 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:46:24,858 Stage-38 map = 0%,  reduce = 0%
2023-01-05 09:47:13,541 Stage-33 map = 0%,  reduce = 0%
2023-01-05 09:47:25,704 Stage-38 map = 0%,  reduce = 0%
MapReduce Total cumulative CPU time: 5 minutes 50 seconds 290 msec
Ended Job = job_1672890466700_0133 with errors
Error during job, obtaining debugging information...
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException: sleep interrupted
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:364)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)
	at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:871)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:331)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:328)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:328)
	at org.apache.hadoop.mapreduce.Job.getStatus(Job.java:346)
	at org.apache.hadoop.mapred.JobClient.getJobInner(JobClient.java:612)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:640)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:295)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:559)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:433)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:149)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:76)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:361)
	... 18 more
Ended Job = job_1672890466700_0128 with exception 'org.apache.hadoop.yarn.exceptions.YarnRuntimeException(java.lang.InterruptedException: sleep interrupted)'
2023-01-05 09:47:48,063 Stage-50 map = 0%,  reduce = 0%
MapReduce Total cumulative CPU time: 10 minutes 39 seconds 210 msec
Ended Job = job_1672890466700_0130 with errors
Error during job, obtaining debugging information...
2023-01-05 09:48:08,376 Stage-38 map = 4%,  reduce = 0%, Cumulative CPU 18.04 sec
2023-01-05 09:48:11,426 Stage-38 map = 6%,  reduce = 0%, Cumulative CPU 35.22 sec
2023-01-05 09:48:14,469 Stage-38 map = 9%,  reduce = 0%, Cumulative CPU 40.28 sec
2023-01-05 09:48:17,529 Stage-38 map = 10%,  reduce = 0%, Cumulative CPU 46.54 sec
2023-01-05 09:48:20,573 Stage-38 map = 14%,  reduce = 0%, Cumulative CPU 52.05 sec
2023-01-05 09:48:23,627 Stage-38 map = 16%,  reduce = 0%, Cumulative CPU 59.27 sec
2023-01-05 09:48:26,678 Stage-38 map = 19%,  reduce = 0%, Cumulative CPU 65.04 sec
2023-01-05 09:48:28,706 Stage-38 map = 21%,  reduce = 0%, Cumulative CPU 69.45 sec
2023-01-05 09:48:32,763 Stage-38 map = 23%,  reduce = 0%, Cumulative CPU 75.23 sec
2023-01-05 09:48:34,789 Stage-38 map = 25%,  reduce = 0%, Cumulative CPU 79.9 sec
2023-01-05 09:48:38,857 Stage-38 map = 27%,  reduce = 0%, Cumulative CPU 85.8 sec
2023-01-05 09:48:40,887 Stage-38 map = 29%,  reduce = 0%, Cumulative CPU 90.55 sec
2023-01-05 09:48:43,944 Stage-38 map = 31%,  reduce = 0%, Cumulative CPU 96.6 sec
2023-01-05 09:48:46,997 Stage-38 map = 33%,  reduce = 0%, Cumulative CPU 102.27 sec
2023-01-05 09:48:50,041 Stage-38 map = 36%,  reduce = 0%, Cumulative CPU 107.17 sec
2023-01-05 09:48:53,089 Stage-38 map = 37%,  reduce = 0%, Cumulative CPU 113.58 sec
2023-01-05 09:48:56,136 Stage-38 map = 41%,  reduce = 0%, Cumulative CPU 118.95 sec
2023-01-05 09:48:59,179 Stage-38 map = 43%,  reduce = 0%, Cumulative CPU 125.38 sec
2023-01-05 09:49:01,219 Stage-38 map = 63%,  reduce = 0%, Cumulative CPU 129.49 sec
2023-01-05 09:49:05,321 Stage-38 map = 64%,  reduce = 0%, Cumulative CPU 135.3 sec
2023-01-05 09:49:11,610 Stage-38 map = 66%,  reduce = 0%, Cumulative CPU 140.45 sec
2023-01-05 09:49:16,753 Stage-38 map = 68%,  reduce = 0%, Cumulative CPU 144.48 sec
2023-01-05 09:49:22,832 Stage-38 map = 70%,  reduce = 0%, Cumulative CPU 148.95 sec
2023-01-05 09:49:24,863 Stage-38 map = 70%,  reduce = 8%, Cumulative CPU 149.92 sec
2023-01-05 09:49:27,907 Stage-38 map = 70%,  reduce = 17%, Cumulative CPU 150.64 sec
2023-01-05 09:49:28,923 Stage-38 map = 72%,  reduce = 17%, Cumulative CPU 156.68 sec
2023-01-05 09:49:35,020 Stage-38 map = 73%,  reduce = 17%, Cumulative CPU 163.33 sec
2023-01-05 09:49:41,107 Stage-38 map = 74%,  reduce = 17%, Cumulative CPU 169.78 sec
2023-01-05 09:49:47,196 Stage-38 map = 75%,  reduce = 17%, Cumulative CPU 175.68 sec
2023-01-05 09:49:53,290 Stage-38 map = 76%,  reduce = 17%, Cumulative CPU 180.39 sec
2023-01-05 09:49:59,373 Stage-38 map = 78%,  reduce = 17%, Cumulative CPU 185.04 sec
2023-01-05 09:50:05,468 Stage-38 map = 80%,  reduce = 17%, Cumulative CPU 189.47 sec
2023-01-05 09:50:11,561 Stage-38 map = 82%,  reduce = 17%, Cumulative CPU 195.69 sec
2023-01-05 09:50:17,649 Stage-38 map = 83%,  reduce = 17%, Cumulative CPU 201.55 sec
2023-01-05 09:50:20,705 Stage-38 map = 100%,  reduce = 17%, Cumulative CPU 205.37 sec
2023-01-05 09:50:22,734 Stage-38 map = 100%,  reduce = 100%, Cumulative CPU 209.73 sec
MapReduce Total cumulative CPU time: 3 minutes 29 seconds 730 msec
Ended Job = job_1672890466700_0134
Hadoop job information for Stage-65: number of mappers: 4; number of reducers: 4
2023-01-05 09:50:35,155 Stage-65 map = 0%,  reduce = 0%
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException: sleep interrupted
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:364)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)
	at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:871)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:331)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:328)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:328)
	at org.apache.hadoop.mapreduce.Job.getStatus(Job.java:346)
	at org.apache.hadoop.mapred.JobClient.getJobInner(JobClient.java:612)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:640)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:295)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:559)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:433)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:149)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:76)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:361)
	... 18 more
Ended Job = job_1672890466700_0129 with exception 'org.apache.hadoop.yarn.exceptions.YarnRuntimeException(java.lang.InterruptedException: sleep interrupted)'
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException: sleep interrupted
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:364)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)
	at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:871)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:331)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:328)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:328)
	at org.apache.hadoop.mapreduce.Job.getStatus(Job.java:346)
	at org.apache.hadoop.mapred.JobClient.getJobInner(JobClient.java:612)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:640)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:295)
	at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:559)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:433)
	at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:149)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:76)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:361)
	... 18 more
Ended Job = job_1672890466700_0131 with exception 'org.apache.hadoop.yarn.exceptions.YarnRuntimeException(java.lang.InterruptedException: sleep interrupted)'
2023-01-05 09:51:08,594 Stage-65 map = 100%,  reduce = 100%
Ended Job = job_1672890466700_0136 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1672890466700_0136_m_000000 (and more) from job job_1672890466700_0136

Task with the most failures(1):
-----
Task ID:
  task_1672890466700_0136_m_000002

URL:
  http://my-hadoop-yarn-rm:8088/taskdetails.jsp?jobid=job_1672890466700_0136&tipid=task_1672890466700_0136_m_000002
-----
Diagnostic Messages for this Task:
Task KILL is received. Killing attempt!

MapReduce Jobs Launched:
Stage-Stage-27: Map: 4  Reduce: 4   Cumulative CPU: 309.12 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-131: Map: 4   Cumulative CPU: 242.84 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-153: Map: 3   Cumulative CPU: 587.11 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-33: Map: 3  Reduce: 3   Cumulative CPU: 350.29 sec   HDFS Read: 0 HDFS Write: 0 FAIL
Stage-Stage-50: Map: 9  Reduce: 1   Cumulative CPU: 639.21 sec   HDFS Read: 0 HDFS Write: 0 FAIL
Stage-Stage-38: Map: 2  Reduce: 2   Cumulative CPU: 209.73 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-65: Map: 4  Reduce: 4   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 38 minutes 58 seconds 300 msec
Hadoop job information for Stage-80: number of mappers: 0; number of reducers: 0
2023-01-05 09:51:10,022 Stage-80 map = 0%,  reduce = 0%
Ended Job = job_1672890466700_0137 with errors
Error during job, obtaining debugging information...
timediff:3862.455018807
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query15.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 843f57d8-a5f6-4c9a-9e49-a3148e319d90

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = ee9f5b54-90bc-45e7-a7b8-5b0ddfb106a0
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_sales_price, cs_bill_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_current_addr_sk, c_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_zip, ca_state, ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_qoy, d_year
Query ID = hdfs_20230105095115_b5028900-4030-4889-b7dc-fb77188f68fb
Total jobs = 6
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.2023-01-05 09:51:29	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 09:51:30	Uploaded 1 File to: file:/tmp/hdfs/843f57d8-a5f6-4c9a-9e49-a3148e319d90/hive_2023-01-05_09-51-15_878_7116661376981472021-1/-local-10014/HashTable-Stage-13/MapJoin-mapfile31--.hashtable (2179 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 6
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0138, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0138/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0138
Hadoop job information for Stage-13: number of mappers: 3; number of reducers: 0
2023-01-05 09:51:46,863 Stage-13 map = 0%,  reduce = 0%
2023-01-05 09:52:03,620 Stage-13 map = 2%,  reduce = 0%, Cumulative CPU 9.66 sec
2023-01-05 09:52:06,719 Stage-13 map = 4%,  reduce = 0%, Cumulative CPU 31.46 sec
2023-01-05 09:52:09,799 Stage-13 map = 6%,  reduce = 0%, Cumulative CPU 34.97 sec
2023-01-05 09:52:12,891 Stage-13 map = 8%,  reduce = 0%, Cumulative CPU 40.91 sec
2023-01-05 09:52:15,952 Stage-13 map = 9%,  reduce = 0%, Cumulative CPU 45.65 sec
2023-01-05 09:52:19,084 Stage-13 map = 12%,  reduce = 0%, Cumulative CPU 51.56 sec
2023-01-05 09:52:22,246 Stage-13 map = 13%,  reduce = 0%, Cumulative CPU 56.56 sec
2023-01-05 09:52:25,297 Stage-13 map = 15%,  reduce = 0%, Cumulative CPU 64.82 sec
2023-01-05 09:52:27,352 Stage-13 map = 16%,  reduce = 0%, Cumulative CPU 69.63 sec
2023-01-05 09:52:30,416 Stage-13 map = 17%,  reduce = 0%, Cumulative CPU 73.59 sec
2023-01-05 09:52:31,443 Stage-13 map = 18%,  reduce = 0%, Cumulative CPU 75.44 sec
2023-01-05 09:52:33,504 Stage-13 map = 20%,  reduce = 0%, Cumulative CPU 80.8 sec
2023-01-05 09:52:36,584 Stage-13 map = 21%,  reduce = 0%, Cumulative CPU 86.59 sec
2023-01-05 09:52:37,606 Stage-13 map = 22%,  reduce = 0%, Cumulative CPU 88.48 sec
2023-01-05 09:52:39,675 Stage-13 map = 23%,  reduce = 0%, Cumulative CPU 93.09 sec
2023-01-05 09:52:42,758 Stage-13 map = 25%,  reduce = 0%, Cumulative CPU 100.0 sec
2023-01-05 09:52:45,849 Stage-13 map = 26%,  reduce = 0%, Cumulative CPU 104.6 sec
2023-01-05 09:52:48,912 Stage-13 map = 29%,  reduce = 0%, Cumulative CPU 112.57 sec
2023-01-05 09:52:51,982 Stage-13 map = 31%,  reduce = 0%, Cumulative CPU 115.95 sec
2023-01-05 09:52:55,050 Stage-13 map = 34%,  reduce = 0%, Cumulative CPU 125.42 sec
2023-01-05 09:52:58,125 Stage-13 map = 35%,  reduce = 0%, Cumulative CPU 128.65 sec
2023-01-05 09:53:01,227 Stage-13 map = 38%,  reduce = 0%, Cumulative CPU 138.22 sec
2023-01-05 09:53:04,302 Stage-13 map = 40%,  reduce = 0%, Cumulative CPU 141.8 sec
2023-01-05 09:53:07,372 Stage-13 map = 43%,  reduce = 0%, Cumulative CPU 151.21 sec
2023-01-05 09:53:10,423 Stage-13 map = 45%,  reduce = 0%, Cumulative CPU 154.19 sec
2023-01-05 09:53:12,477 Stage-13 map = 47%,  reduce = 0%, Cumulative CPU 157.75 sec
2023-01-05 09:53:13,501 Stage-13 map = 48%,  reduce = 0%, Cumulative CPU 162.63 sec
2023-01-05 09:53:15,539 Stage-13 map = 50%,  reduce = 0%, Cumulative CPU 165.93 sec
2023-01-05 09:53:18,601 Stage-13 map = 52%,  reduce = 0%, Cumulative CPU 168.99 sec
2023-01-05 09:53:19,627 Stage-13 map = 54%,  reduce = 0%, Cumulative CPU 173.56 sec
2023-01-05 09:53:21,661 Stage-13 map = 56%,  reduce = 0%, Cumulative CPU 176.27 sec
2023-01-05 09:53:24,713 Stage-13 map = 57%,  reduce = 0%, Cumulative CPU 178.86 sec
2023-01-05 09:53:25,726 Stage-13 map = 58%,  reduce = 0%, Cumulative CPU 181.96 sec
2023-01-05 09:53:27,766 Stage-13 map = 61%,  reduce = 0%, Cumulative CPU 185.09 sec
2023-01-05 09:53:30,819 Stage-13 map = 63%,  reduce = 0%, Cumulative CPU 191.43 sec
2023-01-05 09:53:33,883 Stage-13 map = 64%,  reduce = 0%, Cumulative CPU 196.06 sec
2023-01-05 09:53:36,957 Stage-13 map = 66%,  reduce = 0%, Cumulative CPU 202.28 sec
2023-01-05 09:53:40,017 Stage-13 map = 67%,  reduce = 0%, Cumulative CPU 206.46 sec
2023-01-05 09:53:43,068 Stage-13 map = 70%,  reduce = 0%, Cumulative CPU 213.44 sec
2023-01-05 09:53:46,137 Stage-13 map = 71%,  reduce = 0%, Cumulative CPU 217.86 sec
2023-01-05 09:53:49,207 Stage-13 map = 73%,  reduce = 0%, Cumulative CPU 224.95 sec
2023-01-05 09:53:52,285 Stage-13 map = 74%,  reduce = 0%, Cumulative CPU 229.9 sec
2023-01-05 09:53:55,362 Stage-13 map = 76%,  reduce = 0%, Cumulative CPU 237.65 sec
2023-01-05 09:53:58,437 Stage-13 map = 77%,  reduce = 0%, Cumulative CPU 242.62 sec
2023-01-05 09:54:01,496 Stage-13 map = 80%,  reduce = 0%, Cumulative CPU 249.19 sec
2023-01-05 09:54:03,520 Stage-13 map = 81%,  reduce = 0%, Cumulative CPU 253.55 sec
2023-01-05 09:54:06,573 Stage-13 map = 82%,  reduce = 0%, Cumulative CPU 257.34 sec
2023-01-05 09:54:07,609 Stage-13 map = 84%,  reduce = 0%, Cumulative CPU 258.86 sec
2023-01-05 09:54:09,653 Stage-13 map = 85%,  reduce = 0%, Cumulative CPU 262.24 sec
2023-01-05 09:54:12,715 Stage-13 map = 86%,  reduce = 0%, Cumulative CPU 264.87 sec
2023-01-05 09:54:13,736 Stage-13 map = 88%,  reduce = 0%, Cumulative CPU 266.52 sec
2023-01-05 09:54:15,773 Stage-13 map = 90%,  reduce = 0%, Cumulative CPU 269.63 sec
2023-01-05 09:54:18,824 Stage-13 map = 91%,  reduce = 0%, Cumulative CPU 272.62 sec
2023-01-05 09:54:19,842 Stage-13 map = 94%,  reduce = 0%, Cumulative CPU 276.41 sec
2023-01-05 09:54:24,909 Stage-13 map = 97%,  reduce = 0%, Cumulative CPU 280.86 sec
2023-01-05 09:54:27,957 Stage-13 map = 99%,  reduce = 0%, Cumulative CPU 281.96 sec
2023-01-05 09:54:29,981 Stage-13 map = 100%,  reduce = 0%, Cumulative CPU 283.73 sec
MapReduce Total cumulative CPU time: 4 minutes 43 seconds 730 msec
Ended Job = job_1672890466700_0138
Stage-15 is filtered out by condition resolver.
Stage-16 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]

SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 09:54:41	Dump the side-table for tag: 0 with group count: 44072 into file: file:/tmp/hdfs/843f57d8-a5f6-4c9a-9e49-a3148e319d90/hive_2023-01-05_09-51-15_878_7116661376981472021-1/-local-10012/HashTable-Stage-11/MapJoin-mapfile20--.hashtable2023-01-05 09:54:41	Uploaded 1 File to: file:/tmp/hdfs/843f57d8-a5f6-4c9a-9e49-a3148e319d90/hive_2023-01-05_09-51-15_878_7116661376981472021-1/-local-10012/HashTable-Stage-11/MapJoin-mapfile20--.hashtable (4450872 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 3 out of 6
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0139, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0139/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0139
Hadoop job information for Stage-11: number of mappers: 1; number of reducers: 0
2023-01-05 09:54:50,679 Stage-11 map = 0%,  reduce = 0%
2023-01-05 09:54:59,800 Stage-11 map = 100%,  reduce = 0%, Cumulative CPU 8.03 sec
MapReduce Total cumulative CPU time: 8 seconds 30 msec
Ended Job = job_1672890466700_0139
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 09:55:08	Starting to launch local task to process map join;	maximum memory = 239075328
Hive Runtime Error: Map local work exhausted memory
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-14

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
MapReduce Jobs Launched:
Stage-Stage-13: Map: 3   Cumulative CPU: 283.73 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-11: Map: 1   Cumulative CPU: 8.03 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 51 seconds 760 msec
timediff:241.954507910
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query16.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 4da6e4a9-6671-4fca-9064-fcc8ec61d18b

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 4a000d80-4272-4a6b-8d8f-e02ca74f9190
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_warehouse_sk, cs_order_number, cs_net_profit, cs_ship_date_sk, cs_ship_addr_sk, cs_ext_ship_cost, cs_call_center_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_state, ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@call_center, Columns: cc_call_center_sk, cc_county
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_warehouse_sk, cs_order_number
No Stats for tpcds_bin_partitioned_orc_10@catalog_returns, Columns: cr_order_number
Query ID = hdfs_20230105095518_801d0d5a-5b06-4634-9fd5-61c18de155ad
Total jobs = 8
Launching Job 1 out of 8

Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2023-01-05 09:55:41	Dump the side-table for tag: 1 with group count: 7941 into file: file:/tmp/hdfs/4da6e4a9-6671-4fca-9064-fcc8ec61d18b/hive_2023-01-05_09-55-18_359_8927164023779097470-1/-local-10015/HashTable-Stage-17/MapJoin-mapfile41--.hashtable
2023-01-05 09:55:41	End of local task; Time Taken: 4.348 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 8
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0140, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0140/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0140
Starting Job = job_1672890466700_0141, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0141/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0141
Hadoop job information for Stage-12: number of mappers: 1; number of reducers: 1
2023-01-05 09:55:53,222 Stage-12 map = 0%,  reduce = 0%
Hadoop job information for Stage-17: number of mappers: 3; number of reducers: 0
2023-01-05 09:55:57,146 Stage-17 map = 0%,  reduce = 0%
2023-01-05 09:56:11,314 Stage-12 map = 1%,  reduce = 0%, Cumulative CPU 18.33 sec
2023-01-05 09:56:19,692 Stage-17 map = 1%,  reduce = 0%, Cumulative CPU 17.52 sec
2023-01-05 09:56:23,393 Stage-12 map = 3%,  reduce = 0%, Cumulative CPU 32.38 sec
2023-01-05 09:56:25,121 Stage-17 map = 2%,  reduce = 0%, Cumulative CPU 47.51 sec
2023-01-05 09:56:29,517 Stage-12 map = 4%,  reduce = 0%, Cumulative CPU 39.64 sec
2023-01-05 09:56:30,255 Stage-17 map = 4%,  reduce = 0%, Cumulative CPU 55.78 sec
2023-01-05 09:56:31,277 Stage-17 map = 6%,  reduce = 0%, Cumulative CPU 58.68 sec
2023-01-05 09:56:35,658 Stage-12 map = 6%,  reduce = 0%, Cumulative CPU 46.69 sec
2023-01-05 09:56:36,375 Stage-17 map = 8%,  reduce = 0%, Cumulative CPU 65.42 sec
2023-01-05 09:56:37,390 Stage-17 map = 9%,  reduce = 0%, Cumulative CPU 68.91 sec
2023-01-05 09:56:40,965 Stage-12 map = 8%,  reduce = 0%, Cumulative CPU 54.29 sec
2023-01-05 09:56:42,501 Stage-17 map = 12%,  reduce = 0%, Cumulative CPU 76.03 sec
2023-01-05 09:56:43,517 Stage-17 map = 13%,  reduce = 0%, Cumulative CPU 80.19 sec
2023-01-05 09:56:47,108 Stage-12 map = 11%,  reduce = 0%, Cumulative CPU 60.46 sec
2023-01-05 09:56:48,619 Stage-17 map = 15%,  reduce = 0%, Cumulative CPU 86.25 sec
2023-01-05 09:56:49,637 Stage-17 map = 16%,  reduce = 0%, Cumulative CPU 90.93 sec
2023-01-05 09:56:53,248 Stage-12 map = 13%,  reduce = 0%, Cumulative CPU 67.09 sec
2023-01-05 09:56:54,737 Stage-17 map = 19%,  reduce = 0%, Cumulative CPU 98.23 sec
2023-01-05 09:56:55,788 Stage-17 map = 20%,  reduce = 0%, Cumulative CPU 102.85 sec
2023-01-05 09:56:59,395 Stage-12 map = 15%,  reduce = 0%, Cumulative CPU 73.53 sec
2023-01-05 09:57:00,874 Stage-17 map = 22%,  reduce = 0%, Cumulative CPU 109.78 sec
2023-01-05 09:57:01,891 Stage-17 map = 23%,  reduce = 0%, Cumulative CPU 114.0 sec
2023-01-05 09:57:05,523 Stage-12 map = 17%,  reduce = 0%, Cumulative CPU 80.3 sec
2023-01-05 09:57:05,965 Stage-17 map = 24%,  reduce = 0%, Cumulative CPU 118.56 sec
2023-01-05 09:57:06,990 Stage-17 map = 26%,  reduce = 0%, Cumulative CPU 128.08 sec
2023-01-05 09:57:11,648 Stage-12 map = 18%,  reduce = 0%, Cumulative CPU 87.14 sec
2023-01-05 09:57:12,095 Stage-17 map = 27%,  reduce = 0%, Cumulative CPU 132.72 sec
2023-01-05 09:57:13,113 Stage-17 map = 29%,  reduce = 0%, Cumulative CPU 141.4 sec
2023-01-05 09:57:17,791 Stage-12 map = 20%,  reduce = 0%, Cumulative CPU 93.96 sec
2023-01-05 09:57:18,220 Stage-17 map = 30%,  reduce = 0%, Cumulative CPU 145.08 sec
2023-01-05 09:57:19,237 Stage-17 map = 32%,  reduce = 0%, Cumulative CPU 152.87 sec
2023-01-05 09:57:22,882 Stage-12 map = 23%,  reduce = 0%, Cumulative CPU 100.78 sec
2023-01-05 09:57:24,332 Stage-17 map = 34%,  reduce = 0%, Cumulative CPU 159.42 sec
2023-01-05 09:57:25,349 Stage-17 map = 35%,  reduce = 0%, Cumulative CPU 161.94 sec
2023-01-05 09:57:28,983 Stage-12 map = 26%,  reduce = 0%, Cumulative CPU 107.01 sec
2023-01-05 09:57:30,450 Stage-17 map = 38%,  reduce = 0%, Cumulative CPU 169.08 sec
2023-01-05 09:57:31,470 Stage-17 map = 39%,  reduce = 0%, Cumulative CPU 171.47 sec
2023-01-05 09:57:35,093 Stage-12 map = 28%,  reduce = 0%, Cumulative CPU 113.59 sec
2023-01-05 09:57:36,566 Stage-17 map = 41%,  reduce = 0%, Cumulative CPU 177.96 sec
2023-01-05 09:57:37,585 Stage-17 map = 42%,  reduce = 0%, Cumulative CPU 180.45 sec
2023-01-05 09:57:41,205 Stage-12 map = 29%,  reduce = 0%, Cumulative CPU 119.48 sec
2023-01-05 09:57:42,675 Stage-17 map = 45%,  reduce = 0%, Cumulative CPU 186.49 sec
2023-01-05 09:57:43,693 Stage-17 map = 46%,  reduce = 0%, Cumulative CPU 188.48 sec
2023-01-05 09:57:47,296 Stage-12 map = 30%,  reduce = 0%, Cumulative CPU 125.58 sec
2023-01-05 09:57:48,782 Stage-17 map = 49%,  reduce = 0%, Cumulative CPU 194.58 sec
2023-01-05 09:57:49,801 Stage-17 map = 50%,  reduce = 0%, Cumulative CPU 196.83 sec
2023-01-05 09:57:53,413 Stage-12 map = 32%,  reduce = 0%, Cumulative CPU 132.33 sec
2023-01-05 09:57:54,933 Stage-17 map = 53%,  reduce = 0%, Cumulative CPU 201.43 sec
2023-01-05 09:57:55,948 Stage-17 map = 54%,  reduce = 0%, Cumulative CPU 203.39 sec
2023-01-05 09:57:59,528 Stage-12 map = 34%,  reduce = 0%, Cumulative CPU 138.23 sec
2023-01-05 09:58:00,027 Stage-17 map = 55%,  reduce = 0%, Cumulative CPU 206.63 sec
2023-01-05 09:58:01,048 Stage-17 map = 58%,  reduce = 0%, Cumulative CPU 211.19 sec
2023-01-05 09:58:05,635 Stage-12 map = 37%,  reduce = 0%, Cumulative CPU 144.92 sec
2023-01-05 09:58:07,172 Stage-17 map = 61%,  reduce = 0%, Cumulative CPU 219.77 sec
2023-01-05 09:58:11,748 Stage-12 map = 40%,  reduce = 0%, Cumulative CPU 151.56 sec
2023-01-05 09:58:13,286 Stage-17 map = 64%,  reduce = 0%, Cumulative CPU 228.37 sec
2023-01-05 09:58:17,857 Stage-12 map = 41%,  reduce = 0%, Cumulative CPU 157.54 sec
2023-01-05 09:58:18,365 Stage-17 map = 66%,  reduce = 0%, Cumulative CPU 234.42 sec
2023-01-05 09:58:19,382 Stage-17 map = 67%,  reduce = 0%, Cumulative CPU 236.5 sec
2023-01-05 09:58:22,942 Stage-12 map = 43%,  reduce = 0%, Cumulative CPU 164.39 sec
2023-01-05 09:58:24,482 Stage-17 map = 69%,  reduce = 0%, Cumulative CPU 243.16 sec
2023-01-05 09:58:25,500 Stage-17 map = 70%,  reduce = 0%, Cumulative CPU 246.98 sec
2023-01-05 09:58:29,049 Stage-12 map = 44%,  reduce = 0%, Cumulative CPU 170.65 sec
2023-01-05 09:58:30,585 Stage-17 map = 72%,  reduce = 0%, Cumulative CPU 252.87 sec
2023-01-05 09:58:35,155 Stage-12 map = 46%,  reduce = 0%, Cumulative CPU 177.47 sec
2023-01-05 09:58:36,701 Stage-17 map = 74%,  reduce = 0%, Cumulative CPU 262.76 sec
2023-01-05 09:58:37,714 Stage-17 map = 75%,  reduce = 0%, Cumulative CPU 266.97 sec
2023-01-05 09:58:41,268 Stage-12 map = 49%,  reduce = 0%, Cumulative CPU 183.97 sec
2023-01-05 09:58:42,801 Stage-17 map = 77%,  reduce = 0%, Cumulative CPU 273.86 sec
2023-01-05 09:58:43,819 Stage-17 map = 78%,  reduce = 0%, Cumulative CPU 278.27 sec
2023-01-05 09:58:47,386 Stage-12 map = 52%,  reduce = 0%, Cumulative CPU 191.0 sec
2023-01-05 09:58:48,923 Stage-17 map = 79%,  reduce = 0%, Cumulative CPU 284.12 sec
2023-01-05 09:58:49,940 Stage-17 map = 80%,  reduce = 0%, Cumulative CPU 286.83 sec
2023-01-05 09:58:53,487 Stage-12 map = 54%,  reduce = 0%, Cumulative CPU 197.68 sec
2023-01-05 09:58:56,050 Stage-17 map = 81%,  reduce = 0%, Cumulative CPU 294.11 sec
2023-01-05 09:58:59,597 Stage-12 map = 56%,  reduce = 0%, Cumulative CPU 204.63 sec
2023-01-05 09:59:01,134 Stage-17 map = 82%,  reduce = 0%, Cumulative CPU 300.75 sec
2023-01-05 09:59:05,705 Stage-12 map = 57%,  reduce = 0%, Cumulative CPU 211.13 sec
2023-01-05 09:59:07,229 Stage-17 map = 83%,  reduce = 0%, Cumulative CPU 307.39 sec
2023-01-05 09:59:09,259 Stage-17 map = 96%,  reduce = 0%, Cumulative CPU 308.65 sec
2023-01-05 09:59:11,820 Stage-12 map = 59%,  reduce = 0%, Cumulative CPU 217.74 sec
2023-01-05 09:59:13,333 Stage-17 map = 97%,  reduce = 0%, Cumulative CPU 311.66 sec
2023-01-05 09:59:17,930 Stage-12 map = 61%,  reduce = 0%, Cumulative CPU 224.81 sec
2023-01-05 09:59:19,455 Stage-17 map = 99%,  reduce = 0%, Cumulative CPU 314.54 sec
2023-01-05 09:59:23,023 Stage-12 map = 64%,  reduce = 0%, Cumulative CPU 232.01 sec
2023-01-05 09:59:23,725 Stage-17 map = 100%,  reduce = 0%, Cumulative CPU 316.49 sec
MapReduce Total cumulative CPU time: 5 minutes 16 seconds 490 msec
Ended Job = job_1672890466700_0141
Stage-21 is filtered out by condition resolver.
Stage-4 is selected by condition resolver.
Launching Job 3 out of 8
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
2023-01-05 09:59:29,125 Stage-12 map = 66%,  reduce = 0%, Cumulative CPU 239.13 sec
Starting Job = job_1672890466700_0142, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0142/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0142
2023-01-05 09:59:35,233 Stage-12 map = 67%,  reduce = 0%, Cumulative CPU 245.87 sec
2023-01-05 09:59:41,343 Stage-12 map = 100%,  reduce = 0%, Cumulative CPU 251.67 sec
Hadoop job information for Stage-4: number of mappers: 4; number of reducers: 3
2023-01-05 09:59:41,971 Stage-4 map = 0%,  reduce = 0%
2023-01-05 09:59:47,504 Stage-12 map = 100%,  reduce = 100%, Cumulative CPU 255.62 sec
MapReduce Total cumulative CPU time: 4 minutes 15 seconds 620 msec
Ended Job = job_1672890466700_0140
2023-01-05 10:00:12,334 Stage-4 map = 25%,  reduce = 0%, Cumulative CPU 10.11 sec
2023-01-05 10:00:20,492 Stage-4 map = 26%,  reduce = 0%, Cumulative CPU 44.45 sec
2023-01-05 10:00:25,559 Stage-4 map = 27%,  reduce = 0%, Cumulative CPU 63.67 sec
2023-01-05 10:00:28,615 Stage-4 map = 28%,  reduce = 3%, Cumulative CPU 74.04 sec
2023-01-05 10:00:29,632 Stage-4 map = 28%,  reduce = 6%, Cumulative CPU 74.46 sec
2023-01-05 10:00:31,661 Stage-4 map = 29%,  reduce = 6%, Cumulative CPU 77.39 sec
2023-01-05 10:00:33,686 Stage-4 map = 29%,  reduce = 8%, Cumulative CPU 81.53 sec
2023-01-05 10:00:34,699 Stage-4 map = 30%,  reduce = 8%, Cumulative CPU 84.59 sec
2023-01-05 10:00:37,741 Stage-4 map = 31%,  reduce = 8%, Cumulative CPU 88.54 sec
2023-01-05 10:00:40,795 Stage-4 map = 32%,  reduce = 8%, Cumulative CPU 95.49 sec
2023-01-05 10:00:46,887 Stage-4 map = 33%,  reduce = 8%, Cumulative CPU 95.68 sec
2023-01-05 10:00:52,976 Stage-4 map = 34%,  reduce = 8%, Cumulative CPU 107.79 sec
2023-01-05 10:00:59,158 Stage-4 map = 36%,  reduce = 8%, Cumulative CPU 117.8 sec
2023-01-05 10:01:05,276 Stage-4 map = 37%,  reduce = 8%, Cumulative CPU 129.43 sec
2023-01-05 10:01:11,374 Stage-4 map = 38%,  reduce = 8%, Cumulative CPU 141.37 sec
2023-01-05 10:01:15,431 Stage-4 map = 39%,  reduce = 8%, Cumulative CPU 145.58 sec
2023-01-05 10:01:17,466 Stage-4 map = 40%,  reduce = 8%, Cumulative CPU 153.22 sec
2023-01-05 10:01:22,570 Stage-4 map = 41%,  reduce = 8%, Cumulative CPU 161.97 sec
2023-01-05 10:01:23,583 Stage-4 map = 42%,  reduce = 8%, Cumulative CPU 165.92 sec
2023-01-05 10:01:29,672 Stage-4 map = 43%,  reduce = 8%, Cumulative CPU 178.69 sec
2023-01-05 10:01:34,748 Stage-4 map = 45%,  reduce = 8%, Cumulative CPU 188.46 sec
2023-01-05 10:01:40,845 Stage-4 map = 47%,  reduce = 8%, Cumulative CPU 199.03 sec
2023-01-05 10:01:46,943 Stage-4 map = 49%,  reduce = 8%, Cumulative CPU 210.44 sec
2023-01-05 10:01:51,020 Stage-4 map = 50%,  reduce = 8%, Cumulative CPU 213.34 sec
2023-01-05 10:01:57,146 Stage-4 map = 51%,  reduce = 8%, Cumulative CPU 224.07 sec
2023-01-05 10:01:59,181 Stage-4 map = 52%,  reduce = 8%, Cumulative CPU 231.39 sec
2023-01-05 10:02:05,270 Stage-4 map = 54%,  reduce = 8%, Cumulative CPU 241.02 sec
2023-01-05 10:02:11,370 Stage-4 map = 55%,  reduce = 8%, Cumulative CPU 249.65 sec
2023-01-05 10:02:15,424 Stage-4 map = 56%,  reduce = 8%, Cumulative CPU 252.42 sec
2023-01-05 10:02:17,463 Stage-4 map = 58%,  reduce = 8%, Cumulative CPU 258.95 sec
2023-01-05 10:02:21,525 Stage-4 map = 59%,  reduce = 8%, Cumulative CPU 261.25 sec
2023-01-05 10:02:23,549 Stage-4 map = 60%,  reduce = 8%, Cumulative CPU 267.82 sec
2023-01-05 10:02:28,611 Stage-4 map = 61%,  reduce = 8%, Cumulative CPU 273.21 sec
2023-01-05 10:02:29,626 Stage-4 map = 62%,  reduce = 8%, Cumulative CPU 276.49 sec
2023-01-05 10:02:34,687 Stage-4 map = 63%,  reduce = 8%, Cumulative CPU 282.29 sec
2023-01-05 10:02:39,775 Stage-4 map = 64%,  reduce = 8%, Cumulative CPU 287.46 sec
2023-01-05 10:02:40,791 Stage-4 map = 65%,  reduce = 8%, Cumulative CPU 292.56 sec
2023-01-05 10:02:45,879 Stage-4 map = 66%,  reduce = 8%, Cumulative CPU 295.05 sec
2023-01-05 10:02:46,895 Stage-4 map = 67%,  reduce = 8%, Cumulative CPU 300.18 sec
2023-01-05 10:02:50,948 Stage-4 map = 68%,  reduce = 8%, Cumulative CPU 304.09 sec
2023-01-05 10:02:51,962 Stage-4 map = 76%,  reduce = 8%, Cumulative CPU 308.93 sec
2023-01-05 10:02:52,976 Stage-4 map = 77%,  reduce = 8%, Cumulative CPU 311.35 sec
2023-01-05 10:02:53,988 Stage-4 map = 77%,  reduce = 11%, Cumulative CPU 311.39 sec
2023-01-05 10:02:58,041 Stage-4 map = 77%,  reduce = 17%, Cumulative CPU 314.88 sec
2023-01-05 10:02:59,052 Stage-4 map = 86%,  reduce = 17%, Cumulative CPU 319.92 sec
2023-01-05 10:03:03,094 Stage-4 map = 87%,  reduce = 17%, Cumulative CPU 323.49 sec
2023-01-05 10:03:04,104 Stage-4 map = 87%,  reduce = 22%, Cumulative CPU 323.58 sec
2023-01-05 10:03:05,115 Stage-4 map = 87%,  reduce = 25%, Cumulative CPU 323.65 sec
2023-01-05 10:03:15,225 Stage-4 map = 88%,  reduce = 25%, Cumulative CPU 331.85 sec
2023-01-05 10:03:21,294 Stage-4 map = 89%,  reduce = 25%, Cumulative CPU 336.02 sec
2023-01-05 10:03:27,355 Stage-4 map = 90%,  reduce = 25%, Cumulative CPU 339.65 sec
2023-01-05 10:03:33,419 Stage-4 map = 91%,  reduce = 25%, Cumulative CPU 343.79 sec
2023-01-05 10:03:39,490 Stage-4 map = 100%,  reduce = 25%, Cumulative CPU 348.65 sec
2023-01-05 10:03:40,507 Stage-4 map = 100%,  reduce = 39%, Cumulative CPU 349.89 sec
2023-01-05 10:03:41,518 Stage-4 map = 100%,  reduce = 53%, Cumulative CPU 351.61 sec
2023-01-05 10:03:42,534 Stage-4 map = 100%,  reduce = 71%, Cumulative CPU 354.31 sec
2023-01-05 10:03:44,555 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 362.63 sec
MapReduce Total cumulative CPU time: 6 minutes 2 seconds 630 msec
Ended Job = job_1672890466700_0142
Stage-20 is selected by condition resolver.
Stage-5 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 10:03:54	Processing rows:	200000	Hashtable size:	199999	Memory usage:	99573624	percentage:	0.416
Hive Runtime Error: Map local work exhausted memory
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-20

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
ATTEMPT: Execute BackupTask: org.apache.hadoop.hive.ql.exec.mr.MapRedTask
Launching Job 5 out of 8
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0143, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0143/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0143
Hadoop job information for Stage-5: number of mappers: 2; number of reducers: 1
2023-01-05 10:04:07,783 Stage-5 map = 0%,  reduce = 0%
2023-01-05 10:04:11,878 Stage-5 map = 50%,  reduce = 0%, Cumulative CPU 0.59 sec
2023-01-05 10:04:16,997 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 4.45 sec
2023-01-05 10:04:20,072 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 9.39 sec
MapReduce Total cumulative CPU time: 9 seconds 390 msec
Ended Job = job_1672890466700_0143
Launching Job 6 out of 8
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0144, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0144/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0144
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2023-01-05 10:04:28,791 Stage-6 map = 0%,  reduce = 0%
2023-01-05 10:04:32,868 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 0.68 sec
2023-01-05 10:04:38,996 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 1.79 sec
MapReduce Total cumulative CPU time: 1 seconds 790 msec
Ended Job = job_1672890466700_0144
Launching Job 7 out of 8
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0145, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0145/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0145
Hadoop job information for Stage-7: number of mappers: 1; number of reducers: 1
2023-01-05 10:04:46,859 Stage-7 map = 0%,  reduce = 0%
2023-01-05 10:04:50,932 Stage-7 map = 100%,  reduce = 0%, Cumulative CPU 0.66 sec
2023-01-05 10:04:55,012 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 1.41 sec
MapReduce Total cumulative CPU time: 1 seconds 410 msec
Ended Job = job_1672890466700_0145
MapReduce Jobs Launched:
Stage-Stage-17: Map: 3   Cumulative CPU: 316.49 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-12: Map: 1  Reduce: 1   Cumulative CPU: 255.62 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 4  Reduce: 3   Cumulative CPU: 362.63 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 2  Reduce: 1   Cumulative CPU: 9.39 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-6: Map: 1  Reduce: 1   Cumulative CPU: 1.79 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-7: Map: 1  Reduce: 1   Cumulative CPU: 1.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 15 minutes 47 seconds 330 msec
OK
0	NULL	NULL
Time taken: 578.18 seconds, Fetched: 1 row(s)
timediff:584.583542698
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query17.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 2665b03d-a983-4a60-9a95-18e8cb81d295

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 9c1ab8c7-a50d-4b6a-ba2f-63d117bb7e3b
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk, ss_ticket_number, ss_item_sk, ss_store_sk, ss_quantity
No Stats for tpcds_bin_partitioned_orc_10@store_returns, Columns: sr_ticket_number, sr_item_sk, sr_return_quantity, sr_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_item_sk, cs_quantity, cs_bill_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_quarter_name, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_quarter_name, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_quarter_name, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_state, s_store_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_desc, i_item_id, i_item_sk
Query ID = hdfs_20230105100502_83abcfeb-7b9a-4b26-b572-754242fbb472
Total jobs = 11

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.


SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.


2023-01-05 10:05:37	Dump the side-table for tag: 1 with group count: 274 into file: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10020/HashTable-Stage-21/MapJoin-mapfile41--.hashtable

SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]2023-01-05 10:05:40	Dump the side-table for tag: 1 with group count: 274 into file: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10022/HashTable-Stage-22/MapJoin-mapfile51--.hashtable
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator

Execution completed successfully
MapredLocal task succeeded
Launching Job 2 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator
2023-01-05 10:05:44	Uploaded 1 File to: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10024/HashTable-Stage-23/MapJoin-mapfile61--.hashtable (14345726 bytes)2023-01-05 10:05:44	Dump the side-table for tag: 1 with group count: 91 into file: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10024/HashTable-Stage-23/MapJoin-mapfile71--.hashtable
2023-01-05 10:05:44	Uploaded 1 File to: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10024/HashTable-Stage-23/MapJoin-mapfile71--.hashtable (2179 bytes)2023-01-05 10:05:44	Dump the side-table for tag: 1 with group count: 102 into file: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10024/HashTable-Stage-23/MapJoin-mapfile81--.hashtable
2023-01-05 10:05:44	Uploaded 1 File to: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10024/HashTable-Stage-23/MapJoin-mapfile81--.hashtable (2514 bytes)2023-01-05 10:05:44	End of local task; Time Taken: 7.616 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 3 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0146, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0146/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0146
Starting Job = job_1672890466700_0147, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0147/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0147
Starting Job = job_1672890466700_0148, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0148/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0148
Hadoop job information for Stage-21: number of mappers: 3; number of reducers: 0
2023-01-05 10:06:01,358 Stage-21 map = 0%,  reduce = 0%
Hadoop job information for Stage-22: number of mappers: 1; number of reducers: 0
2023-01-05 10:06:01,559 Stage-22 map = 0%,  reduce = 0%
2023-01-05 10:06:15,871 Stage-22 map = 2%,  reduce = 0%, Cumulative CPU 14.54 sec
2023-01-05 10:06:18,185 Stage-21 map = 2%,  reduce = 0%, Cumulative CPU 8.96 sec
2023-01-05 10:06:21,276 Stage-21 map = 4%,  reduce = 0%, Cumulative CPU 30.71 sec
2023-01-05 10:06:21,993 Stage-22 map = 5%,  reduce = 0%, Cumulative CPU 19.79 sec
2023-01-05 10:06:24,360 Stage-21 map = 5%,  reduce = 0%, Cumulative CPU 33.61 sec
2023-01-05 10:06:27,460 Stage-21 map = 8%,  reduce = 0%, Cumulative CPU 40.19 sec
2023-01-05 10:06:28,127 Stage-22 map = 8%,  reduce = 0%, Cumulative CPU 24.96 sec
2023-01-05 10:06:30,551 Stage-21 map = 9%,  reduce = 0%, Cumulative CPU 44.43 sec
2023-01-05 10:06:33,643 Stage-21 map = 12%,  reduce = 0%, Cumulative CPU 51.45 sec
2023-01-05 10:06:34,305 Stage-22 map = 14%,  reduce = 0%, Cumulative CPU 30.26 sec
2023-01-05 10:06:36,724 Stage-21 map = 13%,  reduce = 0%, Cumulative CPU 56.06 sec
2023-01-05 10:06:39,798 Stage-21 map = 16%,  reduce = 0%, Cumulative CPU 62.83 sec
2023-01-05 10:06:40,458 Stage-22 map = 19%,  reduce = 0%, Cumulative CPU 35.07 sec
2023-01-05 10:06:42,874 Stage-21 map = 17%,  reduce = 0%, Cumulative CPU 67.05 sec
2023-01-05 10:06:45,936 Stage-21 map = 20%,  reduce = 0%, Cumulative CPU 74.58 sec
2023-01-05 10:06:46,624 Stage-22 map = 23%,  reduce = 0%, Cumulative CPU 40.33 sec
2023-01-05 10:06:49,037 Stage-21 map = 21%,  reduce = 0%, Cumulative CPU 79.69 sec
2023-01-05 10:06:51,106 Stage-21 map = 23%,  reduce = 0%, Cumulative CPU 88.93 sec
2023-01-05 10:06:52,780 Stage-22 map = 27%,  reduce = 0%, Cumulative CPU 46.01 sec
2023-01-05 10:06:54,191 Stage-21 map = 24%,  reduce = 0%, Cumulative CPU 95.32 sec
2023-01-05 10:06:57,325 Stage-21 map = 26%,  reduce = 0%, Cumulative CPU 105.59 sec
2023-01-05 10:06:57,903 Stage-22 map = 33%,  reduce = 0%, Cumulative CPU 52.08 sec
2023-01-05 10:07:00,491 Stage-21 map = 27%,  reduce = 0%, Cumulative CPU 110.56 sec
2023-01-05 10:07:03,572 Stage-21 map = 28%,  reduce = 0%, Cumulative CPU 121.4 sec
2023-01-05 10:07:04,083 Stage-22 map = 39%,  reduce = 0%, Cumulative CPU 58.07 sec
2023-01-05 10:07:06,695 Stage-21 map = 29%,  reduce = 0%, Cumulative CPU 126.89 sec
2023-01-05 10:07:09,800 Stage-21 map = 31%,  reduce = 0%, Cumulative CPU 136.55 sec
2023-01-05 10:07:10,240 Stage-22 map = 44%,  reduce = 0%, Cumulative CPU 64.09 sec
2023-01-05 10:07:12,877 Stage-21 map = 32%,  reduce = 0%, Cumulative CPU 141.23 sec
2023-01-05 10:07:15,932 Stage-21 map = 35%,  reduce = 0%, Cumulative CPU 149.35 sec
2023-01-05 10:07:16,369 Stage-22 map = 47%,  reduce = 0%, Cumulative CPU 70.02 sec
2023-01-05 10:07:21,053 Stage-21 map = 38%,  reduce = 0%, Cumulative CPU 158.48 sec
2023-01-05 10:07:22,486 Stage-22 map = 52%,  reduce = 0%, Cumulative CPU 75.09 sec
2023-01-05 10:07:25,127 Stage-21 map = 39%,  reduce = 0%, Cumulative CPU 160.55 sec
2023-01-05 10:07:27,184 Stage-21 map = 42%,  reduce = 0%, Cumulative CPU 166.81 sec
2023-01-05 10:07:28,605 Stage-22 map = 59%,  reduce = 0%, Cumulative CPU 80.36 sec
2023-01-05 10:07:30,226 Stage-21 map = 43%,  reduce = 0%, Cumulative CPU 168.64 sec
2023-01-05 10:07:33,271 Stage-21 map = 46%,  reduce = 0%, Cumulative CPU 173.58 sec
2023-01-05 10:07:34,718 Stage-22 map = 63%,  reduce = 0%, Cumulative CPU 85.55 sec
2023-01-05 10:07:36,309 Stage-21 map = 47%,  reduce = 0%, Cumulative CPU 175.45 sec
2023-01-05 10:07:39,373 Stage-21 map = 50%,  reduce = 0%, Cumulative CPU 181.08 sec
2023-01-05 10:07:40,837 Stage-22 map = 65%,  reduce = 0%, Cumulative CPU 90.29 sec
2023-01-05 10:07:42,439 Stage-21 map = 52%,  reduce = 0%, Cumulative CPU 183.37 sec
2023-01-05 10:07:45,492 Stage-21 map = 54%,  reduce = 0%, Cumulative CPU 187.53 sec
2023-01-05 10:07:45,925 Stage-22 map = 70%,  reduce = 0%, Cumulative CPU 95.15 sec
2023-01-05 10:07:48,531 Stage-21 map = 56%,  reduce = 0%, Cumulative CPU 189.82 sec
2023-01-05 10:07:51,576 Stage-21 map = 60%,  reduce = 0%, Cumulative CPU 194.12 sec
2023-01-05 10:07:52,060 Stage-22 map = 75%,  reduce = 0%, Cumulative CPU 100.05 sec
2023-01-05 10:07:54,652 Stage-21 map = 61%,  reduce = 0%, Cumulative CPU 196.0 sec
2023-01-05 10:07:57,706 Stage-21 map = 65%,  reduce = 0%, Cumulative CPU 199.82 sec
2023-01-05 10:07:58,174 Stage-22 map = 80%,  reduce = 0%, Cumulative CPU 105.03 sec
2023-01-05 10:08:00,748 Stage-21 map = 67%,  reduce = 0%, Cumulative CPU 202.14 sec
2023-01-05 10:08:03,816 Stage-21 map = 70%,  reduce = 0%, Cumulative CPU 205.58 sec
2023-01-05 10:08:04,317 Stage-22 map = 84%,  reduce = 0%, Cumulative CPU 110.38 sec
2023-01-05 10:08:06,878 Stage-21 map = 71%,  reduce = 0%, Cumulative CPU 207.68 sec
2023-01-05 10:08:09,945 Stage-21 map = 75%,  reduce = 0%, Cumulative CPU 213.85 sec
2023-01-05 10:08:10,447 Stage-22 map = 87%,  reduce = 0%, Cumulative CPU 115.88 sec
2023-01-05 10:08:13,031 Stage-21 map = 77%,  reduce = 0%, Cumulative CPU 219.5 sec
2023-01-05 10:08:16,095 Stage-21 map = 78%,  reduce = 0%, Cumulative CPU 224.11 sec
2023-01-05 10:08:16,569 Stage-22 map = 94%,  reduce = 0%, Cumulative CPU 121.85 sec
2023-01-05 10:08:19,175 Stage-21 map = 79%,  reduce = 0%, Cumulative CPU 229.78 sec
2023-01-05 10:08:21,211 Stage-21 map = 80%,  reduce = 0%, Cumulative CPU 233.42 sec
2023-01-05 10:08:22,690 Stage-22 map = 99%,  reduce = 0%, Cumulative CPU 127.68 sec
2023-01-05 10:08:24,277 Stage-21 map = 81%,  reduce = 0%, Cumulative CPU 238.84 sec
2023-01-05 10:08:25,755 Stage-22 map = 100%,  reduce = 0%, Cumulative CPU 130.72 sec
MapReduce Total cumulative CPU time: 2 minutes 10 seconds 720 msec
Ended Job = job_1672890466700_0147
2023-01-05 10:08:27,350 Stage-21 map = 82%,  reduce = 0%, Cumulative CPU 243.69 sec
2023-01-05 10:08:30,418 Stage-21 map = 84%,  reduce = 0%, Cumulative CPU 248.98 sec
2023-01-05 10:08:33,484 Stage-21 map = 85%,  reduce = 0%, Cumulative CPU 252.28 sec
2023-01-05 10:08:36,553 Stage-21 map = 86%,  reduce = 0%, Cumulative CPU 256.42 sec
Hadoop job information for Stage-23: number of mappers: 4; number of reducers: 0
2023-01-05 10:08:40,729 Stage-23 map = 0%,  reduce = 0%
2023-01-05 10:08:42,747 Stage-21 map = 87%,  reduce = 0%, Cumulative CPU 261.82 sec
2023-01-05 10:08:45,917 Stage-21 map = 88%,  reduce = 0%, Cumulative CPU 264.6 sec
2023-01-05 10:08:49,217 Stage-21 map = 89%,  reduce = 0%, Cumulative CPU 266.69 sec
2023-01-05 10:08:54,454 Stage-21 map = 90%,  reduce = 0%, Cumulative CPU 269.69 sec
2023-01-05 10:09:04,316 Stage-21 map = 91%,  reduce = 0%, Cumulative CPU 272.68 sec
2023-01-05 10:09:12,726 Stage-21 map = 92%,  reduce = 0%, Cumulative CPU 275.5 sec
2023-01-05 10:09:13,073 Stage-23 map = 1%,  reduce = 0%, Cumulative CPU 36.37 sec
2023-01-05 10:09:16,917 Stage-21 map = 93%,  reduce = 0%, Cumulative CPU 279.0 sec
2023-01-05 10:09:17,361 Stage-23 map = 2%,  reduce = 0%, Cumulative CPU 55.64 sec
2023-01-05 10:09:19,426 Stage-23 map = 3%,  reduce = 0%, Cumulative CPU 57.67 sec
2023-01-05 10:09:21,493 Stage-23 map = 4%,  reduce = 0%, Cumulative CPU 60.34 sec
2023-01-05 10:09:22,025 Stage-21 map = 94%,  reduce = 0%, Cumulative CPU 281.47 sec
2023-01-05 10:09:23,536 Stage-23 map = 5%,  reduce = 0%, Cumulative CPU 67.0 sec
2023-01-05 10:09:25,569 Stage-23 map = 6%,  reduce = 0%, Cumulative CPU 68.56 sec
2023-01-05 10:09:27,609 Stage-23 map = 7%,  reduce = 0%, Cumulative CPU 70.93 sec
2023-01-05 10:09:28,176 Stage-21 map = 95%,  reduce = 0%, Cumulative CPU 283.51 sec
2023-01-05 10:09:29,643 Stage-23 map = 8%,  reduce = 0%, Cumulative CPU 76.97 sec
2023-01-05 10:09:30,654 Stage-23 map = 9%,  reduce = 0%, Cumulative CPU 78.14 sec
2023-01-05 10:09:32,691 Stage-23 map = 10%,  reduce = 0%, Cumulative CPU 80.01 sec
2023-01-05 10:09:34,332 Stage-21 map = 96%,  reduce = 0%, Cumulative CPU 285.55 sec
2023-01-05 10:09:35,749 Stage-23 map = 12%,  reduce = 0%, Cumulative CPU 84.97 sec
2023-01-05 10:09:36,780 Stage-23 map = 13%,  reduce = 0%, Cumulative CPU 86.43 sec
2023-01-05 10:09:39,436 Stage-21 map = 97%,  reduce = 0%, Cumulative CPU 287.76 sec
2023-01-05 10:09:40,955 Stage-23 map = 14%,  reduce = 0%, Cumulative CPU 89.89 sec
2023-01-05 10:09:41,979 Stage-23 map = 15%,  reduce = 0%, Cumulative CPU 93.79 sec
2023-01-05 10:09:42,999 Stage-23 map = 16%,  reduce = 0%, Cumulative CPU 96.4 sec
2023-01-05 10:09:45,063 Stage-23 map = 17%,  reduce = 0%, Cumulative CPU 98.21 sec
2023-01-05 10:09:45,638 Stage-21 map = 98%,  reduce = 0%, Cumulative CPU 290.68 sec
2023-01-05 10:09:47,115 Stage-23 map = 18%,  reduce = 0%, Cumulative CPU 100.99 sec
2023-01-05 10:09:48,147 Stage-23 map = 19%,  reduce = 0%, Cumulative CPU 104.22 sec
2023-01-05 10:09:51,237 Stage-23 map = 20%,  reduce = 0%, Cumulative CPU 109.15 sec
2023-01-05 10:09:51,785 Stage-21 map = 99%,  reduce = 0%, Cumulative CPU 292.84 sec
2023-01-05 10:09:53,360 Stage-23 map = 22%,  reduce = 0%, Cumulative CPU 115.43 sec
2023-01-05 10:09:54,848 Stage-21 map = 100%,  reduce = 0%, Cumulative CPU 294.09 sec
2023-01-05 10:09:55,393 Stage-23 map = 23%,  reduce = 0%, Cumulative CPU 117.98 sec
MapReduce Total cumulative CPU time: 4 minutes 54 seconds 90 msec
Ended Job = job_1672890466700_0146
Stage-28 is selected by condition resolver.
Stage-29 is filtered out by condition resolver.
Stage-2 is filtered out by condition resolver.
2023-01-05 10:09:57,444 Stage-23 map = 24%,  reduce = 0%, Cumulative CPU 119.69 sec
2023-01-05 10:09:59,492 Stage-23 map = 25%,  reduce = 0%, Cumulative CPU 125.21 sec

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 10:10:01,570 Stage-23 map = 26%,  reduce = 0%, Cumulative CPU 128.8 sec
2023-01-05 10:10:03,616 Stage-23 map = 27%,  reduce = 0%, Cumulative CPU 130.94 sec
2023-01-05 10:10:05,643 Stage-23 map = 29%,  reduce = 0%, Cumulative CPU 135.86 sec
2023-01-05 10:10:06,658 Stage-23 map = 30%,  reduce = 0%, Cumulative CPU 138.75 sec

2023-01-05 10:10:08,705 Stage-23 map = 31%,  reduce = 0%, Cumulative CPU 140.45 sec

2023-01-05 10:10:10,916 Stage-23 map = 32%,  reduce = 0%, Cumulative CPU 140.45 sec
2023-01-05 10:10:11	Processing rows:	400000	Hashtable size:	399999	Memory usage:	165792704	percentage:	0.693
2023-01-05 10:10:12,023 Stage-23 map = 33%,  reduce = 0%, Cumulative CPU 145.53 sec
2023-01-05 10:10:13	Uploaded 1 File to: file:/tmp/hdfs/2665b03d-a983-4a60-9a95-18e8cb81d295/hive_2023-01-05_10-05-02_272_4579854186527812518-1/-local-10016/HashTable-Stage-18/MapJoin-mapfile21--.hashtable (12168050 bytes)
Execution completed successfully
MapredLocal task succeeded
2023-01-05 10:10:15,168 Stage-23 map = 34%,  reduce = 0%, Cumulative CPU 151.74 sec
Launching Job 5 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator
2023-01-05 10:10:17,199 Stage-23 map = 35%,  reduce = 0%, Cumulative CPU 154.04 sec
2023-01-05 10:10:18,210 Stage-23 map = 36%,  reduce = 0%, Cumulative CPU 155.88 sec
2023-01-05 10:10:19,220 Stage-23 map = 37%,  reduce = 0%, Cumulative CPU 158.49 sec
Starting Job = job_1672890466700_0149, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0149/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0149
2023-01-05 10:10:23,314 Stage-23 map = 39%,  reduce = 0%, Cumulative CPU 164.51 sec
2023-01-05 10:10:27,515 Stage-23 map = 40%,  reduce = 0%, Cumulative CPU 168.66 sec
2023-01-05 10:10:29,615 Stage-23 map = 42%,  reduce = 0%, Cumulative CPU 171.79 sec
2023-01-05 10:10:32,735 Stage-23 map = 43%,  reduce = 0%, Cumulative CPU 176.32 sec
Hadoop job information for Stage-18: number of mappers: 1; number of reducers: 0
2023-01-05 10:10:32,780 Stage-18 map = 0%,  reduce = 0%
2023-01-05 10:10:35,832 Stage-23 map = 44%,  reduce = 0%, Cumulative CPU 180.37 sec
2023-01-05 10:10:36,852 Stage-23 map = 45%,  reduce = 0%, Cumulative CPU 182.53 sec
2023-01-05 10:10:41,053 Stage-23 map = 46%,  reduce = 0%, Cumulative CPU 187.38 sec
2023-01-05 10:10:42,070 Stage-23 map = 47%,  reduce = 0%, Cumulative CPU 188.92 sec
2023-01-05 10:10:44,213 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 6.16 sec
2023-01-05 10:10:45,126 Stage-23 map = 48%,  reduce = 0%, Cumulative CPU 193.43 sec
MapReduce Total cumulative CPU time: 6 seconds 160 msec
Ended Job = job_1672890466700_0149
2023-01-05 10:10:48,174 Stage-23 map = 49%,  reduce = 0%, Cumulative CPU 198.2 sec
2023-01-05 10:10:49,192 Stage-23 map = 50%,  reduce = 0%, Cumulative CPU 201.05 sec
2023-01-05 10:10:53,267 Stage-23 map = 51%,  reduce = 0%, Cumulative CPU 206.93 sec
2023-01-05 10:10:55,360 Stage-23 map = 53%,  reduce = 0%, Cumulative CPU 211.2 sec
2023-01-05 10:10:59,434 Stage-23 map = 55%,  reduce = 0%, Cumulative CPU 220.7 sec
2023-01-05 10:11:01,466 Stage-23 map = 56%,  reduce = 0%, Cumulative CPU 223.06 sec
2023-01-05 10:11:03,500 Stage-23 map = 57%,  reduce = 0%, Cumulative CPU 226.61 sec
2023-01-05 10:11:05,541 Stage-23 map = 58%,  reduce = 0%, Cumulative CPU 231.72 sec
2023-01-05 10:11:07,570 Stage-23 map = 59%,  reduce = 0%, Cumulative CPU 233.81 sec
2023-01-05 10:11:09,611 Stage-23 map = 60%,  reduce = 0%, Cumulative CPU 236.77 sec
2023-01-05 10:11:11,649 Stage-23 map = 62%,  reduce = 0%, Cumulative CPU 241.75 sec
2023-01-05 10:11:13,718 Stage-23 map = 63%,  reduce = 0%, Cumulative CPU 243.85 sec
2023-01-05 10:11:17,788 Stage-23 map = 65%,  reduce = 0%, Cumulative CPU 250.61 sec
2023-01-05 10:11:18,803 Stage-23 map = 66%,  reduce = 0%, Cumulative CPU 252.5 sec
2023-01-05 10:11:20,840 Stage-23 map = 67%,  reduce = 0%, Cumulative CPU 255.57 sec
2023-01-05 10:11:23,924 Stage-23 map = 69%,  reduce = 0%, Cumulative CPU 260.21 sec
2023-01-05 10:11:24,936 Stage-23 map = 70%,  reduce = 0%, Cumulative CPU 262.03 sec
2023-01-05 10:11:26,957 Stage-23 map = 71%,  reduce = 0%, Cumulative CPU 264.74 sec
2023-01-05 10:11:28,984 Stage-23 map = 72%,  reduce = 0%, Cumulative CPU 266.34 sec
2023-01-05 10:11:30,005 Stage-23 map = 73%,  reduce = 0%, Cumulative CPU 267.79 sec
2023-01-05 10:11:31,017 Stage-23 map = 74%,  reduce = 0%, Cumulative CPU 269.0 sec
2023-01-05 10:11:33,046 Stage-23 map = 75%,  reduce = 0%, Cumulative CPU 271.11 sec
2023-01-05 10:11:35,081 Stage-23 map = 76%,  reduce = 0%, Cumulative CPU 273.94 sec
2023-01-05 10:11:37,128 Stage-23 map = 77%,  reduce = 0%, Cumulative CPU 277.36 sec
2023-01-05 10:11:39,164 Stage-23 map = 78%,  reduce = 0%, Cumulative CPU 280.19 sec
2023-01-05 10:11:41,207 Stage-23 map = 79%,  reduce = 0%, Cumulative CPU 282.9 sec
2023-01-05 10:11:42,224 Stage-23 map = 80%,  reduce = 0%, Cumulative CPU 284.51 sec
2023-01-05 10:11:43,239 Stage-23 map = 81%,  reduce = 0%, Cumulative CPU 285.68 sec
2023-01-05 10:11:45,274 Stage-23 map = 82%,  reduce = 0%, Cumulative CPU 287.63 sec
2023-01-05 10:11:47,306 Stage-23 map = 83%,  reduce = 0%, Cumulative CPU 290.63 sec
2023-01-05 10:11:48,325 Stage-23 map = 84%,  reduce = 0%, Cumulative CPU 291.81 sec
2023-01-05 10:11:51,387 Stage-23 map = 85%,  reduce = 0%, Cumulative CPU 294.13 sec
2023-01-05 10:11:53,418 Stage-23 map = 87%,  reduce = 0%, Cumulative CPU 298.74 sec
2023-01-05 10:11:55,458 Stage-23 map = 88%,  reduce = 0%, Cumulative CPU 300.01 sec
2023-01-05 10:11:57,490 Stage-23 map = 89%,  reduce = 0%, Cumulative CPU 301.34 sec
2023-01-05 10:11:59,530 Stage-23 map = 91%,  reduce = 0%, Cumulative CPU 306.78 sec
2023-01-05 10:12:01,555 Stage-23 map = 92%,  reduce = 0%, Cumulative CPU 308.32 sec
2023-01-05 10:12:03,599 Stage-23 map = 93%,  reduce = 0%, Cumulative CPU 310.34 sec
2023-01-05 10:12:05,634 Stage-23 map = 95%,  reduce = 0%, Cumulative CPU 316.71 sec
2023-01-05 10:12:07,661 Stage-23 map = 97%,  reduce = 0%, Cumulative CPU 318.66 sec
2023-01-05 10:12:11,715 Stage-23 map = 98%,  reduce = 0%, Cumulative CPU 321.54 sec
2023-01-05 10:12:17,795 Stage-23 map = 99%,  reduce = 0%, Cumulative CPU 323.96 sec
2023-01-05 10:12:24,882 Stage-23 map = 100%,  reduce = 0%, Cumulative CPU 327.41 sec
MapReduce Total cumulative CPU time: 5 minutes 27 seconds 410 msec
Ended Job = job_1672890466700_0148
Stage-26 is filtered out by condition resolver.
Stage-27 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 10:12:34	Starting to launch local task to process map join;	maximum memory = 239075328

2023-01-05 10:12:35	End of local task; Time Taken: 0.419 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 7 out of 11
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0150, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0150/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0150
Hadoop job information for Stage-16: number of mappers: 1; number of reducers: 0
2023-01-05 10:12:44,859 Stage-16 map = 0%,  reduce = 0%
2023-01-05 10:12:53,992 Stage-16 map = 100%,  reduce = 0%, Cumulative CPU 2.94 sec
MapReduce Total cumulative CPU time: 2 seconds 940 msec
Ended Job = job_1672890466700_0150
Launching Job 8 out of 11
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0151, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0151/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0151
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 10:13:02,664 Stage-4 map = 0%,  reduce = 0%
2023-01-05 10:13:06,739 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 0.92 sec
2023-01-05 10:13:10,800 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 3.16 sec
MapReduce Total cumulative CPU time: 3 seconds 160 msec
Ended Job = job_1672890466700_0151
Launching Job 9 out of 11
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0152, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0152/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0152
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 10:13:19,506 Stage-5 map = 0%,  reduce = 0%
2023-01-05 10:13:25,621 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.03 sec
2023-01-05 10:13:29,701 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.14 sec
MapReduce Total cumulative CPU time: 2 seconds 140 msec
Ended Job = job_1672890466700_0152
MapReduce Jobs Launched:
Stage-Stage-22: Map: 1   Cumulative CPU: 130.72 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-21: Map: 3   Cumulative CPU: 294.09 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-18: Map: 1   Cumulative CPU: 6.16 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-23: Map: 4   Cumulative CPU: 327.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-16: Map: 1   Cumulative CPU: 2.94 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 3.16 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.14 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 12 minutes 46 seconds 620 msec
OK
AAAAAAAAAGPBAAAA	Rights sense once gastric streets. Popular, normal animals resist also now certain settings. Easy restaurants perc	TN	1	86.0	NULL	NULL	1	21.0	NULL	NULL	1	67.0	NULL	NULL
AAAAAAAADOGOAAAA	Both aware patients could not persuade culturally through a change	TN	1	65.0	NULL	NULL	1	25.0	NULL	NULL	1	4.0	NULL	NULL
AAAAAAAAEFICBAAA	Enormous words work especially for the heart	AL	1	74.0	NULL	NULL	1	1.0	NULL	NULL	1	41.0	NULL	NULL
AAAAAAAAELOCBAAA	In general likel	SD	1	59.0	NULL	NULL	1	14.0	NULL	NULL	1	62.0	NULL	NULL
AAAAAAAAIPMIBAAA	Common problems distract also artists. Especially mad sections think particularly possible, special symptoms. Happy things uncover at a	AL	1	78.0	NULL	NULL	1	68.0	NULL	NULL	1	21.0	NULL	NULL
AAAAAAAAJKLNAAAA	At last equal resources demonstrate	AL	1	84.0	NULL	NULL	1	76.0	NULL	NULL	1	71.0	NULL	NULL
AAAAAAAAOPKOAAAA	Russian, likely years try in aid of the applications. Completely alone points will not take afterwards fortunate 	SD	1	20.0	NULL	NULL	1	4.0	NULL	NULL	1	24.0	NULL	NULL
Time taken: 509.212 seconds, Fetched: 7 row(s)
timediff:514.967811665
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query18.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = bd20a8c8-f2c8-43c6-90b7-83398003366b

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 4158ccd4-0308-4efd-b03b-880b74c6990b
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_net_profit, cs_bill_cdemo_sk, cs_item_sk, cs_quantity, cs_sales_price, cs_coupon_amt, cs_list_price, cs_bill_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_demographics, Columns: cd_demo_sk, cd_education_status, cd_dep_count, cd_gender
No Stats for tpcds_bin_partitioned_orc_10@customer_demographics, Columns: cd_demo_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_current_addr_sk, c_customer_sk, c_current_cdemo_sk, c_birth_month, c_birth_year
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_county, ca_state, ca_address_sk, ca_country
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_id, i_item_sk
Query ID = hdfs_20230105101336_66d565a2-8b84-4cca-be06-307827a9ed75
Total jobs = 7
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]


SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]


2023-01-05 10:13:58	Processing rows:	300000	Hashtable size:	299999	Memory usage:	88806392	percentage:	0.371
2023-01-05 10:13:58	Processing rows:	500000	Hashtable size:	499999	Memory usage:	108941536	percentage:	0.456
2023-01-05 10:13:59	Processing rows:	700000	Hashtable size:	699999	Memory usage:	125433840	percentage:	0.525
2023-01-05 10:13:59	Processing rows:	900000	Hashtable size:	899999	Memory usage:	152208888	percentage:	0.637
2023-01-05 10:14:01	Processing rows:	1100000	Hashtable size:	1099999	Memory usage:	167298928	percentage:	0.70
2023-01-05 10:14:03	Processing rows:	1300000	Hashtable size:	1299999	Memory usage:	184415744	percentage:	0.771
2023-01-05 10:14:06	Processing rows:	1500000	Hashtable size:	1499999	Memory usage:	202239792	percentage:	0.846
2023-01-05 10:14:14	Dump the side-table for tag: 1 with group count: 365 into file: file:/tmp/hdfs/bd20a8c8-f2c8-43c6-90b7-83398003366b/hive_2023-01-05_10-13-36_993_6326998024310804398-1/-local-10019/HashTable-Stage-20/MapJoin-mapfile51--.hashtable2023-01-05 10:14:15	End of local task; Time Taken: 20.359 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 7
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0153, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0153/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0153
Hadoop job information for Stage-20: number of mappers: 3; number of reducers: 0
2023-01-05 10:14:59,228 Stage-20 map = 0%,  reduce = 0%
Execution failed with exit status: 3
Obtaining error information

Task failed!
Task ID:
  Stage-25

Logs:

FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
2023-01-05 10:15:09,241 Stage-20 map = 100%,  reduce = 0%
Ended Job = job_1672890466700_0153 with errors
Error during job, obtaining debugging information...
Examining task ID: task_1672890466700_0153_m_000000 (and more) from job job_1672890466700_0153
killing job with: job_1672890466700_0153

Task with the most failures(1):
-----
Task ID:
  task_1672890466700_0153_m_000002

URL:
  http://my-hadoop-yarn-rm:8088/taskdetails.jsp?jobid=job_1672890466700_0153&tipid=task_1672890466700_0153_m_000002
-----
Diagnostic Messages for this Task:
Task KILL is received. Killing attempt!
[2023-01-05 10:15:08.645]Container killed by the ApplicationMaster.
[2023-01-05 10:15:09.012]Container killed on request. Exit code is 143
[2023-01-05 10:15:09.024]Container exited with a non-zero exit code 143.


timediff:99.578757713
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query19.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = f23857ba-b360-49db-9f10-f8543015f56a

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = bafa3929-d6b2-479f-bb8c-d1074c46ab8e
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_moy, d_date_sk, d_year
No Stats for tpcds_bin_partitioned_orc_10@store_sales, Columns: ss_customer_sk, ss_ext_sales_price, ss_item_sk, ss_store_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_manager_id, i_brand, i_manufact_id, i_brand_id, i_manufact, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@customer, Columns: c_current_addr_sk, c_customer_sk
No Stats for tpcds_bin_partitioned_orc_10@customer_address, Columns: ca_zip, ca_address_sk
No Stats for tpcds_bin_partitioned_orc_10@store, Columns: s_zip, s_store_sk
Query ID = hdfs_20230105101516_b8030ab2-c4d3-4fc6-a60a-008da600831b
Total jobs = 6

SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 10:15:32	Dump the side-table for tag: 1 with group count: 31 into file: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10016/HashTable-Stage-18/MapJoin-mapfile41--.hashtable2023-01-05 10:15:32	Uploaded 1 File to: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10016/HashTable-Stage-18/MapJoin-mapfile41--.hashtable (914 bytes)
2023-01-05 10:15:32	Dump the side-table for tag: 1 with group count: 1807 into file: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10016/HashTable-Stage-18/MapJoin-mapfile51--.hashtable2023-01-05 10:15:32	Uploaded 1 File to: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10016/HashTable-Stage-18/MapJoin-mapfile51--.hashtable (235307 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 6
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0154, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0154/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0154
Hadoop job information for Stage-18: number of mappers: 4; number of reducers: 0
2023-01-05 10:15:48,722 Stage-18 map = 0%,  reduce = 0%
2023-01-05 10:16:05,613 Stage-18 map = 1%,  reduce = 0%, Cumulative CPU 8.86 sec
2023-01-05 10:16:10,917 Stage-18 map = 2%,  reduce = 0%, Cumulative CPU 19.85 sec
2023-01-05 10:16:12,977 Stage-18 map = 3%,  reduce = 0%, Cumulative CPU 40.49 sec
2023-01-05 10:16:14,011 Stage-18 map = 4%,  reduce = 0%, Cumulative CPU 43.65 sec
2023-01-05 10:16:17,130 Stage-18 map = 6%,  reduce = 0%, Cumulative CPU 48.72 sec
2023-01-05 10:16:19,169 Stage-18 map = 7%,  reduce = 0%, Cumulative CPU 50.31 sec
2023-01-05 10:16:20,188 Stage-18 map = 8%,  reduce = 0%, Cumulative CPU 53.05 sec
2023-01-05 10:16:23,247 Stage-18 map = 10%,  reduce = 0%, Cumulative CPU 56.39 sec
2023-01-05 10:16:25,286 Stage-18 map = 11%,  reduce = 0%, Cumulative CPU 57.63 sec
2023-01-05 10:16:26,304 Stage-18 map = 12%,  reduce = 0%, Cumulative CPU 59.35 sec
2023-01-05 10:16:29,350 Stage-18 map = 13%,  reduce = 0%, Cumulative CPU 62.19 sec
2023-01-05 10:16:31,407 Stage-18 map = 14%,  reduce = 0%, Cumulative CPU 63.29 sec
2023-01-05 10:16:32,435 Stage-18 map = 16%,  reduce = 0%, Cumulative CPU 65.52 sec
2023-01-05 10:16:35,566 Stage-18 map = 17%,  reduce = 0%, Cumulative CPU 69.51 sec
2023-01-05 10:16:37,607 Stage-18 map = 19%,  reduce = 0%, Cumulative CPU 74.41 sec
2023-01-05 10:16:40,666 Stage-18 map = 20%,  reduce = 0%, Cumulative CPU 75.52 sec
2023-01-05 10:16:41,678 Stage-18 map = 21%,  reduce = 0%, Cumulative CPU 77.75 sec
2023-01-05 10:16:43,720 Stage-18 map = 23%,  reduce = 0%, Cumulative CPU 82.57 sec
2023-01-05 10:16:46,775 Stage-18 map = 24%,  reduce = 0%, Cumulative CPU 83.86 sec
2023-01-05 10:16:47,799 Stage-18 map = 25%,  reduce = 0%, Cumulative CPU 86.35 sec
2023-01-05 10:16:48,821 Stage-18 map = 26%,  reduce = 0%, Cumulative CPU 88.92 sec
2023-01-05 10:16:49,847 Stage-18 map = 27%,  reduce = 0%, Cumulative CPU 90.8 sec
2023-01-05 10:16:52,892 Stage-18 map = 28%,  reduce = 0%, Cumulative CPU 92.26 sec
2023-01-05 10:16:53,910 Stage-18 map = 29%,  reduce = 0%, Cumulative CPU 94.82 sec
2023-01-05 10:16:55,962 Stage-18 map = 31%,  reduce = 0%, Cumulative CPU 98.8 sec
2023-01-05 10:16:59,084 Stage-18 map = 32%,  reduce = 0%, Cumulative CPU 100.58 sec
2023-01-05 10:17:01,157 Stage-18 map = 33%,  reduce = 0%, Cumulative CPU 106.08 sec
2023-01-05 10:17:02,178 Stage-18 map = 34%,  reduce = 0%, Cumulative CPU 107.82 sec
2023-01-05 10:17:05,226 Stage-18 map = 36%,  reduce = 0%, Cumulative CPU 113.32 sec
2023-01-05 10:17:07,278 Stage-18 map = 37%,  reduce = 0%, Cumulative CPU 116.26 sec
2023-01-05 10:17:08,298 Stage-18 map = 38%,  reduce = 0%, Cumulative CPU 118.08 sec
2023-01-05 10:17:11,356 Stage-18 map = 40%,  reduce = 0%, Cumulative CPU 123.15 sec
2023-01-05 10:17:13,401 Stage-18 map = 41%,  reduce = 0%, Cumulative CPU 125.87 sec
2023-01-05 10:17:14,415 Stage-18 map = 42%,  reduce = 0%, Cumulative CPU 127.28 sec
2023-01-05 10:17:17,462 Stage-18 map = 44%,  reduce = 0%, Cumulative CPU 131.88 sec
2023-01-05 10:17:20,539 Stage-18 map = 45%,  reduce = 0%, Cumulative CPU 135.56 sec
2023-01-05 10:17:23,656 Stage-18 map = 47%,  reduce = 0%, Cumulative CPU 140.35 sec
2023-01-05 10:17:25,697 Stage-18 map = 49%,  reduce = 0%, Cumulative CPU 146.7 sec
2023-01-05 10:17:28,766 Stage-18 map = 50%,  reduce = 0%, Cumulative CPU 149.8 sec
2023-01-05 10:17:29,780 Stage-18 map = 51%,  reduce = 0%, Cumulative CPU 152.05 sec
2023-01-05 10:17:31,810 Stage-18 map = 52%,  reduce = 0%, Cumulative CPU 157.95 sec
2023-01-05 10:17:34,883 Stage-18 map = 53%,  reduce = 0%, Cumulative CPU 161.54 sec
2023-01-05 10:17:35,911 Stage-18 map = 54%,  reduce = 0%, Cumulative CPU 163.2 sec
2023-01-05 10:17:36,929 Stage-18 map = 55%,  reduce = 0%, Cumulative CPU 164.59 sec
2023-01-05 10:17:37,942 Stage-18 map = 56%,  reduce = 0%, Cumulative CPU 167.02 sec
2023-01-05 10:17:41,009 Stage-18 map = 57%,  reduce = 0%, Cumulative CPU 169.6 sec
2023-01-05 10:17:42,032 Stage-18 map = 58%,  reduce = 0%, Cumulative CPU 171.49 sec
2023-01-05 10:17:44,062 Stage-18 map = 59%,  reduce = 0%, Cumulative CPU 175.67 sec
2023-01-05 10:17:47,125 Stage-18 map = 60%,  reduce = 0%, Cumulative CPU 177.95 sec
2023-01-05 10:17:48,138 Stage-18 map = 61%,  reduce = 0%, Cumulative CPU 179.41 sec
2023-01-05 10:17:49,151 Stage-18 map = 62%,  reduce = 0%, Cumulative CPU 181.05 sec
2023-01-05 10:17:50,168 Stage-18 map = 63%,  reduce = 0%, Cumulative CPU 183.79 sec
2023-01-05 10:17:53,213 Stage-18 map = 65%,  reduce = 0%, Cumulative CPU 186.62 sec
2023-01-05 10:17:55,246 Stage-18 map = 66%,  reduce = 0%, Cumulative CPU 187.88 sec
2023-01-05 10:17:56,262 Stage-18 map = 67%,  reduce = 0%, Cumulative CPU 190.7 sec
2023-01-05 10:17:59,309 Stage-18 map = 69%,  reduce = 0%, Cumulative CPU 194.15 sec
2023-01-05 10:18:01,336 Stage-18 map = 70%,  reduce = 0%, Cumulative CPU 195.24 sec
2023-01-05 10:18:05,415 Stage-18 map = 73%,  reduce = 0%, Cumulative CPU 201.62 sec
2023-01-05 10:18:08,481 Stage-18 map = 74%,  reduce = 0%, Cumulative CPU 205.26 sec
2023-01-05 10:18:11,521 Stage-18 map = 77%,  reduce = 0%, Cumulative CPU 209.02 sec
2023-01-05 10:18:14,579 Stage-18 map = 78%,  reduce = 0%, Cumulative CPU 211.31 sec
2023-01-05 10:18:17,627 Stage-18 map = 80%,  reduce = 0%, Cumulative CPU 213.83 sec
2023-01-05 10:18:19,654 Stage-18 map = 82%,  reduce = 0%, Cumulative CPU 216.53 sec
2023-01-05 10:18:23,717 Stage-18 map = 84%,  reduce = 0%, Cumulative CPU 219.07 sec
2023-01-05 10:18:25,739 Stage-18 map = 86%,  reduce = 0%, Cumulative CPU 222.1 sec
2023-01-05 10:18:28,777 Stage-18 map = 88%,  reduce = 0%, Cumulative CPU 223.74 sec
2023-01-05 10:18:29,790 Stage-18 map = 89%,  reduce = 0%, Cumulative CPU 226.3 sec
2023-01-05 10:18:31,813 Stage-18 map = 90%,  reduce = 0%, Cumulative CPU 227.48 sec
2023-01-05 10:18:35,891 Stage-18 map = 91%,  reduce = 0%, Cumulative CPU 230.55 sec
2023-01-05 10:18:36,911 Stage-18 map = 93%,  reduce = 0%, Cumulative CPU 232.08 sec
2023-01-05 10:18:41,992 Stage-18 map = 94%,  reduce = 0%, Cumulative CPU 235.01 sec
2023-01-05 10:18:43,019 Stage-18 map = 95%,  reduce = 0%, Cumulative CPU 236.2 sec
2023-01-05 10:18:48,125 Stage-18 map = 96%,  reduce = 0%, Cumulative CPU 238.69 sec
2023-01-05 10:18:49,139 Stage-18 map = 97%,  reduce = 0%, Cumulative CPU 239.67 sec
2023-01-05 10:18:53,187 Stage-18 map = 98%,  reduce = 0%, Cumulative CPU 242.7 sec
2023-01-05 10:18:55,221 Stage-18 map = 99%,  reduce = 0%, Cumulative CPU 243.8 sec
2023-01-05 10:18:57,245 Stage-18 map = 100%,  reduce = 0%, Cumulative CPU 244.07 sec
MapReduce Total cumulative CPU time: 4 minutes 4 seconds 70 msec
Ended Job = job_1672890466700_0154
Stage-21 is filtered out by condition resolver.
Stage-22 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]2023-01-05 10:19:06	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 10:19:07	End of local task; Time Taken: 1.073 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 3 out of 6
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1672890466700_0155, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0155/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0155
Hadoop job information for Stage-16: number of mappers: 1; number of reducers: 0
2023-01-05 10:19:17,263 Stage-16 map = 0%,  reduce = 0%
2023-01-05 10:19:25,396 Stage-16 map = 100%,  reduce = 0%, Cumulative CPU 6.06 sec
MapReduce Total cumulative CPU time: 6 seconds 60 msec
Ended Job = job_1672890466700_0155
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2023-01-05 10:19:35	Processing rows:	200000	Hashtable size:	199999	Memory usage:	117531664	percentage:	0.492
2023-01-05 10:19:35	Uploaded 1 File to: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10010/HashTable-Stage-6/MapJoin-mapfile01--.hashtable (3322 bytes)
2023-01-05 10:19:35	Uploaded 1 File to: file:/tmp/hdfs/f23857ba-b360-49db-9f10-f8543015f56a/hive_2023-01-05_10-15-16_656_6996398847164726389-1/-local-10010/HashTable-Stage-6/MapJoin-mapfile11--.hashtable (8140307 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 4 out of 6
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0156, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0156/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0156
Hadoop job information for Stage-6: number of mappers: 1; number of reducers: 1
2023-01-05 10:19:44,404 Stage-6 map = 0%,  reduce = 0%
2023-01-05 10:19:53,610 Stage-6 map = 100%,  reduce = 0%, Cumulative CPU 7.65 sec
2023-01-05 10:19:56,663 Stage-6 map = 100%,  reduce = 100%, Cumulative CPU 8.79 sec
MapReduce Total cumulative CPU time: 8 seconds 790 msec
Ended Job = job_1672890466700_0156
Launching Job 5 out of 6
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0157, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0157/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0157
Hadoop job information for Stage-7: number of mappers: 1; number of reducers: 1
2023-01-05 10:20:05,097 Stage-7 map = 0%,  reduce = 0%
2023-01-05 10:20:11,242 Stage-7 map = 100%,  reduce = 0%, Cumulative CPU 1.14 sec
2023-01-05 10:20:15,331 Stage-7 map = 100%,  reduce = 100%, Cumulative CPU 2.32 sec
MapReduce Total cumulative CPU time: 2 seconds 320 msec
Ended Job = job_1672890466700_0157
MapReduce Jobs Launched:
Stage-Stage-18: Map: 4   Cumulative CPU: 244.07 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-16: Map: 1   Cumulative CPU: 6.06 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-6: Map: 1  Reduce: 1   Cumulative CPU: 8.79 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-7: Map: 1  Reduce: 1   Cumulative CPU: 2.32 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 minutes 21 seconds 240 msec
OK
2004002	edu packimporto #2                                	280	bareingable                                       	91162.87
1004001	edu packamalg #1                                  	82	ableeing                                          	77975.50
4002001	importoedu pack #1                                	567	ationcallyanti                                    	68039.52
7002009	importobrand #9                                   	304	esebarpri                                         	67152.84
2001001	amalgimporto #1                                   	109	n stbarought                                      	67116.00
2001002	amalgimporto #2                                   	74	eseation                                          	65925.72
1003002	exportiamalg #2                                   	60	barcally                                          	65750.98
6005007	scholarcorp #7                                    	332	ablepripri                                        	65616.00
7009002	maxibrand #2                                      	54	eseanti                                           	65605.76
4002001	importoedu pack #1                                	595	antin stanti                                      	65195.29
7013002	exportinameless #2                                	318	eingoughtpri                                      	65164.82
3001002	amalgexporti #2                                   	302	ablebarpri                                        	64673.54
3002001	importoexporti #1                                 	515	antioughtanti                                     	63552.40
3001001	amalgexporti #1                                   	157	ationantiought                                    	63435.71
9010008	univunivamalg #8                                  	10	barought                                          	63269.49
4003001	exportiedu pack #1                                	219	n stoughtable                                     	62167.45
6008001	namelesscorp #1                                   	43	priese                                            	61501.60
9006011	corpmaxi #11                                      	684	eseeingcally                                      	61142.01
8012010	importomaxi #10                                   	224	eseableable                                       	60717.47
8008009	namelessnameless #9                               	78	eingation                                         	59796.56
8004003	edu packnameless #3                               	286	callyeingable                                     	59440.00
3001002	amalgexporti #2                                   	419	n stoughtese                                      	58986.77
9015009	scholarunivamalg #9                               	244	eseeseable                                        	58721.40
3004001	edu packexporti #1                                	59	n stanti                                          	58600.53
8012008	importomaxi #8                                    	82	ableeing                                          	58456.38
7002006	importobrand #6                                   	245	antieseable                                       	58297.54
3001002	amalgexporti #2                                   	93	prin st                                           	57643.28
7006008	corpbrand #8                                      	291	oughtn stable                                     	57595.98
9006011	corpmaxi #11                                      	291	oughtn stable                                     	56922.17
10007016	brandunivamalg #16                                	299	n stn stable                                      	56911.21
9009005	maximaxi #5                                       	19	n stought                                         	56797.74
2002001	importoimporto #1                                 	202	ablebarable                                       	56664.07
2004001	edu packimporto #1                                	78	eingation                                         	56622.24
8015005	scholarmaxi #5                                    	3	pri                                               	56342.55
6009005	maxicorp #5                                       	297	ationn stable                                     	55810.26
10010008	univamalgamalg #8                                 	116	callyoughtought                                   	55435.50
6015005	scholarbrand #5                                   	133	pripriought                                       	55337.57
6012008	importobrand #8                                   	919	n stoughtn st                                     	55267.18
5001002	amalgscholar #2                                   	8	eing                                              	54714.98
1004002	edu packamalg #2                                  	95	antin st                                          	54500.75
1004001	edu packamalg #1                                  	63	prically                                          	53882.26
5004001	edu packscholar #1                                	122	ableableought                                     	53071.40
8002003	importonameless #3                                	131	oughtpriought                                     	52727.83
7004001	edu packbrand #1                                  	151	oughtantiought                                    	52673.18
10003012	exportiunivamalg #12                              	124	eseableought                                      	52565.95
8011010	amalgmaxi #10                                     	144	eseeseought                                       	52528.82
5003001	exportischolar #1                                 	613	prioughtcally                                     	52459.36
3004001	edu packexporti #1                                	447	ationeseese                                       	52456.54
4001001	amalgedu pack #1                                  	545	antieseanti                                       	52392.56
7003006	exportibrand #6                                   	233	pripriable                                        	52316.96
7009009	maxibrand #9                                      	264	esecallyable                                      	52259.93
5004002	edu packscholar #2                                	282	ableeingable                                      	52236.36
8016009	corpmaxi #9                                       	121	oughtableought                                    	52231.64
7015006	scholarnameless #6                                	390	barn stpri                                        	52090.25
7002001	importobrand #1                                   	404	esebarese                                         	51994.84
3003002	exportiexporti #2                                 	314	eseoughtpri                                       	51620.40
4004001	edu packedu pack #1                               	256	callyantiable                                     	51546.10
10008009	namelessunivamalg #9                              	548	eingeseanti                                       	51415.42
3002002	importoexporti #2                                 	303	pribarpri                                         	51110.95
8010005	univmaxi #5                                       	672	ableationcally                                    	51092.99
1003002	exportiamalg #2                                   	296	callyn stable                                     	51019.85
8011002	amalgmaxi #2                                      	340	baresepri                                         	50993.45
6011001	amalgbrand #1                                     	245	antieseable                                       	50987.29
10006010	corpunivamalg #10                                 	252	ableantiable                                      	50928.23
6007002	brandcorp #2                                      	50	baranti                                           	50770.92
1002001	importoamalg #1                                   	336	callypripri                                       	50704.36
2001001	amalgimporto #1                                   	161	oughtcallyought                                   	50608.96
8010007	univmaxi #7                                       	91	oughtn st                                         	50595.07
9006009	corpmaxi #9                                       	246	callyeseable                                      	50575.84
8012007	importomaxi #7                                    	383	prieingpri                                        	50557.70
2002001	importoimporto #1                                 	323	priablepri                                        	50501.64
7009003	maxibrand #3                                      	167	ationcallyought                                   	50349.39
9015005	scholarunivamalg #5                               	516	callyoughtanti                                    	50303.55
1004002	edu packamalg #2                                  	513	prioughtanti                                      	50160.19
3004002	edu packexporti #2                                	629	n stablecally                                     	50158.12
5001001	amalgscholar #1                                   	143	prieseought                                       	50102.00
8003004	exportinameless #4                                	607	ationbarcally                                     	50020.84
6006004	corpcorp #4                                       	51	oughtanti                                         	49958.70
2001001	amalgimporto #1                                   	70	baration                                          	49956.00
1003001	exportiamalg #1                                   	146	callyeseought                                     	49686.41
6011004	amalgbrand #4                                     	80	bareing                                           	49620.52
1002002	importoamalg #2                                   	589	n steinganti                                      	49428.77
9007011	brandmaxi #11                                     	105	antibarought                                      	49289.85
2004001	edu packimporto #1                                	544	eseeseanti                                        	49196.95
9012009	importounivamalg #9                               	570	barationanti                                      	49194.36
10004003	edu packunivamalg #3                              	271	oughtationable                                    	49189.96
5003001	exportischolar #1                                 	146	callyeseought                                     	49177.51
1004001	edu packamalg #1                                  	859	n stantieing                                      	49132.07
2001002	amalgimporto #2                                   	216	callyoughtable                                    	49034.00
9013008	exportiunivamalg #8                               	372	ableationpri                                      	49006.24
8007002	brandnameless #2                                  	392	ablen stpri                                       	48491.52
2001001	amalgimporto #1                                   	287	ationeingable                                     	48469.23
5002001	importoscholar #1                                 	222	ableableable                                      	48420.47
7008009	namelessbrand #9                                  	319	n stoughtpri                                      	48410.89
4001001	amalgedu pack #1                                  	570	barationanti                                      	48345.76
10005007	scholarunivamalg #7                               	82	ableeing                                          	48344.09
6010003	univbrand #3                                      	302	ablebarpri                                        	48178.87
7001009	amalgbrand #9                                     	518	eingoughtanti                                     	48109.29
4004002	edu packedu pack #2                               	205	antibarable                                       	48036.80
10004009	edu packunivamalg #9                              	212	ableoughtable                                     	47987.27
Time taken: 300.274 seconds, Fetched: 100 row(s)
timediff:305.838057090
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query20.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 7df8787c-6f96-49af-b04c-764c62129fa1

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 67297d6c-db9e-4fa2-9491-4094829d2a9c
No Stats for tpcds_bin_partitioned_orc_10@catalog_sales, Columns: cs_ext_sales_price, cs_item_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_desc, i_item_id, i_class, i_category, i_current_price, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date, d_date_sk
Query ID = hdfs_20230105102022_3197b90b-a83a-4d07-ab50-e9aea82da2ad
Total jobs = 3
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2023-01-05 10:20:36	Starting to launch local task to process map join;	maximum memory = 239075328
2023-01-05 10:20:39	Dump the side-table for tag: 1 with group count: 30669 into file: file:/tmp/hdfs/7df8787c-6f96-49af-b04c-764c62129fa1/hive_2023-01-05_10-20-22_411_448191014392154176-1/-local-10008/HashTable-Stage-3/MapJoin-mapfile11--.hashtable
2023-01-05 10:20:39	End of local task; Time Taken: 3.571 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0158, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0158/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0158
Hadoop job information for Stage-3: number of mappers: 3; number of reducers: 3
2023-01-05 10:20:57,017 Stage-3 map = 0%,  reduce = 0%
2023-01-05 10:21:35,576 Stage-3 map = 2%,  reduce = 0%, Cumulative CPU 54.85 sec
2023-01-05 10:21:36,590 Stage-3 map = 3%,  reduce = 0%, Cumulative CPU 66.89 sec
2023-01-05 10:21:41,657 Stage-3 map = 5%,  reduce = 0%, Cumulative CPU 72.44 sec
2023-01-05 10:21:42,672 Stage-3 map = 6%,  reduce = 0%, Cumulative CPU 76.81 sec
2023-01-05 10:21:47,740 Stage-3 map = 7%,  reduce = 0%, Cumulative CPU 83.21 sec
2023-01-05 10:21:48,752 Stage-3 map = 8%,  reduce = 0%, Cumulative CPU 88.53 sec
2023-01-05 10:21:53,818 Stage-3 map = 9%,  reduce = 0%, Cumulative CPU 94.76 sec
2023-01-05 10:21:54,833 Stage-3 map = 10%,  reduce = 0%, Cumulative CPU 100.06 sec
2023-01-05 10:21:59,898 Stage-3 map = 11%,  reduce = 0%, Cumulative CPU 107.66 sec
2023-01-05 10:22:00,911 Stage-3 map = 12%,  reduce = 0%, Cumulative CPU 113.23 sec
2023-01-05 10:22:05,974 Stage-3 map = 13%,  reduce = 0%, Cumulative CPU 122.67 sec
2023-01-05 10:22:06,987 Stage-3 map = 14%,  reduce = 0%, Cumulative CPU 127.04 sec
2023-01-05 10:22:12,052 Stage-3 map = 15%,  reduce = 0%, Cumulative CPU 134.33 sec
2023-01-05 10:22:13,063 Stage-3 map = 17%,  reduce = 0%, Cumulative CPU 138.6 sec
2023-01-05 10:22:18,131 Stage-3 map = 18%,  reduce = 0%, Cumulative CPU 144.76 sec
2023-01-05 10:22:19,148 Stage-3 map = 19%,  reduce = 0%, Cumulative CPU 148.37 sec
2023-01-05 10:22:23,200 Stage-3 map = 20%,  reduce = 0%, Cumulative CPU 152.2 sec
2023-01-05 10:22:24,212 Stage-3 map = 21%,  reduce = 0%, Cumulative CPU 153.72 sec
2023-01-05 10:22:25,224 Stage-3 map = 22%,  reduce = 0%, Cumulative CPU 157.54 sec
2023-01-05 10:22:29,269 Stage-3 map = 23%,  reduce = 0%, Cumulative CPU 160.57 sec
2023-01-05 10:22:30,281 Stage-3 map = 24%,  reduce = 0%, Cumulative CPU 162.2 sec
2023-01-05 10:22:31,293 Stage-3 map = 25%,  reduce = 0%, Cumulative CPU 165.9 sec
2023-01-05 10:22:35,347 Stage-3 map = 26%,  reduce = 0%, Cumulative CPU 169.78 sec
2023-01-05 10:22:36,359 Stage-3 map = 29%,  reduce = 0%, Cumulative CPU 177.66 sec
2023-01-05 10:22:41,418 Stage-3 map = 30%,  reduce = 0%, Cumulative CPU 181.53 sec
2023-01-05 10:22:42,430 Stage-3 map = 32%,  reduce = 0%, Cumulative CPU 191.89 sec
2023-01-05 10:22:47,492 Stage-3 map = 33%,  reduce = 0%, Cumulative CPU 195.29 sec
2023-01-05 10:22:48,504 Stage-3 map = 34%,  reduce = 0%, Cumulative CPU 203.76 sec
2023-01-05 10:22:53,568 Stage-3 map = 35%,  reduce = 0%, Cumulative CPU 206.63 sec
2023-01-05 10:22:54,579 Stage-3 map = 37%,  reduce = 0%, Cumulative CPU 214.87 sec
2023-01-05 10:22:59,643 Stage-3 map = 38%,  reduce = 0%, Cumulative CPU 219.4 sec
2023-01-05 10:23:00,657 Stage-3 map = 39%,  reduce = 0%, Cumulative CPU 227.8 sec
2023-01-05 10:23:05,720 Stage-3 map = 40%,  reduce = 0%, Cumulative CPU 231.95 sec
2023-01-05 10:23:06,732 Stage-3 map = 41%,  reduce = 0%, Cumulative CPU 242.85 sec
2023-01-05 10:23:11,793 Stage-3 map = 42%,  reduce = 0%, Cumulative CPU 248.41 sec
2023-01-05 10:23:12,805 Stage-3 map = 43%,  reduce = 0%, Cumulative CPU 258.38 sec
2023-01-05 10:23:17,864 Stage-3 map = 44%,  reduce = 0%, Cumulative CPU 264.2 sec
2023-01-05 10:23:18,875 Stage-3 map = 45%,  reduce = 0%, Cumulative CPU 273.52 sec
2023-01-05 10:23:23,936 Stage-3 map = 46%,  reduce = 0%, Cumulative CPU 278.49 sec
2023-01-05 10:23:24,947 Stage-3 map = 47%,  reduce = 0%, Cumulative CPU 286.56 sec
2023-01-05 10:23:28,997 Stage-3 map = 48%,  reduce = 0%, Cumulative CPU 291.55 sec
2023-01-05 10:23:31,020 Stage-3 map = 50%,  reduce = 0%, Cumulative CPU 297.57 sec
2023-01-05 10:23:35,069 Stage-3 map = 51%,  reduce = 0%, Cumulative CPU 301.57 sec
2023-01-05 10:23:37,091 Stage-3 map = 53%,  reduce = 0%, Cumulative CPU 308.42 sec
2023-01-05 10:23:43,166 Stage-3 map = 55%,  reduce = 0%, Cumulative CPU 318.25 sec
2023-01-05 10:23:47,216 Stage-3 map = 56%,  reduce = 0%, Cumulative CPU 323.05 sec
2023-01-05 10:23:48,227 Stage-3 map = 57%,  reduce = 0%, Cumulative CPU 325.01 sec
2023-01-05 10:23:49,237 Stage-3 map = 58%,  reduce = 0%, Cumulative CPU 328.54 sec
2023-01-05 10:23:50,248 Stage-3 map = 69%,  reduce = 0%, Cumulative CPU 329.39 sec
2023-01-05 10:23:53,328 Stage-3 map = 70%,  reduce = 0%, Cumulative CPU 332.53 sec
2023-01-05 10:23:54,353 Stage-3 map = 71%,  reduce = 0%, Cumulative CPU 334.73 sec
2023-01-05 10:23:56,449 Stage-3 map = 83%,  reduce = 0%, Cumulative CPU 336.69 sec
2023-01-05 10:24:00,576 Stage-3 map = 84%,  reduce = 0%, Cumulative CPU 338.67 sec
2023-01-05 10:24:06,675 Stage-3 map = 85%,  reduce = 7%, Cumulative CPU 340.56 sec
2023-01-05 10:24:08,701 Stage-3 map = 85%,  reduce = 15%, Cumulative CPU 341.14 sec
2023-01-05 10:24:10,727 Stage-3 map = 85%,  reduce = 22%, Cumulative CPU 341.73 sec
2023-01-05 10:24:12,767 Stage-3 map = 86%,  reduce = 22%, Cumulative CPU 343.45 sec
2023-01-05 10:24:18,862 Stage-3 map = 87%,  reduce = 22%, Cumulative CPU 345.32 sec
2023-01-05 10:24:24,944 Stage-3 map = 88%,  reduce = 22%, Cumulative CPU 346.84 sec
2023-01-05 10:24:31,045 Stage-3 map = 89%,  reduce = 22%, Cumulative CPU 348.47 sec
2023-01-05 10:24:32,058 Stage-3 map = 100%,  reduce = 22%, Cumulative CPU 349.68 sec
2023-01-05 10:24:33,076 Stage-3 map = 100%,  reduce = 74%, Cumulative CPU 353.44 sec
2023-01-05 10:24:34,088 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 355.41 sec
MapReduce Total cumulative CPU time: 5 minutes 55 seconds 410 msec
Ended Job = job_1672890466700_0158
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0159, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0159/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0159
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 10:24:42,353 Stage-4 map = 0%,  reduce = 0%
2023-01-05 10:24:48,435 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 1.57 sec
2023-01-05 10:24:53,506 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.5 sec
MapReduce Total cumulative CPU time: 4 seconds 500 msec
Ended Job = job_1672890466700_0159
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0160, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0160/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0160
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 10:25:02,266 Stage-5 map = 0%,  reduce = 0%
2023-01-05 10:25:08,411 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.89 sec
2023-01-05 10:25:12,483 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.93 sec
MapReduce Total cumulative CPU time: 2 seconds 930 msec
Ended Job = job_1672890466700_0160
MapReduce Jobs Launched:
Stage-Stage-3: Map: 3  Reduce: 3   Cumulative CPU: 355.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.5 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.93 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 6 minutes 2 seconds 840 msec
OK
AAAAAAAAGNGDBAAA	Slowly important boats shall think too instead of the roads. Easy, chemical affairs design silver patients. Rightly good rules may prevent children. Homes allow easily. Whole eggs 	Children                                          	NULL	NULL	13186.69	11.77928142996888649
AAAAAAAAHOOFAAAA	Special memories accept letters. Long, consistent heads vary at least exhibitions. Missiles shall cover places. Beaches fight. Good, extra women change premises. E	Children                                          	NULL	NULL	1603.65	1.43249326898331612
AAAAAAAAIBEABAAA	Parliamentary societies used to allow from the lessons; black, other councils used to relax then high, only systems. Classes can stick no doubt all giant arrangements. Well-known, cleaChildren                                          	NULL	1.58	15109.69	13.49704063943162269
AAAAAAAAJGJBAAAA	Urban, powerful concepts should discern efficiently others. Far other activities should not see politically politica	Children                                          	NULL	4.42	1753.94	1.56674289539525300
AAAAAAAAAADGBAAA	Almost natural books draw central, favorite problems. Special, final keys would run only new banks. Issues will not shake. Other cells shall think carefully in a 	Children                                          	infants                                           	67.57	3683.53	0.03965040432275726
AAAAAAAAAAFHBAAA	Extremely large deaths love. Patients might get immediately also	Children                                          	infants                                           	0.73	21549.76	0.23196680821342066
AAAAAAAAAAFPAAAA	Houses would learn as well then other men. 	Children                                          	infants                                           	4.37	352.24	0.00379159621847739
AAAAAAAAAAOPAAAA	Scottish months woul	Children                                          	infants                                           	2.11	3373.45	0.03631262850108604
AAAAAAAAAAPDBAAA	Rights must go indeed. Hands reduce controls. So difficult words show men. Large, new friends ensure big characters. Alleged problems sound. Real, british attacks red	Children                                          	infants                                           	0.61	4626.73	0.04980323634997697
AAAAAAAAABBIAAAA	Specific men cause again primary station	Children                                          	infants                                           	4.94	4061.68	0.04372090202323768
AAAAAAAAABCFAAAA	Even 	Children                                          	infants                                           	0.88	9269.45	0.09977859291212023
AAAAAAAAABDJAAAA	Physica	Children                                          	infants                                           	11.65	2832.44	0.03048906652584628
AAAAAAAAABHBBAAA	Texts may pay now at a women. Even dual purpos	Children                                          	infants                                           	7.81	7383.76	0.07948057146872758
AAAAAAAAABHHBAAA	Richly local schools may think very new frames. Of course likely students d	Children                                          	infants                                           	4.32	10213.09	0.10993616120534077
AAAAAAAAABNCAAAA	More southern schools get never profits. Parents must not contribute responsible, increased governors; new mi	Children                                          	infants                                           	2.77	277.80	0.00299030612506535
AAAAAAAAACCAAAAA	Homes run political, real interests. Historic, f	Children                                          	infants                                           	2.20	3918.40	0.04217860158551499
AAAAAAAAACFABAAA	French, russian securiti	Children                                          	infants                                           	4.85	294.00	0.00316468682782294
AAAAAAAAACGPAAAA	Tears continue strong, important teachers. Years should use carefully teachers. Horizontal faces would take sincerely hands. Ready, imaginative attempts highlight no	Children                                          	infants                                           	2.48	3920.40	0.04220013006733691
AAAAAAAAACHHAAAA	So common com	Children                                          	infants                                           	8.95	4313.46	0.04643112259979979
AAAAAAAAACJGAAAA	Fond, national services make then hardly british children. Now premier child	Children                                          	infants                                           	0.97	701.00	0.00754573287858463
AAAAAAAAACMIBAAA	Lovely blocks shall not decide. Grey, large women start however double investigations. Ve	Children                                          	infants                                           	1.57	4807.60	0.05175016460354274
AAAAAAAAACNDBAAA	Enough english creatures allow amon	Children                                          	infants                                           	4.96	13779.08	0.14832133665142351
AAAAAAAAACNFAAAA	Currently local cases can make by a drawings. Responsible sales lead again important, blue courses. Conventional drivers follow further arrangem	Children                                          	infants                                           	5.94	2258.76	0.02431383680004539
AAAAAAAAACONAAAA	Limitations cause; t	Children                                          	infants                                           	4.51	13440.78	0.14467979395124494
AAAAAAAAACPLAAAA	Complete, bright professionals succeed further. Commercial, super horses conceal only international causes. Other, common faces dare ideas. Available, peaceful boys could not like especi	Children                                          	infants                                           	1.42	6759.49	0.07276077879524110
AAAAAAAAADCCAAAA	Old-fashioned, traditional proportions used to operate very excellent, previous preparations. Now advisory blacks should not translate perfect walls. Directors cannot advise subsequently depar	Children                                          	infants                                           	4.19	1565.42	0.01685055800683873
AAAAAAAAADHDBAAA	Patterns see so apparently local methods. Secondary m	Children                                          	infants                                           	4.85	3239.04	0.03486580688024359
AAAAAAAAADKAAAAA	Original minutes sell studies. Very certain rocks	Children                                          	infants                                           	3.51	634.48	0.00682969557318741
AAAAAAAAADKCBAAA	Upper, peaceful areas take too then only events; animals ensure special, difficult things. Substantial art	Children                                          	infants                                           	7.64	15512.82	0.16698373168839543
AAAAAAAAADLPAAAA	Measures may not watch in a games. As soviet duties set here now central sc	Children                                          	infants                                           	3.16	14965.69	0.16109429255878058
AAAAAAAAADMHBAAA	Levels think very after a estates. Senior students ensure considerably. Titles seek major resourc	Children                                          	infants                                           	1.65	2063.24	0.02220921241713402
AAAAAAAAADOCAAAA	Hands know european, absolu	Children                                          	infants                                           	98.85	212.04	0.00228244964276046
AAAAAAAAAEEDAAAA	Close firms grow different activities. Broad men may manage wrong, disabled conclusions. Simply fierce men decide. Australian, smooth lists	Children                                          	infants                                           	5.80	4566.81	0.04915824303459211
AAAAAAAAAEEEBAAA	Much good seats used to like else techniques. Centres ought to work single, oth	Children                                          	infants                                           	7.89	5424.65	0.05839223945765207
AAAAAAAAAEGPAAAA	Forms come american groups	Children                                          	infants                                           	3.32	9620.69	0.10355942488968666
AAAAAAAAAEHFBAAA	Systems market with a properties. Men lie. Other instances feel so right necessary families. Capable facilities ought to beat very ways; even professional limit	Children                                          	infants                                           	3.96	525.97	0.00566166779193888
AAAAAAAAAEIEAAAA	Inadequate matters ought to fend. Royal pare	Children                                          	infants                                           	0.11	24260.24	0.26114306791776597
AAAAAAAAAEMCBAAA	Similar followers should not fly sufficiently i	Children                                          	infants                                           	1.79	3345.31	0.03600972276185156
AAAAAAAAAENJAAAA	International times set joint, adjacent differences. Men may become rather young fingers. Invisible computers would not struggle again close, industrial activities. In particular other lines cou	Children                                          	infants                                           	4.41	8466.92	0.09113996665384559
AAAAAAAAAFCAAAAA	Mental, nice projects go enough for the bonds. Good lessons happen free paym	Children                                          	infants                                           	2.91	12743.94	0.13717884031484991
AAAAAAAAAFDCBAAA	Male matters find instead ideas. So other activities use by the arguments. Enough regular t	Children                                          	infants                                           	2.21	2976.56	0.03204040892593418
AAAAAAAAAFDGAAAA	Very new things demand rather new things. Heels become still jobs. Best special sales face to the moth	Children                                          	infants                                           	2.72	12339.34	0.13282362844227453
AAAAAAAAAFIDAAAA	Other, rational years hit suddenly young members. Political goals	Children                                          	infants                                           	0.36	22137.13	0.23828940039729263
AAAAAAAAAGADBAAA	Triumphant	Children                                          	infants                                           	4.49	5279.17	0.05682625768992527
AAAAAAAAAGBMAAAA	Right, used doctors catch clearly to a hands; international voters used to need rather. Valid, critical children leave on a jobs. Therefore classical techniques cal	Children                                          	infants                                           	3.99	32181.89	0.34641361693009111
AAAAAAAAAGEKAAAA	More rigid years used to strike. Parents deny too previous, other pp.; british, go	Children                                          	infants                                           	3.44	4428.23	0.04766653452915094
AAAAAAAAAGJEAAAA	Main, minute members require so. Teams could not cover initially on a subjects. Studies keep meanwhile like the limitations. Memories may not consult about awful levels. Conflicts could impose neither	Children                                          	infants                                           	20.88	7618.45	0.08200683116812134
AAAAAAAAAGLFBAAA	Consequences should place. Domestic, remarkable functions run sales. Other, prime women ought to start etc efficient, financial proposals. Specific st	Children                                          	infants                                           	5.36	5051.67	0.05437739288268133
AAAAAAAAAGLGAAAA	Always effective fr	Children                                          	infants                                           	7.63	20297.33	0.21848534996930404
AAAAAAAAAGLHBAAA	Very poor games get further about a others. Minor manufacturers send specially. Budgets work almost eff	Children                                          	infants                                           	1.63	20051.04	0.21583422507534312
AAAAAAAAAHAABAAA	Required, perfect opponents would not get into a times. So new implications release. Parties send at all 	Children                                          	infants                                           	79.35	12489.84	0.13444364669937437
AAAAAAAAAHAJAAAA	Occupational pains cannot give most great groups. Again hard processes might find of course in a levels. There economic parts manipulate more in common academic elements. Small speakers shall not tak	Children                                          	infants                                           	2.27	13.94	0.00015005351829882
AAAAAAAAAHDDBAAA	Tight examples will give important, odd banks. Companies make openly almost federal crops. Subjects like however official, scottish patterns; voices penetrate i	Children                                          	infants                                           	9.51	9160.62	0.09860712057378020
AAAAAAAAAHDIBAAA	Then useless systems may not read slowly officers. Men see views. Small solicitors must not enable well bare communications. Good, in	Children                                          	infants                                           	4.63	36.72	0.00039526292625054
AAAAAAAAAHGIBAAA	Yet political affairs prepare now with the animals; possible, very consid	Children                                          	infants                                           	4.34	2572.80	0.02769423901572401
AAAAAAAAAHOFAAAA	Available, open sections assist. Details help hurriedly black books. Incidentally safe circumstances ought to relax particularly	Children                                          	infants                                           	3.18	27.26	0.00029343320723283
AAAAAAAAAIBGBAAA	Even necessary contracts see rules. Only, certain words select big societies. Available, electrical friends wipe national, mas	Children                                          	infants                                           	2.14	11274.31	0.12135938894487227
AAAAAAAAAIFCBAAA	Efficiently industrial constraints ma	Children                                          	infants                                           	32.77	4416.12	0.04753617957171919
AAAAAAAAAIIBAAAA	Necessary, unique programmes announce very. Proper arms used to mean under a generations. Political sorts could not cancel to a 	Children                                          	infants                                           	1.98	14099.36	0.15176890773038654
AAAAAAAAAIIDBAAA	General workers may make	Children                                          	infants                                           	9.49	8959.86	0.09644609156849539
AAAAAAAAAIJIBAAA	Letters shall not state generally services. Vital years introduce. Much vast rats involve proposals	Children                                          	infants                                           	6.92	887.12	0.00954917339693295
AAAAAAAAAIMIBAAA	C	Children                                          	infants                                           	9.47	859.05	0.00924702115456223
AAAAAAAAAIPHBAAA	Different, important children could not fill resources. Too simple names might not support with a resources; other	Children                                          	infants                                           	4.93	4577.24	0.04927051406729344
AAAAAAAAAJDHAAAA	Dangerous, outer hours drag. Crazy obligations would not record most parents. Tonight corresponding parts must appreciate enough normal, average members. Angry, considerable earnings sui	Children                                          	infants                                           	6.40	5552.88	0.05977253806966478
AAAAAAAAAJFHBAAA	Recent, certain differences ensure remarkable enemies. Marine, various officers assist probably doctors. Weekends may tell ago almost human relationships. Slow, wrong resources 	Children                                          	infants                                           	1.57	4448.10	0.04788041999605176
AAAAAAAAAJILAAAA	Years ought to mean less spiritual, other minutes. Fundamental, military pieces can take really jewish owners; difficulties see silently unusual, true causes. Cells cannot go adequate	Children                                          	infants                                           	20.50	34796.13	0.37455392608916540
AAAAAAAAAJLIBAAA	Basic, small children shall consider then major pati	Children                                          	infants                                           	9.15	2252.15	0.02424268516762392
AAAAAAAAAKABAAAA	Stages would not see then of course large lands; p	Children                                          	infants                                           	1.73	9812.07	0.10561948531522664
AAAAAAAAAKADAAAA	Married, new customers ensure vehicles. Maximum sons lift nuclear, structural sources. Crucial, poor r	Children                                          	infants                                           	25.59	699.60	0.00753066294130928
AAAAAAAAAKCBBAAA	Internal, intense clothes make excitedly; customs become politically glasses. Useful t	Children                                          	infants                                           	11.13	5113.99	0.05504822037625250
AAAAAAAAAKEFAAAA	Common, exciting eyes conclude white arts. Simply private sides find european, final approaches. Accounts could not sound regardless including the paths. Earlier main weaknesses see e	Children                                          	infants                                           	3.15	12402.54	0.13350392846784736
AAAAAAAAAKELAAAA	Moments relieve popula	Children                                          	infants                                           	4.32	14383.48	0.15482724385800917
AAAAAAAAAKGDAAAA	Slowly academic teachers would think only, legal molecules. Long 	Children                                          	infants                                           	0.71	52.92	0.00056964362900813
AAAAAAAAAKIGBAAA	Contemporary, national functions can attend areas. General, central ways might ask e	Children                                          	infants                                           	4.53	3287.39	0.03538625792828862
AAAAAAAAAKJMAAAA	Northern, rigid letters could intr	Children                                          	infants                                           	2.76	5105.16	0.05495317212900870
AAAAAAAAAKKOAAAA	Useful, various vegetables attend thus hitherto small factors. Difficult men read; black soldiers signify much difficult prices. Normal, similar alternatives can live so	Children                                          	infants                                           	2.02	963.12	0.01036725570616609
AAAAAAAAAKNAAAAA	Crucial, safe rights used to endure also nuclear days. Able members reply	Children                                          	infants                                           	12.53	12435.51	0.13385882549068179
AAAAAAAAAKNFAAAA	Important children stir adequately for a agents. Years would not reduce in a lives. Possible, other schools could see. Front forms cannot investigate rather fina	Children                                          	infants                                           	0.31	2349.16	0.02528692417839638
AAAAAAAAAKPEAAAA	At first national facts should produce. Very familiar words feel methods; real departments appear police. Large, special years would mix all 	Children                                          	infants                                           	34.64	18699.86	0.20128979804126897
AAAAAAAAALBBBAAA	Hands may not stop very of course easy profits; forward bright employers might take during a achievements. Ideas may not fail to a weapons; current developments examine pro	Children                                          	infants                                           	2.56	14963.98	0.16107588570682284
AAAAAAAAALBEAAAA	Sections work always. Gradually public models can take however brief useful clients; frequent, major hands apply scottish, wide 	Children                                          	infants                                           	1.12	14152.63	0.15234231884371351
AAAAAAAAALCEAAAA	Different, soft farms meet on the implications. Even dark stations pay all rarely nice facts; daughters may 	Children                                          	infants                                           	4.50	5584.15	0.06010913588295057
AAAAAAAAALDGAAAA	Middle-class eyes must remember. Perhaps digital explanations should take just sharply dead ministers; long, new pensioners might take then original parties. Now old	Children                                          	infants                                           	9.29	4458.31	0.04799032289575269
AAAAAAAAAMAFAAAA	Wide companies wo	Children                                          	infants                                           	9.47	11736.78	0.12633752743896504
AAAAAAAAAMAHAAAA	Simple aspects might gain on a students; then empty bodies step also far miserable actions. Enormous cases must feel individuals. Labou	Children                                          	infants                                           	2.47	1232.64	0.01326843391648867
AAAAAAAAAMKAAAAA	Numbers could muster 	Children                                          	infants                                           	3.03	488.80	0.00526156095727841
AAAAAAAAAMLGAAAA	Constant, urban pool	Children                                          	infants                                           	4.77	3198.00	0.03442404243325769
AAAAAAAAANAABAAA	Grounds might provide theoretically years. Complete	Children                                          	infants                                           	1.73	3895.98	0.04193726730429121
AAAAAAAAANKDAAAA	Recent statements must spare payments. Drivers allow to the services. Damp 	Children                                          	infants                                           	3.08	2418.00	0.02602793452270703
AAAAAAAAANLDBAAA	Other clergy build miners; teachers shall not come now in a 	Children                                          	infants                                           	3.32	20579.09	0.22151828248837680
AAAAAAAAANMKAAAA	New	Children                                          	infants                                           	1.24	5280.55	0.05684111234238240
AAAAAAAAAOADAAAA	Before difficult years ought to override parents. English, true women must provoke more severe times. Only, short cases may not spend usually by a objects	Children                                          	infants                                           	1.76	27262.27	0.29345764205970236
AAAAAAAAAOAFAAAA	Clients ought to strike; little, different teeth inject easy certain solutions. Sick 	Children                                          	infants                                           	3.81	1984.17	0.02135808388830423
AAAAAAAAAOAMAAAA	Significant, bold studies suffice clear goods; about political books must not produce hardly great red months. Active, 	Children                                          	infants                                           	5.76	5396.57	0.05808997957287225
AAAAAAAAAOBDBAAA	Years assume. Others see again; real, urgent services exercise almost in a details. Months can call never responsible doors. Al	Children                                          	infants                                           	35.91	2279.67	0.02453891707749361
AAAAAAAAAODBBAAA	English, corpor	Children                                          	infants                                           	0.23	12621.39	0.13585968259121147
AAAAAAAAAOFDAAAA	Today heavy owners continue from a interests. Scottish benefits could say everyday standards. Ashamed days begin for sure indian reasons. Good st	Children                                          	infants                                           	0.37	5101.05	0.05490893109886465
AAAAAAAAAOFDBAAA	Poor symbols interpret; parents ought to say about. Sure shoes expect nowhere coastal kids. Regions help as well models. Relative	Children                                          	infants                                           	2.84	19475.93	0.20964360248503955
AAAAAAAAAOHBBAAA	Important funds can urge very british, practical leaders; likely facilities go best conservative points. Alternative workers will lose less a	Children                                          	infants                                           	3.32	1117.85	0.01203280670231930
AAAAAAAAAOICBAAA	Appropriate teachers rear comfortable boys. Authors choose stiffly guilty, young cases. Now criminal facil	Children                                          	infants                                           	87.31	8361.78	0.09000821436446701
Time taken: 293.673 seconds, Fetched: 100 row(s)
timediff:299.150402682
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query21.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = f815a594-8bce-4ef1-9159-98864ac83504

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 42d4266a-7bdd-484a-93a7-dece93c44be4
No Stats for tpcds_bin_partitioned_orc_10@inventory, Columns: inv_date_sk, inv_quantity_on_hand, inv_warehouse_sk, inv_item_sk
No Stats for tpcds_bin_partitioned_orc_10@warehouse, Columns: w_warehouse_name, w_warehouse_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_item_id, i_current_price, i_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_date, d_date_sk
Query ID = hdfs_20230105102521_a75d0860-dcc9-4b58-9938-cceaf361995a
Total jobs = 2
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

2023-01-05 10:25:32	Dump the side-table for tag: 1 with group count: 61 into file: file:/tmp/hdfs/f815a594-8bce-4ef1-9159-98864ac83504/hive_2023-01-05_10-25-21_380_5902046218710559683-1/-local-10008/HashTable-Stage-4/MapJoin-mapfile11--.hashtable
2023-01-05 10:25:32	Uploaded 1 File to: file:/tmp/hdfs/f815a594-8bce-4ef1-9159-98864ac83504/hive_2023-01-05_10-25-21_380_5902046218710559683-1/-local-10008/HashTable-Stage-4/MapJoin-mapfile11--.hashtable (1793 bytes)
2023-01-05 10:25:32	End of local task; Time Taken: 2.331 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0161, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0161/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0161
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 10:25:44,100 Stage-4 map = 0%,  reduce = 0%
2023-01-05 10:26:01,378 Stage-4 map = 33%,  reduce = 0%, Cumulative CPU 11.21 sec
2023-01-05 10:26:07,475 Stage-4 map = 53%,  reduce = 0%, Cumulative CPU 15.25 sec
2023-01-05 10:26:09,506 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 17.27 sec
2023-01-05 10:26:14,591 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 22.24 sec
MapReduce Total cumulative CPU time: 22 seconds 240 msec
Ended Job = job_1672890466700_0161
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0162, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0162/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0162
Hadoop job information for Stage-5: number of mappers: 1; number of reducers: 1
2023-01-05 10:26:24,606 Stage-5 map = 0%,  reduce = 0%
2023-01-05 10:26:28,747 Stage-5 map = 100%,  reduce = 0%, Cumulative CPU 1.67 sec
2023-01-05 10:26:32,830 Stage-5 map = 100%,  reduce = 100%, Cumulative CPU 2.81 sec
MapReduce Total cumulative CPU time: 2 seconds 810 msec
Ended Job = job_1672890466700_0162
MapReduce Jobs Launched:
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 22.24 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-5: Map: 1  Reduce: 1   Cumulative CPU: 2.81 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 25 seconds 50 msec
OK
NULL	AAAAAAAAAAANAAAA	2161	1769
NULL	AAAAAAAAAABFBAAA	2158	2112
NULL	AAAAAAAAAACDAAAA	2250	1529
NULL	AAAAAAAAAAEFBAAA	2118	1420
NULL	AAAAAAAAAAFLAAAA	1145	1164
NULL	AAAAAAAAAAFPAAAA	1421	1689
NULL	AAAAAAAAAAHMAAAA	2518	2454
NULL	AAAAAAAAAALAAAAA	1502	1257
NULL	AAAAAAAAAALCBAAA	2302	2266
NULL	AAAAAAAAAALMAAAA	1553	2109
NULL	AAAAAAAAAANCBAAA	2291	1848
NULL	AAAAAAAAAANDAAAA	3222	2198
NULL	AAAAAAAAAANKAAAA	2945	2614
NULL	AAAAAAAAAAOCAAAA	1765	1586
NULL	AAAAAAAAAAPBBAAA	2761	2321
NULL	AAAAAAAAABCFAAAA	1593	2083
NULL	AAAAAAAAABDHAAAA	1438	2009
NULL	AAAAAAAAABECBAAA	2028	1624
NULL	AAAAAAAAABEFAAAA	1532	2152
NULL	AAAAAAAAABEOAAAA	1578	2094
NULL	AAAAAAAAABFABAAA	1494	1078
NULL	AAAAAAAAABGNAAAA	1849	1910
NULL	AAAAAAAAABHFBAAA	2410	2249
NULL	AAAAAAAAABLHBAAA	1841	2041
NULL	AAAAAAAAABLNAAAA	1889	2466
NULL	AAAAAAAAABPCBAAA	2597	2706
NULL	AAAAAAAAACDIBAAA	2701	1898
NULL	AAAAAAAAACEHBAAA	3368	2695
NULL	AAAAAAAAACGFBAAA	3104	2122
NULL	AAAAAAAAACGPAAAA	2868	2095
NULL	AAAAAAAAACHABAAA	2483	2589
NULL	AAAAAAAAACHCAAAA	1361	1993
NULL	AAAAAAAAACICBAAA	1973	2715
NULL	AAAAAAAAACIDBAAA	1941	1459
NULL	AAAAAAAAACJIAAAA	2973	2309
NULL	AAAAAAAAACKCAAAA	1738	2076
NULL	AAAAAAAAACLGAAAA	2432	2665
NULL	AAAAAAAAACLNAAAA	1606	2231
NULL	AAAAAAAAACNDBAAA	2336	1878
NULL	AAAAAAAAACPGAAAA	1332	1553
NULL	AAAAAAAAADAFAAAA	2147	1696
NULL	AAAAAAAAADCBBAAA	1891	2502
NULL	AAAAAAAAADCMAAAA	2412	1612
NULL	AAAAAAAAADIJAAAA	1943	2493
NULL	AAAAAAAAADIOAAAA	2625	1837
NULL	AAAAAAAAADKBAAAA	2762	3030
NULL	AAAAAAAAADLBBAAA	2636	2107
NULL	AAAAAAAAADMOAAAA	2504	1999
NULL	AAAAAAAAADPOAAAA	2425	2218
NULL	AAAAAAAAAEAGAAAA	1500	1613
NULL	AAAAAAAAAEBFAAAA	2728	2716
NULL	AAAAAAAAAEHOAAAA	2882	3238
NULL	AAAAAAAAAEJGBAAA	3153	2520
NULL	AAAAAAAAAEMAAAAA	1861	1273
NULL	AAAAAAAAAEOHBAAA	3090	2505
NULL	AAAAAAAAAEPGBAAA	3157	2643
NULL	AAAAAAAAAFAEBAAA	2822	3146
NULL	AAAAAAAAAFGEBAAA	2253	1594
NULL	AAAAAAAAAFIDBAAA	1724	2493
NULL	AAAAAAAAAFKEAAAA	2187	1849
NULL	AAAAAAAAAFKGBAAA	1824	2364
NULL	AAAAAAAAAFKHAAAA	1866	2696
NULL	AAAAAAAAAFLFBAAA	2121	2079
NULL	AAAAAAAAAFPCAAAA	2699	2189
NULL	AAAAAAAAAFPHBAAA	1648	1151
NULL	AAAAAAAAAFPJAAAA	2413	2186
NULL	AAAAAAAAAGECBAAA	2333	2080
NULL	AAAAAAAAAGLFAAAA	2444	1903
NULL	AAAAAAAAAGNDBAAA	2029	2054
NULL	AAAAAAAAAGNMAAAA	2524	2166
NULL	AAAAAAAAAGOCBAAA	2212	1519
NULL	AAAAAAAAAHCNAAAA	2614	2713
NULL	AAAAAAAAAHDEAAAA	3044	2221
NULL	AAAAAAAAAHFHBAAA	3047	2741
NULL	AAAAAAAAAHGHAAAA	1437	1526
NULL	AAAAAAAAAHHAAAAA	2138	1814
NULL	AAAAAAAAAHHBBAAA	1472	1701
NULL	AAAAAAAAAHIGBAAA	1915	1912
NULL	AAAAAAAAAHJBAAAA	3106	2147
NULL	AAAAAAAAAHNFBAAA	2384	1812
NULL	AAAAAAAAAHPGBAAA	2211	1557
NULL	AAAAAAAAAIAEBAAA	2644	3429
NULL	AAAAAAAAAIDHBAAA	2544	2259
NULL	AAAAAAAAAIECAAAA	1889	1617
NULL	AAAAAAAAAIENAAAA	1993	2670
NULL	AAAAAAAAAIHABAAA	2437	1946
NULL	AAAAAAAAAIHLAAAA	1997	2467
NULL	AAAAAAAAAIIGBAAA	1815	1549
NULL	AAAAAAAAAILJAAAA	2322	2279
NULL	AAAAAAAAAIOEAAAA	2044	1570
NULL	AAAAAAAAAJAABAAA	2420	2273
NULL	AAAAAAAAAJAIAAAA	2496	2847
NULL	AAAAAAAAAJCAAAAA	2444	1699
NULL	AAAAAAAAAJEDAAAA	2078	1634
NULL	AAAAAAAAAJFIAAAA	1763	2421
NULL	AAAAAAAAAJIEBAAA	2063	2117
NULL	AAAAAAAAAJILAAAA	1650	1574
NULL	AAAAAAAAAJJEBAAA	2668	2274
NULL	AAAAAAAAAJKBAAAA	2634	2142
NULL	AAAAAAAAAJKPAAAA	1664	2336
Time taken: 73.472 seconds, Fetched: 100 row(s)
timediff:78.735755788
----------------------------------------------------------------------------------------------------------------------------------------
queryfile:sample-queries-tpcds/query22.sql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = d8015320-5604-428d-af45-6f461139ec49

Logging initialized using configuration in jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = e1546993-dd94-40f1-881c-0726fe71f5d0
No Stats for tpcds_bin_partitioned_orc_10@inventory, Columns: inv_date_sk, inv_quantity_on_hand, inv_item_sk
No Stats for tpcds_bin_partitioned_orc_10@date_dim, Columns: d_month_seq, d_date_sk
No Stats for tpcds_bin_partitioned_orc_10@item, Columns: i_class, i_category, i_brand, i_product_name, i_item_sk
Query ID = hdfs_20230105102640_01f6dacc-8c2b-4caa-b65f-92e9a882188f
Total jobs = 2
SLF4J: Found binding in [jar:file:/app/hdfs/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/app/hdfs/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2023-01-05 10:26:48	Starting to launch local task to process map join;	maximum memory = 2390753282023-01-05 10:26:51	Dump the side-table for tag: 1 with group count: 102000 into file: file:/tmp/hdfs/d8015320-5604-428d-af45-6f461139ec49/hive_2023-01-05_10-26-40_190_9098958181864368955-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile01--.hashtable2023-01-05 10:26:51	Uploaded 1 File to: file:/tmp/hdfs/d8015320-5604-428d-af45-6f461139ec49/hive_2023-01-05_10-26-40_190_9098958181864368955-1/-local-10007/HashTable-Stage-3/MapJoin-mapfile01--.hashtable (23049534 bytes)

Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0163, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0163/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0163
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1
2023-01-05 10:27:02,924 Stage-3 map = 0%,  reduce = 0%
2023-01-05 10:28:02,953 Stage-3 map = 0%,  reduce = 0%, Cumulative CPU 71.42 sec
2023-01-05 10:28:12,092 Stage-3 map = 7%,  reduce = 0%, Cumulative CPU 83.96 sec
2023-01-05 10:28:59,808 Stage-3 map = 13%,  reduce = 0%, Cumulative CPU 144.8 sec
2023-01-05 10:29:48,460 Stage-3 map = 20%,  reduce = 0%, Cumulative CPU 206.7 sec
2023-01-05 10:30:49,265 Stage-3 map = 20%,  reduce = 0%, Cumulative CPU 287.16 sec
2023-01-05 10:30:54,327 Stage-3 map = 27%,  reduce = 0%, Cumulative CPU 292.6 sec
2023-01-05 10:31:41,932 Stage-3 map = 33%,  reduce = 0%, Cumulative CPU 352.17 sec
2023-01-05 10:32:30,545 Stage-3 map = 40%,  reduce = 0%, Cumulative CPU 412.84 sec
2023-01-05 10:33:24,219 Stage-3 map = 47%,  reduce = 0%, Cumulative CPU 489.47 sec
2023-01-05 10:34:12,851 Stage-3 map = 53%,  reduce = 0%, Cumulative CPU 549.28 sec
2023-01-05 10:35:06,543 Stage-3 map = 60%,  reduce = 0%, Cumulative CPU 615.46 sec
2023-01-05 10:35:54,144 Stage-3 map = 67%,  reduce = 0%, Cumulative CPU 675.79 sec
2023-01-05 10:36:06,277 Stage-3 map = 69%,  reduce = 0%, Cumulative CPU 688.03 sec
2023-01-05 10:36:12,344 Stage-3 map = 73%,  reduce = 0%, Cumulative CPU 694.07 sec
2023-01-05 10:36:18,406 Stage-3 map = 76%,  reduce = 0%, Cumulative CPU 700.28 sec
2023-01-05 10:36:24,468 Stage-3 map = 78%,  reduce = 0%, Cumulative CPU 706.31 sec
2023-01-05 10:36:30,532 Stage-3 map = 81%,  reduce = 0%, Cumulative CPU 712.35 sec
2023-01-05 10:36:36,596 Stage-3 map = 84%,  reduce = 0%, Cumulative CPU 718.32 sec
2023-01-05 10:36:42,659 Stage-3 map = 87%,  reduce = 0%, Cumulative CPU 724.29 sec
2023-01-05 10:36:48,727 Stage-3 map = 90%,  reduce = 0%, Cumulative CPU 729.13 sec
2023-01-05 10:36:54,790 Stage-3 map = 93%,  reduce = 0%, Cumulative CPU 735.12 sec
2023-01-05 10:37:00,854 Stage-3 map = 96%,  reduce = 0%, Cumulative CPU 741.15 sec
2023-01-05 10:37:06,920 Stage-3 map = 99%,  reduce = 0%, Cumulative CPU 747.17 sec
2023-01-05 10:37:08,939 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 749.88 sec
2023-01-05 10:37:34,211 Stage-3 map = 100%,  reduce = 33%, Cumulative CPU 755.3 sec
2023-01-05 10:37:40,275 Stage-3 map = 100%,  reduce = 67%, Cumulative CPU 756.03 sec
2023-01-05 10:37:52,423 Stage-3 map = 100%,  reduce = 68%, Cumulative CPU 767.36 sec
2023-01-05 10:37:58,499 Stage-3 map = 100%,  reduce = 69%, Cumulative CPU 773.54 sec
2023-01-05 10:38:04,571 Stage-3 map = 100%,  reduce = 70%, Cumulative CPU 779.74 sec
2023-01-05 10:38:10,654 Stage-3 map = 100%,  reduce = 71%, Cumulative CPU 785.92 sec
2023-01-05 10:38:16,725 Stage-3 map = 100%,  reduce = 72%, Cumulative CPU 792.02 sec
2023-01-05 10:38:22,797 Stage-3 map = 100%,  reduce = 73%, Cumulative CPU 798.35 sec
2023-01-05 10:38:28,885 Stage-3 map = 100%,  reduce = 74%, Cumulative CPU 804.51 sec
2023-01-05 10:38:41,048 Stage-3 map = 100%,  reduce = 75%, Cumulative CPU 816.76 sec
2023-01-05 10:38:47,125 Stage-3 map = 100%,  reduce = 76%, Cumulative CPU 822.97 sec
2023-01-05 10:38:58,275 Stage-3 map = 100%,  reduce = 77%, Cumulative CPU 835.28 sec
2023-01-05 10:39:04,361 Stage-3 map = 100%,  reduce = 78%, Cumulative CPU 841.47 sec
2023-01-05 10:39:16,518 Stage-3 map = 100%,  reduce = 79%, Cumulative CPU 853.83 sec
2023-01-05 10:39:22,594 Stage-3 map = 100%,  reduce = 80%, Cumulative CPU 860.03 sec
2023-01-05 10:39:34,746 Stage-3 map = 100%,  reduce = 81%, Cumulative CPU 872.32 sec
2023-01-05 10:39:40,822 Stage-3 map = 100%,  reduce = 82%, Cumulative CPU 878.5 sec
2023-01-05 10:39:46,898 Stage-3 map = 100%,  reduce = 83%, Cumulative CPU 884.66 sec
2023-01-05 10:39:59,042 Stage-3 map = 100%,  reduce = 84%, Cumulative CPU 897.0 sec
2023-01-05 10:40:05,119 Stage-3 map = 100%,  reduce = 85%, Cumulative CPU 903.19 sec
2023-01-05 10:40:11,201 Stage-3 map = 100%,  reduce = 86%, Cumulative CPU 909.4 sec
2023-01-05 10:40:22,338 Stage-3 map = 100%,  reduce = 87%, Cumulative CPU 921.8 sec
2023-01-05 10:40:28,412 Stage-3 map = 100%,  reduce = 88%, Cumulative CPU 927.96 sec
2023-01-05 10:40:40,562 Stage-3 map = 100%,  reduce = 89%, Cumulative CPU 940.39 sec
2023-01-05 10:40:46,637 Stage-3 map = 100%,  reduce = 90%, Cumulative CPU 946.55 sec
2023-01-05 10:40:58,801 Stage-3 map = 100%,  reduce = 91%, Cumulative CPU 958.89 sec
2023-01-05 10:41:04,876 Stage-3 map = 100%,  reduce = 92%, Cumulative CPU 965.05 sec
2023-01-05 10:41:10,950 Stage-3 map = 100%,  reduce = 93%, Cumulative CPU 971.16 sec
2023-01-05 10:41:23,096 Stage-3 map = 100%,  reduce = 94%, Cumulative CPU 983.61 sec
2023-01-05 10:41:29,171 Stage-3 map = 100%,  reduce = 95%, Cumulative CPU 989.77 sec
2023-01-05 10:41:40,307 Stage-3 map = 100%,  reduce = 96%, Cumulative CPU 1002.07 sec
2023-01-05 10:41:46,383 Stage-3 map = 100%,  reduce = 97%, Cumulative CPU 1008.18 sec
2023-01-05 10:41:58,528 Stage-3 map = 100%,  reduce = 98%, Cumulative CPU 1020.51 sec
2023-01-05 10:42:04,600 Stage-3 map = 100%,  reduce = 99%, Cumulative CPU 1026.65 sec
2023-01-05 10:42:15,763 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 1037.77 sec
MapReduce Total cumulative CPU time: 17 minutes 17 seconds 770 msec
Ended Job = job_1672890466700_0163
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1672890466700_0164, Tracking URL = http://my-hadoop-yarn-rm-0.my-hadoop-yarn-rm.hadoop.svc.cluster.local:8088/proxy/application_1672890466700_0164/
Kill Command = /app/hdfs/hadoop/bin/mapred job  -kill job_1672890466700_0164
Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1
2023-01-05 10:42:23,499 Stage-4 map = 0%,  reduce = 0%
2023-01-05 10:42:29,605 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 3.46 sec
2023-01-05 10:42:34,678 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 4.66 sec
MapReduce Total cumulative CPU time: 4 seconds 660 msec
Ended Job = job_1672890466700_0164
MapReduce Jobs Launched:
Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 1037.77 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Stage-Stage-4: Map: 1  Reduce: 1   Cumulative CPU: 4.66 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 17 minutes 22 seconds 430 msec
OK
bareseeseableation                                	NULL	NULL	NULL	440.6734693877551
bareseeseableation                                	amalgamalg #1                                     	NULL	NULL	440.6734693877551
bareseeseableation                                	amalgamalg #1                                     	dresses                                           	NULL	440.6734693877551
bareseeseableation                                	amalgamalg #1                                     	dresses                                           	Women                                             	440.6734693877551
barpripriantipri                                  	NULL	NULL	NULL	447.97
barpripriantipri                                  	brandunivamalg #5                                 	NULL	NULL	447.97
barpripriantipri                                  	brandunivamalg #5                                 	personal                                          	NULL	447.97
barpripriantipri                                  	brandunivamalg #5                                 	personal                                          	Electronics                                       	447.97
n stbarn stought                                  	NULL	NULL	NULL	451.8808080808081
n stbarn stought                                  	importoedu pack #2                                	NULL	NULL	451.8808080808081
n stbarn stought                                  	importoedu pack #2                                	mens                                              	NULL	451.8808080808081
n stbarn stought                                  	importoedu pack #2                                	mens                                              	Shoes                                             	451.8808080808081
barationbarationn st                              	NULL	NULL	NULL	452.3805668016194
barationbarationn st                              	scholarbrand #7                                   	NULL	NULL	452.3805668016194
barationbarationn st                              	scholarbrand #7                                   	custom                                            	NULL	452.3805668016194
barationbarationn st                              	scholarbrand #7                                   	custom                                            	Jewelry                                           	452.3805668016194
priationeingationpri                              	NULL	NULL	NULL	452.51485148514854
priationeingationpri                              	edu packscholar #2                                	NULL	NULL	452.51485148514854
priationeingationpri                              	edu packscholar #2                                	classical                                         	NULL	452.51485148514854
priationeingationpri                              	edu packscholar #2                                	classical                                         	Music                                             	452.51485148514854
ableoughtablecallyanti                            	NULL	NULL	NULL	452.93951612903226
ableoughtablecallyanti                            	namelessnameless #3                               	NULL	NULL	452.93951612903226
ableoughtablecallyanti                            	namelessnameless #3                               	outdoor                                           	NULL	452.93951612903226
ableoughtablecallyanti                            	namelessnameless #3                               	outdoor                                           	Sports                                            	452.93951612903226
callyeingeingeseable                              	NULL	NULL	NULL	453.8971193415638
callyeingeingeseable                              	edu packamalg #1                                  	NULL	NULL	453.8971193415638
callyeingeingeseable                              	edu packamalg #1                                  	swimwear                                          	NULL	453.8971193415638
callyeingeingeseable                              	edu packamalg #1                                  	swimwear                                          	Women                                             	453.8971193415638
eseablebarantiable                                	NULL	NULL	NULL	454.0201207243461
eseablebarantiable                                	scholarnameless #5                                	NULL	NULL	454.0201207243461
eseablebarantiable                                	scholarnameless #5                                	tables                                            	NULL	454.0201207243461
eseablebarantiable                                	scholarnameless #5                                	tables                                            	Home                                              	454.0201207243461
ationn stcallyn stpri                             	NULL	NULL	NULL	454.1788617886179
ationn stcallyn stpri                             	amalgnameless #8                                  	NULL	NULL	454.1788617886179
ationn stcallyn stpri                             	amalgnameless #8                                  	accent                                            	NULL	454.1788617886179
ationn stcallyn stpri                             	amalgnameless #8                                  	accent                                            	Home                                              	454.1788617886179
ableationbaresen st                               	NULL	NULL	NULL	455.5263157894737
ableationbaresen st                               	amalgamalg #1                                     	NULL	NULL	455.5263157894737
ableationbaresen st                               	amalgamalg #1                                     	dresses                                           	NULL	455.5263157894737
ableationbaresen st                               	amalgamalg #1                                     	dresses                                           	Women                                             	455.5263157894737
ableantiableantiable                              	NULL	NULL	NULL	455.64489795918365
ableantiableantiable                              	amalgamalg #1                                     	NULL	NULL	455.64489795918365
ableantiableantiable                              	amalgamalg #1                                     	dresses                                           	NULL	455.64489795918365
ableantiableantiable                              	amalgamalg #1                                     	dresses                                           	Women                                             	455.64489795918365
pricallyn stationn st                             	NULL	NULL	NULL	455.75757575757575
pricallyn stationn st                             	importoedu pack #2                                	NULL	NULL	455.75757575757575
pricallyn stationn st                             	importoedu pack #2                                	mens                                              	NULL	455.75757575757575
pricallyn stationn st                             	importoedu pack #2                                	mens                                              	Shoes                                             	455.75757575757575
barprioughteseeing                                	NULL	NULL	NULL	455.82484725050915
barprioughteseeing                                	exportimaxi #1                                    	NULL	NULL	455.82484725050915
barprioughteseeing                                	exportimaxi #1                                    	sailing                                           	NULL	455.82484725050915
barprioughteseeing                                	exportimaxi #1                                    	sailing                                           	Sports                                            	455.82484725050915
ationeseeseesecally                               	NULL	NULL	NULL	455.966
ationeseeseesecally                               	exportiedu pack #2                                	NULL	NULL	455.966
ationeseeseesecally                               	exportiedu pack #2                                	kids                                              	NULL	455.966
ationeseeseesecally                               	exportiedu pack #2                                	kids                                              	Shoes                                             	455.966
ationn stantiesecally                             	NULL	NULL	NULL	456.0674846625767
ationn stantiesecally                             	namelesscorp #6                                   	NULL	NULL	456.0674846625767
ationn stantiesecally                             	namelesscorp #6                                   	mens watch                                        	NULL	456.0674846625767
ationn stantiesecally                             	namelesscorp #6                                   	mens watch                                        	Jewelry                                           	456.0674846625767
barcallyeingbarcally                              	NULL	NULL	NULL	456.3137651821862
barcallyeingbarcally                              	importomaxi #9                                    	NULL	NULL	456.3137651821862
barcallyeingbarcally                              	importomaxi #9                                    	business                                          	NULL	456.3137651821862
barcallyeingbarcally                              	importomaxi #9                                    	business                                          	Books                                             	456.3137651821862
n steingprin stable                               	NULL	NULL	NULL	456.3231707317073
n steingprin stable                               	exportiimporto #2                                 	NULL	NULL	456.3231707317073
n steingprin stable                               	exportiimporto #2                                 	pants                                             	NULL	456.3231707317073
n steingprin stable                               	exportiimporto #2                                 	pants                                             	Men                                               	456.3231707317073
ationeingoughtbarn st                             	NULL	NULL	NULL	456.8577235772358
ationeingoughtbarn st                             	exportischolar #2                                 	NULL	NULL	456.8577235772358
ationeingoughtbarn st                             	exportischolar #2                                 	pop                                               	NULL	456.8577235772358
ationeingoughtbarn st                             	exportischolar #2                                 	pop                                               	Music                                             	456.8577235772358
eseoughteseeingcally                              	NULL	NULL	NULL	456.99392712550605
eseoughteseeingcally                              	brandbrand #5                                     	NULL	NULL	456.99392712550605
eseoughteseeingcally                              	brandbrand #5                                     	decor                                             	NULL	456.99392712550605
eseoughteseeingcally                              	brandbrand #5                                     	decor                                             	Home                                              	456.99392712550605
ablen stantibarcally                              	NULL	NULL	NULL	457.0543259557344
ablen stantibarcally                              	importoamalg #1                                   	NULL	NULL	457.0543259557344
ablen stantibarcally                              	importoamalg #1                                   	fragrances                                        	NULL	457.0543259557344
ablen stantibarcally                              	importoamalg #1                                   	fragrances                                        	Women                                             	457.0543259557344
priantianticallycally                             	NULL	NULL	NULL	457.0612244897959
priantianticallycally                             	importounivamalg #2                               	NULL	NULL	457.0612244897959
priantianticallycally                             	importounivamalg #2                               	home repair                                       	NULL	457.0612244897959
priantianticallycally                             	importounivamalg #2                               	home repair                                       	Books                                             	457.0612244897959
prin steingeingn st                               	NULL	NULL	NULL	457.1796407185629
prin steingeingn st                               	brandmaxi #2                                      	NULL	NULL	457.1796407185629
prin steingeingn st                               	brandmaxi #2                                      	reference                                         	NULL	457.1796407185629
prin steingeingn st                               	brandmaxi #2                                      	reference                                         	Books                                             	457.1796407185629
antieingationeseation                             	NULL	NULL	NULL	457.3225806451613
antieingationeseation                             	amalgbrand #2                                     	NULL	NULL	457.3225806451613
antieingationeseation                             	amalgbrand #2                                     	semi-precious                                     	NULL	457.3225806451613
antieingationeseation                             	amalgbrand #2                                     	semi-precious                                     	Jewelry                                           	457.3225806451613
ableprieseoughtn st                               	NULL	NULL	NULL	457.5081632653061
ableprieseoughtn st                               	amalgexporti #1                                   	NULL	NULL	457.5081632653061
ableprieseoughtn st                               	amalgexporti #1                                   	newborn                                           	NULL	457.5081632653061
ableprieseoughtn st                               	amalgexporti #1                                   	newborn                                           	Children                                          	457.5081632653061
eingoughtn stesen st                              	NULL	NULL	NULL	457.52226720647775
eingoughtn stesen st                              	edu packexporti #1                                	NULL	NULL	457.52226720647775
eingoughtn stesen st                              	edu packexporti #1                                	school-uniforms                                   	NULL	457.52226720647775
eingoughtn stesen st                              	edu packexporti #1                                	school-uniforms                                   	Children                                          	457.52226720647775
Time taken: 956.989 seconds, Fetched: 100 row(s)
timediff:962.337765133
----------------------------------------------------------------------------------------------------------------------------------------