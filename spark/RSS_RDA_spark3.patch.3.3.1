diff -uprN spark-3.3.1/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala spark-3.3.1.rss/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala
--- spark-3.3.1/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala	2022-10-15 13:56:02.000000000 +0800
+++ spark-3.3.1.rss/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala	2023-01-22 18:12:49.000000000 +0800
@@ -209,7 +209,8 @@ private[spark] class ExecutorAllocationM
       } else if (decommissionEnabled &&
           conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED)) {
         logInfo("Shuffle data decommission is enabled without a shuffle service.")
-      } else if (!testing) {
+//      } else if (!testing) {
+      } else if (!testing && !Utils.isRssEnabled(conf)) {
         throw new SparkException("Dynamic allocation of executors requires the external " +
           "shuffle service. You may enable this through spark.shuffle.service.enabled.")
       }
diff -uprN spark-3.3.1/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala spark-3.3.1.rss/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala
--- spark-3.3.1/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala	2022-10-15 13:56:02.000000000 +0800
+++ spark-3.3.1.rss/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala	2023-01-22 18:12:49.000000000 +0800
@@ -2418,7 +2418,9 @@ private[spark] class DAGScheduler(
     // if the cluster manager explicitly tells us that the entire worker was lost, then
     // we know to unregister shuffle output.  (Note that "worker" specifically refers to the process
     // from a Standalone cluster, where the shuffle service lives in the Worker.)
-    val fileLost = workerHost.isDefined || !env.blockManager.externalShuffleServiceEnabled
+    //val fileLost = workerHost.isDefined || !env.blockManager.externalShuffleServiceEnabled
+    val fileLost = !Utils.isRssEnabled(sc.getConf) &&
+      (workerHost.isDefined || !env.blockManager.externalShuffleServiceEnabled)
     removeExecutorAndUnregisterOutputs(
       execId = execId,
       fileLost = fileLost,
diff -uprN spark-3.3.1/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala spark-3.3.1.rss/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
--- spark-3.3.1/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala	2022-10-15 13:56:02.000000000 +0800
+++ spark-3.3.1.rss/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala	2023-01-22 18:12:49.000000000 +0800
@@ -1032,7 +1032,9 @@ private[spark] class TaskSetManager(
     // and we are not using an external shuffle server which could serve the shuffle outputs.
     // The reason is the next stage wouldn't be able to fetch the data from this dead executor
     // so we would need to rerun these tasks on other executors.
-    if (isShuffleMapTasks && !env.blockManager.externalShuffleServiceEnabled && !isZombie) {
+//    if (isShuffleMapTasks && !env.blockManager.externalShuffleServiceEnabled && !isZombie) {
+    if (isShuffleMapTasks && !env.blockManager.externalShuffleServiceEnabled && !isZombie &&
+      !Utils.isRssEnabled(conf)) {
       for ((tid, info) <- taskInfos if info.executorId == execId) {
         val index = info.index
         // We may have a running task whose partition has been marked as successful,
diff -uprN spark-3.3.1/core/src/main/scala/org/apache/spark/util/Utils.scala spark-3.3.1.rss/core/src/main/scala/org/apache/spark/util/Utils.scala
--- spark-3.3.1/core/src/main/scala/org/apache/spark/util/Utils.scala	2022-10-15 13:56:02.000000000 +0800
+++ spark-3.3.1.rss/core/src/main/scala/org/apache/spark/util/Utils.scala	2023-01-22 18:12:49.000000000 +0800
@@ -3262,6 +3262,9 @@ private[spark] object Utils extends Logg
       case _ => math.max(sortedSize(len / 2), 1)
     }
   }
+
+  def isRssEnabled(conf: SparkConf): Boolean =
+    conf.get("spark.shuffle.manager", "sort") == "org.apache.spark.shuffle.celeborn.RssShuffleManager"
 }
 
 private[util] object CallerContext extends Logging {
