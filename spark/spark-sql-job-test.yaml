apiVersion: batch/v1
kind: Job
metadata:
  name: spark-sql-job-test
spec:
  template:
    metadata:
      labels:
        app: spark-sql
    spec:
      restartPolicy: OnFailure   #job不能使用Always为默认的重新启动策略
      containers:
        - name: spark-sql
          image: harbor.my.org:1080/bronzels/spark-juicefs:3.3.1
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K8S_APISERVER
              value: kubernetes.default.svc.cluster.local:443
            - name: SPARK_EXECUTOR_MEMORY
              value: "4G"
            - name: SPARK_KUBERNETES_EXECUTOR_REQUEST_CORES
              value: "500m"
            - name: SPARK_KUBERNETES_EXECUTOR_LIMIT_CORES
              value: "1000m"
          command: ["spark-sql-job.sh"]
          args: ["harbor.my.org:1080/bronzels/spark-juicefs-tpc:3.3.1", "dbuse.sql", "spark-queries-tpcds/q1.sql"]
      volumes:
        - name: juicefs-pv
          persistentVolumeClaim:
            claimName: rss-juicefs-pvc