#FROM harbor.my.org:1080/bronzels/hive-ubussh-juicefs:0.1 as hadoop
#ARG FLINK_SHORT_VERSION=?
#FROM flink:${FLINK_SHORT_VERSION}
#FROM eclipse-temurin:11-jre-jammy
FROM eclipse-temurin:8u362-b09-jre-jammy
#eclipse-temurin:11-jre
#ubuntu:22.04

# Install dependencies
RUN set -ex; \
  apt-get update; \
  apt-get -y install gpg libsnappy1v5 gettext-base libjemalloc-dev; \
  rm -rf /var/lib/apt/lists/*

# Grab gosu for easy step-down from root
ENV GOSU_VERSION 1.11
RUN set -ex; \
  wget -nv -O /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)"; \
  wget -nv -O /usr/local/bin/gosu.asc "https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc"; \
  export GNUPGHOME="$(mktemp -d)"; \
  for server in ha.pool.sks-keyservers.net $(shuf -e \
                          hkp://p80.pool.sks-keyservers.net:80 \
                          keyserver.ubuntu.com \
                          hkp://keyserver.ubuntu.com:80 \
                          pgp.mit.edu) ; do \
      gpg --batch --keyserver "$server" --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 && break || : ; \
  done && \
  gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \
  gpgconf --kill all; \
  rm -rf "$GNUPGHOME" /usr/local/bin/gosu.asc; \
  chmod +x /usr/local/bin/gosu; \
  gosu nobody true

# Configure Flink version
ARG FLINK_VERSION=?
ARG TARGET_BUILT=?
COPY --chown=flink:flink flink-${FLINK_VERSION}-${TARGET_BUILT} /opt/flink

# Prepare environment
ENV FLINK_HOME=/opt/flink
RUN groupadd --system --gid=9999 flink && \
    useradd --system --home-dir $FLINK_HOME --uid=9999 --gid=flink flink
WORKDIR $FLINK_HOME

# Install Flink
# Replace default REST/RPC endpoint bind address to use the container's network interface \
RUN \
  sed -i 's/rest.address: localhost/rest.address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml; \
  sed -i 's/rest.bind-address: localhost/rest.bind-address: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml; \
  sed -i 's/jobmanager.bind-host: localhost/jobmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml; \
  sed -i 's/taskmanager.bind-host: localhost/taskmanager.bind-host: 0.0.0.0/g' $FLINK_HOME/conf/flink-conf.yaml; \
  sed -i '/taskmanager.host: localhost/d' $FLINK_HOME/conf/flink-conf.yaml;

ENV OLDPATH=$PATH
ENV PATH=$PATH:$FLINK_HOME/bin

RUN mkdir /opt/flink/usrlib
RUN chown flink:flink /opt/flink/usrlib
#USER root

ARG JUICEFS_VERSION=?
#COPY --chown=flink:flink juicefs-hadoop-${JUICEFS_VERSION}-jdk11-ubuntu22.04.jar $FLINK_HOME/lib/
COPY --chown=flink:flink juicefs-hadoop-${JUICEFS_VERSION}.jar $FLINK_HOME/lib/

COPY --chown=flink:flink conf/log4j.properties $FLINK_HOME/conf/
COPY --chown=flink:flink conf/log4j-console.properties $FLINK_HOME/conf/
COPY --chown=flink:flink conf/core-site.xml $FLINK_HOME/conf/
COPY --chown=flink:flink conf/hdfs-site.xml $FLINK_HOME/conf/
COPY --chown=flink:flink conf/hive-site.xml $FLINK_HOME/conf/

ARG HADOOP_VERSION=?
COPY --chown=flink:flink hadoop-${HADOOP_VERSION} /opt/hadoop
ENV HADOOP_HOME=/opt/hadoop \
    HADOOP_COMMON_HOME=/opt/hadoop \
    HADOOP_HDFS_HOME=/opt/hadoop \
    HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop \
    PATH=${PATH}:/opt/hadoop/bin
ENV HADOOP_CLASSPATH=/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*
COPY --chown=flink:flink config.sh $FLINK_HOME/bin/config.sh

RUN useradd -u 10001 -d /opt/hdfs hdfs && \
 mkdir -p /opt/hdfs && \
 chown hdfs:hdfs /opt/hdfs && \
 usermod -g root hdfs

RUN mkdir /opt/hadoopconf
RUN chown flink:flink /opt/hadoopconf
COPY --chown=flink:flink conf/core-site.xml /opt/hadoopconf/
COPY --chown=flink:flink conf/hdfs-site.xml /opt/hadoopconf/

RUN mkdir /opt/hiveconf
RUN chown flink:flink /opt/hiveconf
COPY --chown=flink:flink conf/hive-site.xml /opt/hiveconf/

ARG HIVEREV=?
ARG SCALA_VERSION=?
ARG FLINK_VERSION=?
#no need as rebuilt&copied already
#COPY --chown=flink:flink flink-sql-connector-hive-${HIVEREV}_${SCALA_VERSION}-${FLINK_VERSION}.jar $FLINK_HOME/lib/
#COPY --chown=flink:flink flink-shaded-hadoop-3-3.1.1.7.2.9.0-173-9.0.jar $FLINK_HOME/lib/

#flink 1.15-16
#no need as rebuilt&copied already
#COPY --chown=flink:flink flink-sql-connector-kafka-${FLINK_VERSION}.jar $FLINK_HOME/lib/
#flink <= 1.4
#COPY --chown=flink:flink flink-sql-connector-kafka_${SCALA_VERSION}-${FLINK_VERSION}.jar $FLINK_HOME/lib/

ARG CDC_VERSION=?
COPY --chown=flink:flink flink-sql-connector-mysql-cdc-${CDC_VERSION}.jar $FLINK_HOME/lib/
COPY --chown=flink:flink flink-sql-connector-postgres-cdc-${CDC_VERSION}.jar $FLINK_HOME/lib/

#ARG FLINKOP_VERSION=?
#COPY --chown=flink:flink flink-kubernetes-operator-release-${FLINKOP_VERSION}/examples/flink-sql-runner-example/target/flink-sql-runner-example-${FLINKOP_VERSION}.jar /opt/flink/usrlib/sql-runner.jar
#COPY --chown=flink:flink flink-kubernetes-operator-release-${FLINKOP_VERSION}/examples/flink-sql-runner-example/sql-scripts /opt/flink/usrlib/sql-scripts

ENV HIVE_CONF_DIR=$FLINK_HOME/hiveconf
ENV HADOOP_USER_NAME=hdfs

ARG FLINK_SHORT_VERSION=?
ARG HUDI_VERSION=?
#1.16/1.17也用的是hudi用1.15编译出的版本
#COPY --chown=flink:flink hudibk/${TARGET_BUILT}/hudi-flink${FLINK_SHORT_VERSION}-bundle-${HUDI_VERSION}.jar $FLINK_HOME/lib/
COPY --chown=flink:flink hudibk/${TARGET_BUILT}/hudi-flink-bundle-${HUDI_VERSION}.jar $FLINK_HOME/lib/
#COPY --chown=flink:flink hudi-hive-sync-bundle-${HUDI_VERSION}.jar $FLINK_HOME/lib/
#COPY --chown=flink:flink htrace-core4-4.1.0-incubating.jar $FLINK_HOME/lib/

ARG STARROCKS_CONNECTOR_VERSION=?
#COPY --chown=flink:flink starrocks-connector-for-apache-flink-${STARROCKS_CONNECTOR_VERSION}/target/flink-connector-starrocks-${STARROCKS_CONNECTOR_VERSION}_flink-${FLINK_SHORT_VERSION}.jar $FLINK_HOME/lib/
COPY --chown=flink:flink starrocksbk/flink-connector-starrocks-${STARROCKS_CONNECTOR_VERSION}_flink.jar $FLINK_HOME/lib/

#USER flink

#three of the files which may lead to hive/hudi conflict and hudi not working if added into flink lib/, hive dialect can not be set maybe due to absence these files
#COPY --chown=flink:flink hive-exec-${HIVEREV}.jar $FLINK_HOME/lib/
#COPY --chown=flink:flink antlr-runtime-3.5.2.jar $FLINK_HOME/lib/
#only for hive3.1.x
#COPY --chown=flink:flink libfb303-0.9.3.jar $FLINK_HOME/lib/

#no need as rebuilt&copied already
#one of the files which may lead to hive/hudi conflict and hudi not working if added into flink lib/, hive dialect can not be set maybe due to absence these files
#COPY --chown=flink:flink flinkbk/${TARGET_BUILT}/flink-connector-hive_${SCALA_VERSION}-${FLINK_VERSION}.jar $FLINK_HOME/lib/

#1.15.4需要
#1.16.1？
#1.17.0不需要
COPY --chown=flink:flink calcite-core-1.14.0.jar $FLINK_HOME/lib/

# Configure container
#比较了1.15.4/1.16.1/1.17.0入口脚本相同
#COPY docker-entrypoint-${FLINK_SHORT_VERSION}.sh /docker-entrypoint.sh
COPY docker-entrypoint.sh /docker-entrypoint.sh
ENTRYPOINT ["/docker-entrypoint.sh"]
EXPOSE 6123 8081
CMD ["help"]

COPY --chown=flink:flink setting.sql $FLINK_HOME/usrlib/
COPY --chown=flink:flink conf/flink-conf.yaml $FLINK_HOME/conf/
